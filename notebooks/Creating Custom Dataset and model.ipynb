{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.status.busy": "2025-05-04T14:05:02.382742Z",
     "iopub.status.idle": "2025-05-04T14:05:02.382985Z",
     "shell.execute_reply": "2025-05-04T14:05:02.382887Z",
     "shell.execute_reply.started": "2025-05-04T14:05:02.382877Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8/5/25 Creating Custom Dataset using GitHub Issues of Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T11:39:48.835013Z",
     "iopub.status.busy": "2025-05-08T11:39:48.834470Z",
     "iopub.status.idle": "2025-05-08T11:39:49.080492Z",
     "shell.execute_reply": "2025-05-08T11:39:49.079882Z",
     "shell.execute_reply.started": "2025-05-08T11:39:48.834989Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://api.github.com/repos/pytorch/pytorch/issues?page=1&per_page=1\"\n",
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T11:39:49.171709Z",
     "iopub.status.busy": "2025-05-08T11:39:49.171076Z",
     "iopub.status.idle": "2025-05-08T11:39:49.176771Z",
     "shell.execute_reply": "2025-05-08T11:39:49.176017Z",
     "shell.execute_reply.started": "2025-05-08T11:39:49.171687Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T11:39:50.439940Z",
     "iopub.status.busy": "2025-05-08T11:39:50.439092Z",
     "iopub.status.idle": "2025-05-08T11:39:50.446420Z",
     "shell.execute_reply": "2025-05-08T11:39:50.445613Z",
     "shell.execute_reply.started": "2025-05-08T11:39:50.439909Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'url': 'https://api.github.com/repos/pytorch/pytorch/issues/153144',\n",
       "  'repository_url': 'https://api.github.com/repos/pytorch/pytorch',\n",
       "  'labels_url': 'https://api.github.com/repos/pytorch/pytorch/issues/153144/labels{/name}',\n",
       "  'comments_url': 'https://api.github.com/repos/pytorch/pytorch/issues/153144/comments',\n",
       "  'events_url': 'https://api.github.com/repos/pytorch/pytorch/issues/153144/events',\n",
       "  'html_url': 'https://github.com/pytorch/pytorch/pull/153144',\n",
       "  'id': 3048581827,\n",
       "  'node_id': 'PR_kwDOA-j9z86VabfF',\n",
       "  'number': 153144,\n",
       "  'title': '[ROCm][Windows] Fix building torch 2.8 wheel with ROCm (added hipblasLt and rocblas directories)',\n",
       "  'user': {'login': 'tvukovic-amd',\n",
       "   'id': 127323445,\n",
       "   'node_id': 'U_kgDOB5bNNQ',\n",
       "   'avatar_url': 'https://avatars.githubusercontent.com/u/127323445?v=4',\n",
       "   'gravatar_id': '',\n",
       "   'url': 'https://api.github.com/users/tvukovic-amd',\n",
       "   'html_url': 'https://github.com/tvukovic-amd',\n",
       "   'followers_url': 'https://api.github.com/users/tvukovic-amd/followers',\n",
       "   'following_url': 'https://api.github.com/users/tvukovic-amd/following{/other_user}',\n",
       "   'gists_url': 'https://api.github.com/users/tvukovic-amd/gists{/gist_id}',\n",
       "   'starred_url': 'https://api.github.com/users/tvukovic-amd/starred{/owner}{/repo}',\n",
       "   'subscriptions_url': 'https://api.github.com/users/tvukovic-amd/subscriptions',\n",
       "   'organizations_url': 'https://api.github.com/users/tvukovic-amd/orgs',\n",
       "   'repos_url': 'https://api.github.com/users/tvukovic-amd/repos',\n",
       "   'events_url': 'https://api.github.com/users/tvukovic-amd/events{/privacy}',\n",
       "   'received_events_url': 'https://api.github.com/users/tvukovic-amd/received_events',\n",
       "   'type': 'User',\n",
       "   'user_view_type': 'public',\n",
       "   'site_admin': False},\n",
       "  'labels': [{'id': 1078897659,\n",
       "    'node_id': 'MDU6TGFiZWwxMDc4ODk3NjU5',\n",
       "    'url': 'https://api.github.com/repos/pytorch/pytorch/labels/module:%20rocm',\n",
       "    'name': 'module: rocm',\n",
       "    'color': 'f7e101',\n",
       "    'default': False,\n",
       "    'description': 'AMD GPU support for Pytorch'},\n",
       "   {'id': 1392590051,\n",
       "    'node_id': 'MDU6TGFiZWwxMzkyNTkwMDUx',\n",
       "    'url': 'https://api.github.com/repos/pytorch/pytorch/labels/open%20source',\n",
       "    'name': 'open source',\n",
       "    'color': 'ededed',\n",
       "    'default': False,\n",
       "    'description': None},\n",
       "   {'id': 3773064655,\n",
       "    'node_id': 'LA_kwDOA-j9z87g5GXP',\n",
       "    'url': 'https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing',\n",
       "    'name': 'topic: not user facing',\n",
       "    'color': 'B08798',\n",
       "    'default': False,\n",
       "    'description': 'topic category'}],\n",
       "  'state': 'open',\n",
       "  'locked': False,\n",
       "  'assignee': None,\n",
       "  'assignees': [],\n",
       "  'milestone': None,\n",
       "  'comments': 1,\n",
       "  'created_at': '2025-05-08T10:36:36Z',\n",
       "  'updated_at': '2025-05-08T10:47:53Z',\n",
       "  'closed_at': None,\n",
       "  'author_association': 'CONTRIBUTOR',\n",
       "  'type': None,\n",
       "  'active_lock_reason': None,\n",
       "  'draft': False,\n",
       "  'pull_request': {'url': 'https://api.github.com/repos/pytorch/pytorch/pulls/153144',\n",
       "   'html_url': 'https://github.com/pytorch/pytorch/pull/153144',\n",
       "   'diff_url': 'https://github.com/pytorch/pytorch/pull/153144.diff',\n",
       "   'patch_url': 'https://github.com/pytorch/pytorch/pull/153144.patch',\n",
       "   'merged_at': None},\n",
       "  'body': \"Since rocblas.dll and hipblaslt.dll are copied to torch/lib, rocblas and hipblaslt directories are needed to be stored there too (otherwise we have an error after wheel installation while searching for files in rocblas/library and hipblaslt/library which doesn't exist). This PR fixes this issue. \\n\\ncc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd\",\n",
       "  'closed_by': None,\n",
       "  'reactions': {'url': 'https://api.github.com/repos/pytorch/pytorch/issues/153144/reactions',\n",
       "   'total_count': 0,\n",
       "   '+1': 0,\n",
       "   '-1': 0,\n",
       "   'laugh': 0,\n",
       "   'hooray': 0,\n",
       "   'confused': 0,\n",
       "   'heart': 0,\n",
       "   'rocket': 0,\n",
       "   'eyes': 0},\n",
       "  'timeline_url': 'https://api.github.com/repos/pytorch/pytorch/issues/153144/timeline',\n",
       "  'performed_via_github_app': None,\n",
       "  'state_reason': None}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T14:43:01.120493Z",
     "iopub.status.busy": "2025-05-08T14:43:01.119771Z",
     "iopub.status.idle": "2025-05-08T14:43:01.209562Z",
     "shell.execute_reply": "2025-05-08T14:43:01.208993Z",
     "shell.execute_reply.started": "2025-05-08T14:43:01.120467Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "secret_value_1 = user_secrets.get_secret(\"Github_token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T14:43:08.226206Z",
     "iopub.status.busy": "2025-05-08T14:43:08.225927Z",
     "iopub.status.idle": "2025-05-08T14:43:08.230074Z",
     "shell.execute_reply": "2025-05-08T14:43:08.229317Z",
     "shell.execute_reply.started": "2025-05-08T14:43:08.226184Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "GITHUB_TOKEN = secret_value_1# Copy your GitHub token here\n",
    "headers = {\"Authorization\": f\"token {GITHUB_TOKEN}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T10:40:33.167068Z",
     "iopub.status.busy": "2025-05-08T10:40:33.166715Z",
     "iopub.status.idle": "2025-05-08T10:40:33.173446Z",
     "shell.execute_reply": "2025-05-08T10:40:33.172642Z",
     "shell.execute_reply.started": "2025-05-08T10:40:33.167045Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "def fetch_issues(\n",
    "    owner=\"pytorch\",\n",
    "    repo=\"pytorch\",\n",
    "    num_issues=10_000,\n",
    "    rate_limit=5_000,\n",
    "    issues_path=Path(\".\"),\n",
    "):\n",
    "    if not issues_path.is_dir():\n",
    "        issues_path.mkdir(exist_ok=True)\n",
    "\n",
    "    batch = []\n",
    "    all_issues = []\n",
    "    per_page = 100  # Number of issues to return per page\n",
    "    num_pages = math.ceil(num_issues / per_page)\n",
    "    base_url = \"https://api.github.com/repos\"\n",
    "\n",
    "    for page in tqdm(range(num_pages)):\n",
    "        # Query with state=all to get both open and closed issues\n",
    "        query = f\"issues?page={page}&per_page={per_page}&state=all\"\n",
    "        issues = requests.get(f\"{base_url}/{owner}/{repo}/{query}\", headers=headers)\n",
    "        batch.extend(issues.json())\n",
    "\n",
    "        if len(batch) > rate_limit and len(all_issues) < num_issues:\n",
    "            all_issues.extend(batch)\n",
    "            batch = []  # Flush batch for next time period\n",
    "            print(f\"Reached GitHub rate limit. Sleeping for one hour ...\")\n",
    "            time.sleep(60 * 60 + 1)\n",
    "\n",
    "    all_issues.extend(batch)\n",
    "    df = pd.DataFrame.from_records(all_issues)\n",
    "    df.to_json(f\"{issues_path}/{repo}-issues.jsonl\", orient=\"records\", lines=True)\n",
    "    print(\n",
    "        f\"Downloaded all the issues for {repo}! Dataset stored at {issues_path}/{repo}-issues.jsonl\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Depending on your internet connection, this can take several minutes to run...\n",
    "fetch_issues()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T13:44:41.158072Z",
     "iopub.status.busy": "2025-05-08T13:44:41.157529Z",
     "iopub.status.idle": "2025-05-08T13:44:42.339814Z",
     "shell.execute_reply": "2025-05-08T13:44:42.339051Z",
     "shell.execute_reply.started": "2025-05-08T13:44:41.158049Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'type', 'active_lock_reason', 'sub_issues_summary', 'body', 'closed_by', 'reactions', 'timeline_url', 'performed_via_github_app', 'state_reason', 'draft', 'pull_request']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "jsonObj = pd.read_json(path_or_buf=\"/kaggle/working/pytorch-issues.jsonl\", lines=True)\n",
    "\n",
    "# Show all column names\n",
    "print(jsonObj.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " > using the above features to create a dataset for the pytorch-issues "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T13:44:42.341099Z",
     "iopub.status.busy": "2025-05-08T13:44:42.340875Z",
     "iopub.status.idle": "2025-05-08T13:44:42.376801Z",
     "shell.execute_reply": "2025-05-08T13:44:42.376198Z",
     "shell.execute_reply.started": "2025-05-08T13:44:42.341073Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 33 columns):\n",
      " #   Column                    Non-Null Count  Dtype              \n",
      "---  ------                    --------------  -----              \n",
      " 0   url                       10000 non-null  object             \n",
      " 1   repository_url            10000 non-null  object             \n",
      " 2   labels_url                10000 non-null  object             \n",
      " 3   comments_url              10000 non-null  object             \n",
      " 4   events_url                10000 non-null  object             \n",
      " 5   html_url                  10000 non-null  object             \n",
      " 6   id                        10000 non-null  int64              \n",
      " 7   node_id                   10000 non-null  object             \n",
      " 8   number                    10000 non-null  int64              \n",
      " 9   title                     10000 non-null  object             \n",
      " 10  user                      10000 non-null  object             \n",
      " 11  labels                    10000 non-null  object             \n",
      " 12  state                     10000 non-null  object             \n",
      " 13  locked                    10000 non-null  bool               \n",
      " 14  assignee                  946 non-null    object             \n",
      " 15  assignees                 10000 non-null  object             \n",
      " 16  milestone                 151 non-null    object             \n",
      " 17  comments                  10000 non-null  int64              \n",
      " 18  created_at                10000 non-null  datetime64[ns, UTC]\n",
      " 19  updated_at                10000 non-null  datetime64[ns, UTC]\n",
      " 20  closed_at                 7016 non-null   datetime64[ns, UTC]\n",
      " 21  author_association        10000 non-null  object             \n",
      " 22  type                      34 non-null     object             \n",
      " 23  active_lock_reason        0 non-null      float64            \n",
      " 24  sub_issues_summary        3280 non-null   object             \n",
      " 25  body                      9954 non-null   object             \n",
      " 26  closed_by                 7231 non-null   object             \n",
      " 27  reactions                 10000 non-null  object             \n",
      " 28  timeline_url              10000 non-null  object             \n",
      " 29  performed_via_github_app  0 non-null      float64            \n",
      " 30  state_reason              1663 non-null   object             \n",
      " 31  draft                     6720 non-null   float64            \n",
      " 32  pull_request              6720 non-null   object             \n",
      "dtypes: bool(1), datetime64[ns, UTC](3), float64(3), int64(3), object(23)\n",
      "memory usage: 2.5+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(jsonObj.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T14:00:15.417542Z",
     "iopub.status.busy": "2025-05-08T14:00:15.417017Z",
     "iopub.status.idle": "2025-05-08T14:00:15.422540Z",
     "shell.execute_reply": "2025-05-08T14:00:15.421803Z",
     "shell.execute_reply.started": "2025-05-08T14:00:15.417517Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cc @jeffdaily @sunway513 @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jsonObj['body'][2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T14:20:02.642876Z",
     "iopub.status.busy": "2025-05-08T14:20:02.642558Z",
     "iopub.status.idle": "2025-05-08T14:20:02.649467Z",
     "shell.execute_reply": "2025-05-08T14:20:02.648678Z",
     "shell.execute_reply.started": "2025-05-08T14:20:02.642854Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         open\n",
       "1         open\n",
       "2         open\n",
       "3         open\n",
       "4         open\n",
       "         ...  \n",
       "9995    closed\n",
       "9996    closed\n",
       "9997    closed\n",
       "9998    closed\n",
       "9999    closed\n",
       "Name: state, Length: 10000, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jsonObj['state']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T14:21:12.502701Z",
     "iopub.status.busy": "2025-05-08T14:21:12.501963Z",
     "iopub.status.idle": "2025-05-08T14:21:12.509320Z",
     "shell.execute_reply": "2025-05-08T14:21:12.508756Z",
     "shell.execute_reply.started": "2025-05-08T14:21:12.502673Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['COLLABORATOR', 'NONE', 'CONTRIBUTOR', 'MEMBER'], dtype=object)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jsonObj['author_association'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T14:22:03.770095Z",
     "iopub.status.busy": "2025-05-08T14:22:03.769786Z",
     "iopub.status.idle": "2025-05-08T14:22:03.777704Z",
     "shell.execute_reply": "2025-05-08T14:22:03.776976Z",
     "shell.execute_reply.started": "2025-05-08T14:22:03.770071Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       ### 🐛 Describe the bug\\n\\nUsing NNPACK for con...\n",
       "1       ### 📚 The doc issue\\n\\nI have noticed there is...\n",
       "2       ### 🐛 Describe the bug\\n\\n## Description\\n\\nWh...\n",
       "3       ### 🐛 Describe the bug\\n\\nI use nn.MultiheadAt...\n",
       "4       This PR replaces most `std::chrono::system_clo...\n",
       "                              ...                        \n",
       "9995    cc @voznesenskym @penguinwu @EikanWang @jgong5...\n",
       "9996    Stack from [ghstack](https://github.com/ezyang...\n",
       "9997    By just extending the matrix and invoking scri...\n",
       "9998    Stack from [ghstack](https://github.com/ezyang...\n",
       "9999    Stack from [ghstack](https://github.com/ezyang...\n",
       "Name: body, Length: 10000, dtype: object"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jsonObj['body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T14:12:14.605693Z",
     "iopub.status.busy": "2025-05-08T14:12:14.605275Z",
     "iopub.status.idle": "2025-05-08T14:12:14.610993Z",
     "shell.execute_reply": "2025-05-08T14:12:14.610371Z",
     "shell.execute_reply.started": "2025-05-08T14:12:14.605661Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'login': 'Flamefire',\n",
       " 'id': 309017,\n",
       " 'node_id': 'MDQ6VXNlcjMwOTAxNw==',\n",
       " 'avatar_url': 'https://avatars.githubusercontent.com/u/309017?v=4',\n",
       " 'gravatar_id': '',\n",
       " 'url': 'https://api.github.com/users/Flamefire',\n",
       " 'html_url': 'https://github.com/Flamefire',\n",
       " 'followers_url': 'https://api.github.com/users/Flamefire/followers',\n",
       " 'following_url': 'https://api.github.com/users/Flamefire/following{/other_user}',\n",
       " 'gists_url': 'https://api.github.com/users/Flamefire/gists{/gist_id}',\n",
       " 'starred_url': 'https://api.github.com/users/Flamefire/starred{/owner}{/repo}',\n",
       " 'subscriptions_url': 'https://api.github.com/users/Flamefire/subscriptions',\n",
       " 'organizations_url': 'https://api.github.com/users/Flamefire/orgs',\n",
       " 'repos_url': 'https://api.github.com/users/Flamefire/repos',\n",
       " 'events_url': 'https://api.github.com/users/Flamefire/events{/privacy}',\n",
       " 'received_events_url': 'https://api.github.com/users/Flamefire/received_events',\n",
       " 'type': 'User',\n",
       " 'user_view_type': 'public',\n",
       " 'site_admin': False}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jsonObj['user'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T14:07:07.824070Z",
     "iopub.status.busy": "2025-05-08T14:07:07.823785Z",
     "iopub.status.idle": "2025-05-08T14:07:07.829006Z",
     "shell.execute_reply": "2025-05-08T14:07:07.828191Z",
     "shell.execute_reply.started": "2025-05-08T14:07:07.824049Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'login': 'shadow150519', 'id': 55205022, 'node_id': 'MDQ6VXNlcjU1MjA1MDIy', 'avatar_url': 'https://avatars.githubusercontent.com/u/55205022?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/shadow150519', 'html_url': 'https://github.com/shadow150519', 'followers_url': 'https://api.github.com/users/shadow150519/followers', 'following_url': 'https://api.github.com/users/shadow150519/following{/other_user}', 'gists_url': 'https://api.github.com/users/shadow150519/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/shadow150519/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/shadow150519/subscriptions', 'organizations_url': 'https://api.github.com/users/shadow150519/orgs', 'repos_url': 'https://api.github.com/users/shadow150519/repos', 'events_url': 'https://api.github.com/users/shadow150519/events{/privacy}', 'received_events_url': 'https://api.github.com/users/shadow150519/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}\n"
     ]
    }
   ],
   "source": [
    "if jsonObj['user'][1]['login'] == \"shadow150519\":\n",
    "    print(jsonObj['user'][1])\n",
    "else:\n",
    "    print(\"User Not Found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T14:24:11.359695Z",
     "iopub.status.busy": "2025-05-08T14:24:11.359051Z",
     "iopub.status.idle": "2025-05-08T14:24:11.365676Z",
     "shell.execute_reply": "2025-05-08T14:24:11.364932Z",
     "shell.execute_reply.started": "2025-05-08T14:24:11.359669Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       153139\n",
       "1       153138\n",
       "2       153137\n",
       "3       153136\n",
       "4       153135\n",
       "         ...  \n",
       "9995    143214\n",
       "9996    143213\n",
       "9997    143212\n",
       "9998    143211\n",
       "9999    143210\n",
       "Name: number, Length: 10000, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jsonObj['number']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T14:27:11.030168Z",
     "iopub.status.busy": "2025-05-08T14:27:11.029562Z",
     "iopub.status.idle": "2025-05-08T14:27:11.036067Z",
     "shell.execute_reply": "2025-05-08T14:27:11.035297Z",
     "shell.execute_reply.started": "2025-05-08T14:27:11.030142Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       None\n",
       "1       None\n",
       "2       None\n",
       "3       None\n",
       "4       None\n",
       "        ... \n",
       "9995    None\n",
       "9996    None\n",
       "9997    None\n",
       "9998    None\n",
       "9999    None\n",
       "Name: type, Length: 10000, dtype: object"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jsonObj[\"type\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T14:27:41.109643Z",
     "iopub.status.busy": "2025-05-08T14:27:41.109136Z",
     "iopub.status.idle": "2025-05-08T14:27:42.045394Z",
     "shell.execute_reply": "2025-05-08T14:27:42.044842Z",
     "shell.execute_reply.started": "2025-05-08T14:27:41.109606Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "input_path = \"/kaggle/working/pytorch-issues.jsonl\"\n",
    "output_path = \"/kaggle/working/pytorch-issues-selected.jsonl\"\n",
    "\n",
    "# List the fields you want to keep\n",
    "fields_to_keep = [\"id\", \"title\", \"user\", \"state\", \"labels\", 'comments', \"author_association\", \"body\", ]\n",
    "\n",
    "with open(input_path, \"r\") as f_in, open(output_path, \"w\") as f_out:\n",
    "    for line in f_in:\n",
    "        data = json.loads(line)\n",
    "        filtered_data = {k: data.get(k) for k in fields_to_keep}\n",
    "        # Optional: flatten nested 'user'\n",
    "        if isinstance(filtered_data.get(\"user\"), dict):\n",
    "            filtered_data[\"user\"] = filtered_data[\"user\"].get(\"login\", str(filtered_data[\"user\"]))\n",
    "        if isinstance(filtered_data.get(\"labels\"), list):\n",
    "            filtered_data[\"labels\"] = [label[\"name\"] if isinstance(label, dict) else label for label in filtered_data[\"labels\"]]\n",
    "        f_out.write(json.dumps(filtered_data) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T14:27:53.840525Z",
     "iopub.status.busy": "2025-05-08T14:27:53.839903Z",
     "iopub.status.idle": "2025-05-08T14:27:54.098969Z",
     "shell.execute_reply": "2025-05-08T14:27:54.098256Z",
     "shell.execute_reply.started": "2025-05-08T14:27:53.840502Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06f9b7c57b934713b8120dee5d866c51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'title', 'user', 'state', 'labels', 'comments', 'author_association', 'body'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "issue_dataset = load_dataset(\"json\", data_files= \"/kaggle/working/pytorch-issues-selected.jsonl\")\n",
    "issue_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T10:41:45.907787Z",
     "iopub.status.busy": "2025-05-09T10:41:45.907512Z",
     "iopub.status.idle": "2025-05-09T10:41:46.199735Z",
     "shell.execute_reply": "2025-05-09T10:41:46.199012Z",
     "shell.execute_reply.started": "2025-05-09T10:41:45.907767Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bdebf9033434be38fd3637ffcd10239",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "issues_dataset  = load_dataset(\n",
    "    \"json\",\n",
    "    data_files=\"/kaggle/working/pytorch-issues-selected.jsonl\",\n",
    "    split=\"train\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T14:30:13.549876Z",
     "iopub.status.busy": "2025-05-08T14:30:13.549172Z",
     "iopub.status.idle": "2025-05-08T14:30:13.564949Z",
     "shell.execute_reply": "2025-05-08T14:30:13.564131Z",
     "shell.execute_reply.started": "2025-05-08T14:30:13.549850Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Flamefire', 'shadow150519', 'SilentTester73', 'Neronjust2017', 'cyyever']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issues_dataset['user'][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">  basically here url is input and pr is output for our instruction tuning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T14:33:47.864812Z",
     "iopub.status.busy": "2025-05-08T14:33:47.864232Z",
     "iopub.status.idle": "2025-05-08T14:33:47.880673Z",
     "shell.execute_reply": "2025-05-08T14:33:47.880051Z",
     "shell.execute_reply.started": "2025-05-08T14:33:47.864788Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Title (input): Fix corner case in `torch.arange()` where int64_t truncation leads to size 0\n",
      ">> Body (output): Fixes #149097\n",
      "\n",
      "### Changes\n",
      "\n",
      "This PR introduces a workaround for corner case where casting start/end/step to int64_t may introduce precision loss. If all values are within the range that double can represent exactly (i.e., [-2^53, 2^53]), we prefer using double arithmetic for consistency across devices. Otherwise, fallback to int64_t computation.\n",
      "\n",
      "### Tests\n",
      "\n",
      "All results are same as np\n",
      "\n",
      "```\n",
      "python test/test_torch.py -k test_arange\n",
      "```\n",
      "\n",
      "cc: @albanD \n",
      "\n",
      ">> Title (input): Support SymmetricMemory's signaling kernels on sm60 and sm70\n",
      ">> Body (output): Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n",
      "* __->__ #146308\n",
      "\n",
      "By leveraging libcudacxx's utilities: https://nvidia.github.io/cccl/libcudacxx/extended_api/synchronization_primitives/atomic_ref.html\n",
      "\n",
      "cc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o\n",
      "\n",
      ">> Title (input): [ca][ddp] loud error with c++ reducer\n",
      ">> Body (output): Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n",
      "* #150258\n",
      "* __->__ #150073\n",
      "* #150074\n",
      "\n",
      "\n",
      "\n",
      "cc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample = issue_dataset[\"train\"].shuffle(seed=666).select(range(3))\n",
    "\n",
    "# Print out the title and body as input/output\n",
    "for title, body in zip(sample[\"title\"], sample[\"body\"]):\n",
    "    print(f\">> Title (input): {title}\")\n",
    "    print(f\">> Body (output): {body}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T14:34:12.947743Z",
     "iopub.status.busy": "2025-05-08T14:34:12.947070Z",
     "iopub.status.idle": "2025-05-08T14:34:13.889035Z",
     "shell.execute_reply": "2025-05-08T14:34:13.888228Z",
     "shell.execute_reply.started": "2025-05-08T14:34:12.947718Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "319cd5f89ec34be383c736519b8d71b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "issues_dataset = issues_dataset.map(\n",
    "    lambda x: {\"is_title\": False if x[\"title\"] is None else True}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T14:43:18.054775Z",
     "iopub.status.busy": "2025-05-08T14:43:18.054225Z",
     "iopub.status.idle": "2025-05-08T14:43:18.058127Z",
     "shell.execute_reply": "2025-05-08T14:43:18.057376Z",
     "shell.execute_reply.started": "2025-05-08T14:43:18.054752Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "headers = {\n",
    "    \"Authorization\": f\"token {GITHUB_TOKEN}\",\n",
    "    \"Accept\": \"application/vnd.github+json\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*well that looks helping for our finetuning environment*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T15:22:32.666021Z",
     "iopub.status.busy": "2025-05-08T15:22:32.665742Z",
     "iopub.status.idle": "2025-05-08T15:22:32.670131Z",
     "shell.execute_reply": "2025-05-08T15:22:32.669435Z",
     "shell.execute_reply.started": "2025-05-08T15:22:32.666002Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'title', 'user', 'state', 'labels', 'comments', 'author_association', 'body', 'is_title'],\n",
      "    num_rows: 10000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(issues_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T15:23:25.396228Z",
     "iopub.status.busy": "2025-05-08T15:23:25.394689Z",
     "iopub.status.idle": "2025-05-08T15:23:25.401033Z",
     "shell.execute_reply": "2025-05-08T15:23:25.399963Z",
     "shell.execute_reply.started": "2025-05-08T15:23:25.396198Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'title', 'user', 'state', 'labels', 'comments', 'author_association', 'body', 'is_title']\n"
     ]
    }
   ],
   "source": [
    "print(issues_dataset.column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T15:23:34.644258Z",
     "iopub.status.busy": "2025-05-08T15:23:34.643997Z",
     "iopub.status.idle": "2025-05-08T15:23:34.661520Z",
     "shell.execute_reply": "2025-05-08T15:23:34.660981Z",
     "shell.execute_reply.started": "2025-05-08T15:23:34.644239Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`cuda.Event` handling in dynamo is broken\n"
     ]
    }
   ],
   "source": [
    "print((issues_dataset[\"title\"][81]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T15:23:58.679540Z",
     "iopub.status.busy": "2025-05-08T15:23:58.678988Z",
     "iopub.status.idle": "2025-05-08T15:23:58.732801Z",
     "shell.execute_reply": "2025-05-08T15:23:58.732068Z",
     "shell.execute_reply.started": "2025-05-08T15:23:58.679517Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's an example:\n",
      "```\n",
      "import torch\n",
      "\n",
      "lst = []\n",
      "\n",
      "@torch.compile(backend=\"eager\", fullgraph=True)\n",
      "def f(x):\n",
      "    start_event = torch.cuda.Event(enable_timing=True)\n",
      "    end_event = torch.cuda.Event(enable_timing=True)\n",
      "\n",
      "    start_event.record()\n",
      "    out = torch.matmul(x, x)\n",
      "    end_event.record()\n",
      "\n",
      "    lst.append(start_event)\n",
      "    lst.append(end_event)\n",
      "    return out\n",
      "\n",
      "x = torch.randn(5000, device='cuda')\n",
      "out = f(x)\n",
      "print(lst[0].elapsed_time(lst[1]))\n",
      "```\n",
      "\n",
      "without compile this prints the elapsed time between the two events.\n",
      "```\n",
      "55.96131134033203\n",
      "```\n",
      "\n",
      "with compile this gives an error:\n",
      "```\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/users/hirsheybar/a/pytorch/tmp6.py\", line 20, in <module>\n",
      "    print(lst[0].elapsed_time(lst[1]))\n",
      "  File \"/data/users/hirsheybar/a/pytorch/torch/cuda/streams.py\", line 216, in elapsed_time\n",
      "    return super().elapsed_time(end_event)\n",
      "ValueError: Both events must be recorded before calculating elapsed time.\n",
      "```\n",
      "\n",
      "Why? here's the generated dynamo graph + residual bytecode below. It looks like:\n",
      "\n",
      "(1) dynamo handles the `cuda.Event()` creation + list appending as compile-time constants, stashing them as globals and putting them in the list as residual bytecode\n",
      "\n",
      "(2) dynamo *also* proxies the `cuda.Event()` object into the graph, even though it is also treating it as a constant. The `Event` object is unused though and gets DCEd\n",
      "\n",
      "(3) dynamo also proxies the `cuda.Event.record` calls into the graph, but they are DCEd\n",
      "\n",
      "(4) at runtime, none of the logic to record the events runs (even if it did it wouldn't run because dynamo is ignoring the events that were proxied in the graph)\n",
      "```\n",
      "# graph\n",
      " ===== __compiled_fn_1 =====\n",
      " /data/users/hirsheybar/a/pytorch/torch/fx/_lazy_graph_module.py class GraphModule(torch.nn.Module):\n",
      "    def forward(self, L_x_: \"f32[5000][1]cuda:0\"):\n",
      "        l_x_ = L_x_\n",
      "\n",
      "         # File: /data/users/hirsheybar/a/pytorch/tmp6.py:7 in f, code: start_event = torch.cuda.Event(enable_timing=True)\n",
      "        event = torch.cuda.streams.Event(enable_timing = True)\n",
      "\n",
      "         # File: /data/users/hirsheybar/a/pytorch/tmp6.py:8 in f, code: end_event = torch.cuda.Event(enable_timing=True)\n",
      "        event_1 = torch.cuda.streams.Event(enable_timing = True)\n",
      "\n",
      "         # File: /data/users/hirsheybar/a/pytorch/tmp6.py:10 in f, code: start_event.record()\n",
      "        record = event.record();  event = record = None\n",
      "\n",
      "         # File: /data/users/hirsheybar/a/pytorch/tmp6.py:11 in f, code: out = torch.matmul(x, x)\n",
      "        out: \"f32[][]cuda:0\" = torch.matmul(l_x_, l_x_);  l_x_ = None\n",
      "\n",
      "         # File: /data/users/hirsheybar/a/pytorch/tmp6.py:12 in f, code: end_event.record()\n",
      "        record_1 = event_1.record();  event_1 = record_1 = None\n",
      "        return (out,)\n",
      "\n",
      "# bytecode\n",
      "DEBUG: MODIFIED BYTECODE f /data/users/hirsheybar/a/pytorch/tmp6.py line 5\n",
      "  5           0 LOAD_GLOBAL              9 (__compiled_fn_1)\n",
      "              2 LOAD_FAST                0 (x)\n",
      "              4 DUP_TOP\n",
      "              6 STORE_FAST               7 (tmp_3)\n",
      "              8 CALL_FUNCTION            1\n",
      "             10 STORE_FAST               4 (graph_out_0)\n",
      "             12 LOAD_FAST                4 (graph_out_0)\n",
      "             14 LOAD_CONST               3 (0)\n",
      "             16 BINARY_SUBSCR\n",
      "             18 LOAD_GLOBAL              7 (_event_140622680852736_c0)\n",
      "             20 LOAD_GLOBAL              8 (_event_140622672089088_c0)\n",
      "             22 BUILD_LIST               2\n",
      "             24 LOAD_GLOBAL              5 (lst)\n",
      "             26 DUP_TOP\n",
      "             28 STORE_FAST               6 (tmp_2)\n",
      "             30 LOAD_CONST               0 (None)\n",
      "             32 LOAD_CONST               0 (None)\n",
      "             34 BUILD_SLICE              2\n",
      "             36 STORE_SUBSCR\n",
      "             38 DELETE_FAST              4 (graph_out_0)\n",
      "             40 RETURN_VALUE\n",
      "```\n",
      "\n",
      "cc @ptrblck @msaroufim @eqy @jerryzh168 @chauhang @penguinwu @voznesenskym @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @amjames\n"
     ]
    }
   ],
   "source": [
    "print((issues_dataset[\"body\"][81]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T15:24:25.980794Z",
     "iopub.status.busy": "2025-05-08T15:24:25.980230Z",
     "iopub.status.idle": "2025-05-08T15:24:29.143237Z",
     "shell.execute_reply": "2025-05-08T15:24:29.142494Z",
     "shell.execute_reply.started": "2025-05-08T15:24:25.980770Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc486d031c0a41799af70291ec572f4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb5a4fef18f0468fadf5c5f5cdc6fa37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/mayankpuvvala/github-pytorch-issues/commit/a0a58a44501a7d2715771f14fed814e09b135ebc', commit_message='Upload dataset', commit_description='', oid='a0a58a44501a7d2715771f14fed814e09b135ebc', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/mayankpuvvala/github-pytorch-issues', endpoint='https://huggingface.co', repo_type='dataset', repo_id='mayankpuvvala/github-pytorch-issues'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issues_dataset.push_to_hub(\"github-pytorch-issues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> we can see our custom dataset loaded to hugging face, change your dataset card readme for more better understanding of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T17:09:09.627330Z",
     "iopub.status.busy": "2025-05-10T17:09:09.626726Z",
     "iopub.status.idle": "2025-05-10T17:09:15.517076Z",
     "shell.execute_reply": "2025-05-10T17:09:15.516514Z",
     "shell.execute_reply.started": "2025-05-10T17:09:09.627304Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b384f9e025045c7992293165fedde30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/2.10k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc1bd016505e4c3ea538352856720854",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/7.32M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "495e9b2b07054ba1ad556243a2579397",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'title', 'user', 'state', 'labels', 'comments', 'author_association', 'body', 'is_title'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"mayankpuvvala/github-pytorch-issues\", split=\"train\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  9/5/25 Fine tuning using PEFT+ LoRA on the custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T17:09:15.518545Z",
     "iopub.status.busy": "2025-05-10T17:09:15.518227Z",
     "iopub.status.idle": "2025-05-10T17:09:15.623692Z",
     "shell.execute_reply": "2025-05-10T17:09:15.623043Z",
     "shell.execute_reply.started": "2025-05-10T17:09:15.518527Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   title   10000 non-null  object\n",
      " 1   body    9954 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 156.4+ KB\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 1. Convert original dataset to pandas and remove all unwanted columns\n",
    "df = dataset.to_pandas()\n",
    "df = df[[\"title\", \"body\"]].copy()  # Keep only needed columns\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T17:09:15.875215Z",
     "iopub.status.busy": "2025-05-10T17:09:15.874644Z",
     "iopub.status.idle": "2025-05-10T17:09:16.079442Z",
     "shell.execute_reply": "2025-05-10T17:09:16.078787Z",
     "shell.execute_reply.started": "2025-05-10T17:09:15.875195Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['title', 'body'],\n",
      "    num_rows: 10000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Rebuild dataset with sanitized dataframe\n",
    "dataset = Dataset.from_pandas(df, preserve_index=False)\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T17:09:19.212744Z",
     "iopub.status.busy": "2025-05-10T17:09:19.212099Z",
     "iopub.status.idle": "2025-05-10T17:09:19.224431Z",
     "shell.execute_reply": "2025-05-10T17:09:19.223716Z",
     "shell.execute_reply.started": "2025-05-10T17:09:19.212717Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['title', 'body'],\n",
       "        num_rows: 9000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['title', 'body'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T10:26:24.212976Z",
     "iopub.status.busy": "2025-05-11T10:26:24.212190Z",
     "iopub.status.idle": "2025-05-11T10:26:26.408481Z",
     "shell.execute_reply": "2025-05-11T10:26:26.407562Z",
     "shell.execute_reply.started": "2025-05-11T10:26:24.212946Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "267dfdb3f6d04b4e81fbfc5044f29112",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6dee5487f5e4ebb97b5355c1a779876",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/mayankpuvvala/peft_lora_t5_merged_model_pytorch_issues/commit/5c326874f96fa2f0f3493fe8be4627dea32e5b63', commit_message='Upload tokenizer', commit_description='', oid='5c326874f96fa2f0f3493fe8be4627dea32e5b63', pr_url=None, repo_url=RepoUrl('https://huggingface.co/mayankpuvvala/peft_lora_t5_merged_model_pytorch_issues', endpoint='https://huggingface.co', repo_type='model', repo_id='mayankpuvvala/peft_lora_t5_merged_model_pytorch_issues'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "tokenizer.push_to_hub(\"mayankpuvvala/peft_lora_t5_merged_model_pytorch_issues\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T17:44:04.806957Z",
     "iopub.status.busy": "2025-05-10T17:44:04.806414Z",
     "iopub.status.idle": "2025-05-10T17:44:04.811810Z",
     "shell.execute_reply": "2025-05-10T17:44:04.811034Z",
     "shell.execute_reply.started": "2025-05-10T17:44:04.806934Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def preprocess(example):\n",
    "    # Ensure values are strings\n",
    "    title = example[\"title\"] if example[\"title\"] is not None else \"\"\n",
    "    body = example[\"body\"] if example[\"body\"] is not None else \"\"\n",
    "\n",
    "    # Tokenize the input (title)\n",
    "    model_input = tokenizer(\n",
    "        text=title,\n",
    "        max_length=128,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "    # Tokenize the target (body)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            text=body,\n",
    "            max_length=128,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "    model_input[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T19:22:06.520236Z",
     "iopub.status.busy": "2025-05-09T19:22:06.519963Z",
     "iopub.status.idle": "2025-05-09T19:22:26.716380Z",
     "shell.execute_reply": "2025-05-09T19:22:26.715241Z",
     "shell.execute_reply.started": "2025-05-09T19:22:06.520211Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8eac08ff97f4274b342b5b77a91c19e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:3980: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63fae0dd0c4d454db23c37b8a4a9b047",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(\n",
    "    preprocess,\n",
    "    batched=False,\n",
    "    remove_columns=[\"title\", \"body\"]  # we no longer need raw text after tokenization\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T17:01:09.522357Z",
     "iopub.status.busy": "2025-05-10T17:01:09.522091Z",
     "iopub.status.idle": "2025-05-10T17:01:12.019431Z",
     "shell.execute_reply": "2025-05-10T17:01:12.018877Z",
     "shell.execute_reply.started": "2025-05-10T17:01:09.522338Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9f36a26342c46ca84da372a09ecd85c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "029cbd3b1ab34645b39c56ecd3916d45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "993feb7c57d844d39101ff66572356bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q\", \"v\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(model, lora_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T19:35:32.832151Z",
     "iopub.status.busy": "2025-05-09T19:35:32.831820Z",
     "iopub.status.idle": "2025-05-09T19:35:32.861851Z",
     "shell.execute_reply": "2025-05-09T19:35:32.861324Z",
     "shell.execute_reply.started": "2025-05-09T19:35:32.832124Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./peft+lora_FineTuning_Custom_Dataset\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir=\"./logs\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    "    disable_tqdm=False,  # 👈 ensures tqdm is enabled\n",
    "    report_to=\"none\",     # optionally disable WandB etc.\n",
    "    # fp16= True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T19:35:34.175834Z",
     "iopub.status.busy": "2025-05-09T19:35:34.175131Z",
     "iopub.status.idle": "2025-05-09T19:35:34.189053Z",
     "shell.execute_reply": "2025-05-09T19:35:34.188573Z",
     "shell.execute_reply.started": "2025-05-09T19:35:34.175802Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# from transformers import DataCollatorForSeq2Seq, default_data_collator\n",
    "# def custom_collator(batch):\n",
    "#     for k in batch[0].keys():\n",
    "#         print(f\"BATCH KEY: {k}\")\n",
    "#     return default_data_collator(batch)\n",
    "\n",
    "# collator = custom_collator\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    processing_class=tokenizer,\n",
    "    # data_collator=collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T19:35:42.045750Z",
     "iopub.status.busy": "2025-05-09T19:35:42.045455Z",
     "iopub.status.idle": "2025-05-09T19:35:42.517750Z",
     "shell.execute_reply": "2025-05-09T19:35:42.516908Z",
     "shell.execute_reply.started": "2025-05-09T19:35:42.045727Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trainer.save_model(\"full-checkpoint-trainer\")  # Saves model + trainer state\n",
    "trainer.save_state()                      # Optional but saves training arguments, RNG etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T19:35:42.519258Z",
     "iopub.status.busy": "2025-05-09T19:35:42.519027Z",
     "iopub.status.idle": "2025-05-09T19:35:42.977178Z",
     "shell.execute_reply": "2025-05-09T19:35:42.976486Z",
     "shell.execute_reply.started": "2025-05-09T19:35:42.519240Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "peft_model.save_pretrained(\"./lora-t5-pytorch\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> pushing the lora model to HuggingFace Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T19:35:42.978873Z",
     "iopub.status.busy": "2025-05-09T19:35:42.978280Z",
     "iopub.status.idle": "2025-05-09T19:35:42.982308Z",
     "shell.execute_reply": "2025-05-09T19:35:42.981658Z",
     "shell.execute_reply.started": "2025-05-09T19:35:42.978847Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# model_name = \"lora-t5-pytorch-issues\"  # change this to your preferred model name\n",
    "# hf_username = \"mayankpuvvala\"     # replace with your HF username\n",
    "\n",
    "# # Save and push the model\n",
    "# peft_model.push_to_hub(f\"{hf_username}/{model_name}\")\n",
    "# tokenizer.push_to_hub(f\"{hf_username}/{model_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 10/5/25 Inference on the custom model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-10T17:43:41.385978Z",
     "iopub.status.idle": "2025-05-10T17:43:41.386206Z",
     "shell.execute_reply": "2025-05-10T17:43:41.386098Z",
     "shell.execute_reply.started": "2025-05-10T17:43:41.386088Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "peft_model = PeftModel.from_pretrained(model, \"./lora-t5-pytorch\")  # Load your trained LoRA adapter\n",
    "merged_model =  peft_model.merge_and_unload()\n",
    "\n",
    "merged_model.save_pretrained(\"merged-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Print a few named layers to inspect structure\n",
    "for name, param in list(merged_model.named_parameters())[:5]:\n",
    "    print(f\"{name}: {param.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "merged_model.push_to_hub(\"mayankpuvvala/peft_lora_t5_merged_model_pytorch_issues\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T17:41:26.935862Z",
     "iopub.status.busy": "2025-05-10T17:41:26.935306Z",
     "iopub.status.idle": "2025-05-10T17:41:28.227054Z",
     "shell.execute_reply": "2025-05-10T17:41:28.226295Z",
     "shell.execute_reply.started": "2025-05-10T17:41:26.935825Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GitHub: @kungshinio's iPhone, reclassified to XNMACK....speak.com @kongwaopun.com @doync@echotbopun.nu @githubzang @shrkong\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "input_text = \"bump XNNPACK dependency to fix GCC 14 build on aarch64-linux\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(merged_model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=200,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    do_sample= True,\n",
    "    temperature=0.95\n",
    ")\n",
    "\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T17:43:34.483480Z",
     "iopub.status.busy": "2025-05-10T17:43:34.482907Z",
     "iopub.status.idle": "2025-05-10T17:43:34.488270Z",
     "shell.execute_reply": "2025-05-10T17:43:34.487682Z",
     "shell.execute_reply.started": "2025-05-10T17:43:34.483461Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['title', 'body'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = dataset[\"test\"]\n",
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T17:44:11.589457Z",
     "iopub.status.busy": "2025-05-10T17:44:11.589179Z",
     "iopub.status.idle": "2025-05-10T17:44:11.592791Z",
     "shell.execute_reply": "2025-05-10T17:44:11.592124Z",
     "shell.execute_reply.started": "2025-05-10T17:44:11.589438Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !pip install rouge_score evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T17:47:11.631100Z",
     "iopub.status.busy": "2025-05-10T17:47:11.630448Z",
     "iopub.status.idle": "2025-05-10T17:47:13.646855Z",
     "shell.execute_reply": "2025-05-10T17:47:13.646096Z",
     "shell.execute_reply.started": "2025-05-10T17:47:11.631071Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aeaa1f58bbb54350a60436608ae7715d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "import evaluate\n",
    "import torch\n",
    "\n",
    "tokenized_test = test_dataset.map(preprocess, batched=False, remove_columns=[\"title\", \"body\"]  )\n",
    "tokenized_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> testing inference on a single input sentence using the merged inference model of lora+t5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T19:16:14.437734Z",
     "iopub.status.busy": "2025-05-10T19:16:14.437463Z",
     "iopub.status.idle": "2025-05-10T19:16:16.902340Z",
     "shell.execute_reply": "2025-05-10T19:16:16.901691Z",
     "shell.execute_reply.started": "2025-05-10T19:16:14.437715Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "input_text = \"bump XNNPACK dependency to fix GCC 14 build on aarch64-linux\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(merged_model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = merged_model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=200,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    do_sample= True,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "merged_model_output= tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "# print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T19:16:16.904312Z",
     "iopub.status.busy": "2025-05-10T19:16:16.903798Z",
     "iopub.status.idle": "2025-05-10T19:16:18.141991Z",
     "shell.execute_reply": "2025-05-10T19:16:18.141357Z",
     "shell.execute_reply.started": "2025-05-10T19:16:16.904292Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "input_text = \"bump XNNPACK dependency to fix GCC 14 build on aarch64-linux\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=200,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    do_sample= True,\n",
    "    # temperature=0.95\n",
    ")\n",
    "base_model_output= tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Comparing base model and custom built model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T19:16:18.143060Z",
     "iopub.status.busy": "2025-05-10T19:16:18.142781Z",
     "iopub.status.idle": "2025-05-10T19:16:18.147453Z",
     "shell.execute_reply": "2025-05-10T19:16:18.146885Z",
     "shell.execute_reply.started": "2025-05-10T19:16:18.143037Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Model Output for the issue: \"?, \"Chang (Chongqing's), \"Chang (\"Ham-Salz\" = stag-\" = vgb) = '-:same ton \", if in a linux language-shifter it's only one error,\" xx.\n",
      "\n",
      "\n",
      "My Model Output for the issue: Uninstalled XnNPACK dependency on XNNPACK - aarch64-linux-cma. This will help support the build on arg #0  XNNPACK - #0   XnNPACK    #0         @sui.j  nnnnNPACK is    #10 @Bossap: #cttynnnn.com/ji.\n"
     ]
    }
   ],
   "source": [
    "print(\"Base Model Output for the issue: \" + base_model_output)\n",
    "print(\"\\n\")\n",
    "print(\"My Model Output for the issue: \" + merged_model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T19:16:18.149021Z",
     "iopub.status.busy": "2025-05-10T19:16:18.148794Z",
     "iopub.status.idle": "2025-05-10T19:16:19.217570Z",
     "shell.execute_reply": "2025-05-10T19:16:19.216780Z",
     "shell.execute_reply.started": "2025-05-10T19:16:18.149007Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import evaluate\n",
    "rouge = evaluate.load(\"rouge\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T17:58:28.068076Z",
     "iopub.status.busy": "2025-05-10T17:58:28.067772Z",
     "iopub.status.idle": "2025-05-10T17:58:28.081234Z",
     "shell.execute_reply": "2025-05-10T17:58:28.080579Z",
     "shell.execute_reply.started": "2025-05-10T17:58:28.068057Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "references = test_dataset[\"body\"]  # These are your gold labels\n",
    "len(references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T18:58:17.625891Z",
     "iopub.status.busy": "2025-05-10T18:58:17.625257Z",
     "iopub.status.idle": "2025-05-10T19:11:12.854102Z",
     "shell.execute_reply": "2025-05-10T19:11:12.853480Z",
     "shell.execute_reply.started": "2025-05-10T18:58:17.625867Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "batch_size = 16\n",
    "predicted_outcome = []\n",
    "\n",
    "for i in range(0, len(test_dataset), batch_size):\n",
    "    titles = test_dataset[\"title\"][i:i+batch_size]\n",
    "    inputs = tokenizer(titles, return_tensors=\"pt\", padding=True, truncation=True).to(merged_model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=200,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            do_sample=True,\n",
    "            temperature=0.95\n",
    "        )\n",
    "\n",
    "    # Decode all outputs in batch\n",
    "    decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    predicted_outcome.extend(decoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T19:11:12.855747Z",
     "iopub.status.busy": "2025-05-10T19:11:12.855474Z",
     "iopub.status.idle": "2025-05-10T19:11:19.078727Z",
     "shell.execute_reply": "2025-05-10T19:11:19.078036Z",
     "shell.execute_reply.started": "2025-05-10T19:11:12.855726Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE results: {'rouge1': 0.0638729614520744, 'rouge2': 0.012113988201299337, 'rougeL': 0.047053712668345155, 'rougeLsum': 0.055277033992282835}\n"
     ]
    }
   ],
   "source": [
    "# Make sure both predictions and references contain only strings, no None\n",
    "predictions = [p if p is not None else \"\" for p in predicted_outcome]\n",
    "references = [r if r is not None else \"\" for r in test_dataset[\"body\"]]\n",
    "\n",
    "# Now compute ROUGE\n",
    "results = rouge.compute(predictions=predictions, references=references)\n",
    "print(\"ROUGE results:\", results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Bert Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T19:21:40.395804Z",
     "iopub.status.busy": "2025-05-10T19:21:40.395504Z",
     "iopub.status.idle": "2025-05-10T19:21:40.399792Z",
     "shell.execute_reply": "2025-05-10T19:21:40.399165Z",
     "shell.execute_reply.started": "2025-05-10T19:21:40.395783Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ! pip install bert-score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T19:24:20.469528Z",
     "iopub.status.busy": "2025-05-10T19:24:20.469266Z",
     "iopub.status.idle": "2025-05-10T19:24:50.796644Z",
     "shell.execute_reply": "2025-05-10T19:24:50.795906Z",
     "shell.execute_reply.started": "2025-05-10T19:24:20.469511Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a20501917c8e48f0a3d180a6606d027b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8af6823e56224a1aa8ca574421a6b756",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e99fa243fe4b444f8dfd52101a8b23cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0f53d342ffb45f2b81439f8dbb2659f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02d0ca5f5f4246bab2d93ec244f94584",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTScore Precision: 0.5320\n",
      "BERTScore Recall:    0.4619\n",
      "BERTScore F1:        0.4902\n"
     ]
    }
   ],
   "source": [
    "from bert_score import score\n",
    "\n",
    "# Compute BERTScore\n",
    "P, R, F1 = score(predictions, references, lang=\"en\", model_type=\"bert-base-uncased\")\n",
    "\n",
    "# Print averaged scores\n",
    "print(f\"BERTScore Precision: {P.mean().item():.4f}\")\n",
    "print(f\"BERTScore Recall:    {R.mean().item():.4f}\")\n",
    "print(f\"BERTScore F1:        {F1.mean().item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T17:32:55.163751Z",
     "iopub.status.busy": "2025-05-10T17:32:55.163475Z",
     "iopub.status.idle": "2025-05-10T17:32:55.167097Z",
     "shell.execute_reply": "2025-05-10T17:32:55.166395Z",
     "shell.execute_reply.started": "2025-05-10T17:32:55.163733Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # ! pip install vllm\n",
    "# ! python3 -m vllm.entrypoints.openai.api_server --model \"mayankpuvvala/peft_lora_t5_merged_model_pytorch_issues\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T10:19:32.745402Z",
     "iopub.status.busy": "2025-05-11T10:19:32.745123Z",
     "iopub.status.idle": "2025-05-11T10:19:32.893488Z",
     "shell.execute_reply": "2025-05-11T10:19:32.892653Z",
     "shell.execute_reply.started": "2025-05-11T10:19:32.745384Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flan-t5-custom\t\t peft+lora_FineTuning_Custom_Dataset\n",
      "formatted.jsonl\t\t pytorch_fine_tune_training_args\n",
      "full-checkpoint-trainer  pytorch-issues-flat.jsonl\n",
      "logs\t\t\t pytorch-issues.jsonl\n",
      "lora-t5-pytorch\t\t pytorch-issues-selected.jsonl\n",
      "merged-model\t\t runs\n",
      "peft-flan-t5\t\t state.db\n",
      "peft-flan-t5-lora\n"
     ]
    }
   ],
   "source": [
    "! dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T10:26:32.743083Z",
     "iopub.status.busy": "2025-05-11T10:26:32.742761Z",
     "iopub.status.idle": "2025-05-11T10:26:34.077378Z",
     "shell.execute_reply": "2025-05-11T10:26:34.076641Z",
     "shell.execute_reply.started": "2025-05-11T10:26:32.743056Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbf3d641e4a9454f9eb636041b063d8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/20.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "673decc2705d437186b9e459019b34d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "504e92fdd38740349eb826f5c0a9af31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94d6a6dcac4544b098b3787d34876274",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text2text-generation\", model=\"mayankpuvvala/peft_lora_t5_merged_model_pytorch_issues\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 3438844,
     "sourceId": 6004344,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
