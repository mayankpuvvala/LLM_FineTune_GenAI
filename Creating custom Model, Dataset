{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":6004344,"sourceType":"datasetVersion","datasetId":3438844}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-04T14:05:02.382742Z","iopub.status.idle":"2025-05-04T14:05:02.382985Z","shell.execute_reply.started":"2025-05-04T14:05:02.382877Z","shell.execute_reply":"2025-05-04T14:05:02.382887Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 1. Fine-tuning LLM's","metadata":{}},{"cell_type":"markdown","source":">  Generating responses using mistral_instruct_7b_en/6 model ","metadata":{}},{"cell_type":"code","source":"# !pip install -q git+https://github.com/huggingface/transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T14:05:02.384457Z","iopub.status.idle":"2025-05-04T14:05:02.384687Z","shell.execute_reply.started":"2025-05-04T14:05:02.384577Z","shell.execute_reply":"2025-05-04T14:05:02.384587Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"hf\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T10:26:15.965015Z","iopub.execute_input":"2025-05-11T10:26:15.965342Z","iopub.status.idle":"2025-05-11T10:26:16.040109Z","shell.execute_reply.started":"2025-05-11T10:26:15.965322Z","shell.execute_reply":"2025-05-11T10:26:16.039337Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"from huggingface_hub import login\nlogin(secret_value_0)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T10:26:16.607384Z","iopub.execute_input":"2025-05-11T10:26:16.608175Z","iopub.status.idle":"2025-05-11T10:26:16.653536Z","shell.execute_reply.started":"2025-05-11T10:26:16.608149Z","shell.execute_reply":"2025-05-11T10:26:16.652952Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\", use_auth_token=True)\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"mistralai/Mistral-7B-Instruct-v0.1\",\n    device_map=\"auto\",\n    torch_dtype=\"auto\",\n    use_auth_token=True\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T14:05:02.390038Z","iopub.status.idle":"2025-05-04T14:05:02.390327Z","shell.execute_reply.started":"2025-05-04T14:05:02.390213Z","shell.execute_reply":"2025-05-04T14:05:02.390223Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 1.1  Learning: to Token the inputs","metadata":{}},{"cell_type":"code","source":"import torch\n\ninputs = tokenizer(\"[INST] What is Keras? [/INST]\", return_tensors=\"pt\").to(\"cuda\")\noutputs = model.generate(**inputs, max_new_tokens=100)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T14:05:02.391850Z","iopub.status.idle":"2025-05-04T14:05:02.392122Z","shell.execute_reply.started":"2025-05-04T14:05:02.391975Z","shell.execute_reply":"2025-05-04T14:05:02.391985Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"messages = [\n    {\"role\": \"user\", \"content\": \"What do you think about AGI? Are we close to it?\"},\n    {\"role\": \"assistant\", \"content\": \"I'm fascinated \\\n    by AI's data analysis and prediction abilities. \\\n    It has the potential to revolutionize industries and improve problem-solving.\"},\n    {\"role\": \"user\", \"content\": \"Should I be afraid of AI?\"}\n]\nmodel_inputs = tokenizer.apply_chat_template(messages,return_tensors = \"pt\")\ngenerated_ids = model.generate(\n    model_inputs,\n    max_new_tokens = 1000,\n    do_sample = True,\n)\ndecoded = tokenizer.batch_decode(generated_ids)\nprint(decoded[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T14:05:02.393744Z","iopub.status.idle":"2025-05-04T14:05:02.394149Z","shell.execute_reply.started":"2025-05-04T14:05:02.393947Z","shell.execute_reply":"2025-05-04T14:05:02.393964Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"messages = [{\n    \"role\":\"user\",\n    \"content\": \"Can you tell us 3 reasons why Rwanda is a good place to visit?\"\n}]\n\ntokenizer.apply_chat_template(messages, tokenize=False)\n\nmodel_inputs = tokenizer.apply_chat_template(messages, return_tensors = \"pt\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T14:05:02.395878Z","iopub.status.idle":"2025-05-04T14:05:02.396644Z","shell.execute_reply.started":"2025-05-04T14:05:02.396497Z","shell.execute_reply":"2025-05-04T14:05:02.396510Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_inputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T14:05:02.397518Z","iopub.status.idle":"2025-05-04T14:05:02.397810Z","shell.execute_reply.started":"2025-05-04T14:05:02.397654Z","shell.execute_reply":"2025-05-04T14:05:02.397671Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"generated_ids = model.generate(\n    model_inputs,\n    max_new_tokens = 1000,\n    do_sample = True,\n)\n\ndecoded = tokenizer.batch_decode(generated_ids)\n\nprint(decoded[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T14:05:02.399284Z","iopub.status.idle":"2025-05-04T14:05:02.399604Z","shell.execute_reply.started":"2025-05-04T14:05:02.399454Z","shell.execute_reply":"2025-05-04T14:05:02.399472Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"*Hyderabi Banger rhyme apparently*","metadata":{}},{"cell_type":"code","source":"messages = [{\n    \"role\": \"user\", \n    \"content\": \"you want to rhymne and bully that bully in your class in hyderabad, India by saying something which your local classmates can understand use local dishes, environment, songs, and stuff like that in references and make it a banger\"\n}]\n\nmodel_inputs = tokenizer.apply_chat_template(messages,return_tensors = \"pt\")\n\ngenerated_ids = model.generate(\n    model_inputs,\n    max_new_tokens = 1000,\n    do_sample = True,\n)\n\ndecoded = tokenizer.batch_decode(generated_ids)\n\nprint(decoded[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T14:05:02.401102Z","iopub.status.idle":"2025-05-04T14:05:02.401468Z","shell.execute_reply.started":"2025-05-04T14:05:02.401290Z","shell.execute_reply":"2025-05-04T14:05:02.401306Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 2. Using: google/flan-t5-small ","metadata":{}},{"cell_type":"markdown","source":"# 2.a Comparing Base models and peft+lora for google/flan-t5 model\n","metadata":{}},{"cell_type":"markdown","source":"* 2.1 Base Model Output","metadata":{}},{"cell_type":"code","source":"\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration, Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq\n# Step 2: Load tokenizer and model\nmodel_name = \"google/flan-t5-small\"\ntokenizer = T5Tokenizer.from_pretrained(model_name)\nmodel = T5ForConditionalGeneration.from_pretrained(model_name)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T00:08:52.132239Z","iopub.execute_input":"2025-05-05T00:08:52.133247Z","iopub.status.idle":"2025-05-05T00:08:52.869643Z","shell.execute_reply.started":"2025-05-05T00:08:52.133213Z","shell.execute_reply":"2025-05-05T00:08:52.868699Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"\n# Input example\ninput_text = \"\"\"Edward: Rachel, I think I'm in love with Bella.\\nRachel: Don't say anything else.\\nEdward: What do you mean??\\nRachel: Open your fu**ing door.. I'm outside\n\"\"\"\ninputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n\n# Generate summary\n\noutput_ids = model.generate(inputs[\"input_ids\"], max_length=128, num_beams=4)\nsummary_base_model = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n\nprint(\"Generated Summary:\", summary_base_model)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T00:08:52.871091Z","iopub.execute_input":"2025-05-05T00:08:52.871382Z","iopub.status.idle":"2025-05-05T00:08:53.349367Z","shell.execute_reply.started":"2025-05-05T00:08:52.871353Z","shell.execute_reply":"2025-05-05T00:08:53.348455Z"}},"outputs":[{"name":"stdout","text":"Generated Summary: Rachel is in love with Bella.\n","output_type":"stream"}],"execution_count":22},{"cell_type":"markdown","source":"* Fine-tuned mistral model on a dataset from kaggle ","metadata":{}},{"cell_type":"code","source":"\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration, Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq\nfrom datasets import Dataset\nimport pandas as pd\n\ndata=  pd.read_csv(\"/kaggle/input/samsum-dataset-text-summarization/samsum-test.csv\")\ndata = data.drop('id', axis=1)\ndata","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T00:08:53.793506Z","iopub.execute_input":"2025-05-05T00:08:53.793832Z","iopub.status.idle":"2025-05-05T00:08:53.831346Z","shell.execute_reply.started":"2025-05-05T00:08:53.793808Z","shell.execute_reply":"2025-05-05T00:08:53.830442Z"}},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"                                              dialogue  \\\n0    Hannah: Hey, do you have Betty's number?\\nAman...   \n1    Eric: MACHINE!\\r\\nRob: That's so gr8!\\r\\nEric:...   \n2    Lenny: Babe, can you help me with something?\\r...   \n3    Will: hey babe, what do you want for dinner to...   \n4    Ollie: Hi , are you in Warsaw\\r\\nJane: yes, ju...   \n..                                                 ...   \n814  Alex: Were you able to attend Friday night's b...   \n815  Jamilla: remember that the audition starts at ...   \n816  Marta: <file_gif>\\r\\nMarta: Sorry girls, I cli...   \n817  Cora: Have you heard how much fuss British med...   \n818  Rachel: <file_other>\\r\\nRachel: Top 50 Best Fi...   \n\n                                               summary  \n0    Hannah needs Betty's number but Amanda doesn't...  \n1    Eric and Rob are going to watch a stand-up on ...  \n2    Lenny can't decide which trousers to buy. Bob ...  \n3    Emma will be home soon and she will let Will k...  \n4    Jane is in Warsaw. Ollie and Jane has a party....  \n..                                                 ...  \n814  Benjamin didn't come to see a basketball game ...  \n815      The audition starts at 7.30 P.M. in Antena 3.  \n816                    Marta sent a file accidentally,  \n817  There was a meet-and-greet with James Charles ...  \n818  Rachel sends a list of Top 50 films of 2018. J...  \n\n[819 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>dialogue</th>\n      <th>summary</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Hannah: Hey, do you have Betty's number?\\nAman...</td>\n      <td>Hannah needs Betty's number but Amanda doesn't...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Eric: MACHINE!\\r\\nRob: That's so gr8!\\r\\nEric:...</td>\n      <td>Eric and Rob are going to watch a stand-up on ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Lenny: Babe, can you help me with something?\\r...</td>\n      <td>Lenny can't decide which trousers to buy. Bob ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Will: hey babe, what do you want for dinner to...</td>\n      <td>Emma will be home soon and she will let Will k...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Ollie: Hi , are you in Warsaw\\r\\nJane: yes, ju...</td>\n      <td>Jane is in Warsaw. Ollie and Jane has a party....</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>814</th>\n      <td>Alex: Were you able to attend Friday night's b...</td>\n      <td>Benjamin didn't come to see a basketball game ...</td>\n    </tr>\n    <tr>\n      <th>815</th>\n      <td>Jamilla: remember that the audition starts at ...</td>\n      <td>The audition starts at 7.30 P.M. in Antena 3.</td>\n    </tr>\n    <tr>\n      <th>816</th>\n      <td>Marta: &lt;file_gif&gt;\\r\\nMarta: Sorry girls, I cli...</td>\n      <td>Marta sent a file accidentally,</td>\n    </tr>\n    <tr>\n      <th>817</th>\n      <td>Cora: Have you heard how much fuss British med...</td>\n      <td>There was a meet-and-greet with James Charles ...</td>\n    </tr>\n    <tr>\n      <th>818</th>\n      <td>Rachel: &lt;file_other&gt;\\r\\nRachel: Top 50 Best Fi...</td>\n      <td>Rachel sends a list of Top 50 films of 2018. J...</td>\n    </tr>\n  </tbody>\n</table>\n<p>819 rows × 2 columns</p>\n</div>"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"dataset = Dataset.from_pandas(data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T00:08:54.002149Z","iopub.execute_input":"2025-05-05T00:08:54.002950Z","iopub.status.idle":"2025-05-05T00:08:54.016043Z","shell.execute_reply.started":"2025-05-05T00:08:54.002926Z","shell.execute_reply":"2025-05-05T00:08:54.015115Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainer, Seq2SeqTrainingArguments\nfrom peft import get_peft_model, LoraConfig, TaskType\nfrom datasets import load_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T00:08:54.186384Z","iopub.execute_input":"2025-05-05T00:08:54.187057Z","iopub.status.idle":"2025-05-05T00:08:54.190999Z","shell.execute_reply.started":"2025-05-05T00:08:54.187033Z","shell.execute_reply":"2025-05-05T00:08:54.190155Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"peft_config = LoraConfig(\n    task_type=TaskType.SEQ_2_SEQ_LM,  # For T5 or BART\n    inference_mode=False,\n    r=8,\n    lora_alpha=32,\n    lora_dropout=0.1\n)\nmodel = get_peft_model(model, peft_config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T00:08:54.344057Z","iopub.execute_input":"2025-05-05T00:08:54.344339Z","iopub.status.idle":"2025-05-05T00:08:54.405188Z","shell.execute_reply.started":"2025-05-05T00:08:54.344318Z","shell.execute_reply":"2025-05-05T00:08:54.404441Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"def format_dialogue(text: str) -> str:\n    \"\"\"\n    Normalize and clean dialogue format:\n    - Capitalize speaker names.\n    - Remove bad punctuation.\n    - Ensure each utterance is on its own line.\n    \"\"\"\n    lines = text.strip().split(\"\\n\")\n    formatted = []\n    for line in lines:\n        if ':' in line:\n            speaker, message = line.split(\":\", 1)\n            speaker = speaker.strip().capitalize()\n            message = message.strip().replace(\"..\", \".\")\n            formatted.append(f\"{speaker}: {message}\")\n    return \"\\n\".join(formatted)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T00:08:55.040515Z","iopub.execute_input":"2025-05-05T00:08:55.040853Z","iopub.status.idle":"2025-05-05T00:08:55.046174Z","shell.execute_reply.started":"2025-05-05T00:08:55.040829Z","shell.execute_reply":"2025-05-05T00:08:55.045150Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"def preprocess(example):\n    formatted_dialogue = format_dialogue(example[\"dialogue\"])\n    summary = example[\"summary\"]\n    model_inputs = tokenizer(formatted_dialogue, max_length=512, truncation=True, padding=\"max_length\")\n    labels = tokenizer(summary, max_length=128, truncation=True, padding=\"max_length\")\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T17:43:54.098401Z","iopub.execute_input":"2025-05-10T17:43:54.099013Z","iopub.status.idle":"2025-05-10T17:43:54.103233Z","shell.execute_reply.started":"2025-05-10T17:43:54.098991Z","shell.execute_reply":"2025-05-10T17:43:54.102520Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"tokenized = dataset.map(preprocess)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T00:08:55.455292Z","iopub.execute_input":"2025-05-05T00:08:55.455593Z","iopub.status.idle":"2025-05-05T00:08:56.560695Z","shell.execute_reply.started":"2025-05-05T00:08:55.455572Z","shell.execute_reply":"2025-05-05T00:08:56.559875Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/819 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b03d60ddc4c4ad9a4aca753b39d694a"}},"metadata":{}}],"execution_count":29},{"cell_type":"code","source":"training_args = Seq2SeqTrainingArguments(\n    output_dir=\"./peft-flan-t5\",\n    per_device_train_batch_size=2,\n    num_train_epochs=5,\n    learning_rate=5e-4,\n    logging_dir=\"./logs\",\n    logging_steps=1,\n    save_total_limit=1,\n    save_strategy=\"no\",\n    report_to=\"none\",  # Disable wandb\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T00:08:56.561914Z","iopub.execute_input":"2025-05-05T00:08:56.562125Z","iopub.status.idle":"2025-05-05T00:08:56.598705Z","shell.execute_reply.started":"2025-05-05T00:08:56.562109Z","shell.execute_reply":"2025-05-05T00:08:56.598049Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"\n\n# Step 5: Trainer\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized,\n    tokenizer= tokenizer,\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T00:08:57.197327Z","iopub.execute_input":"2025-05-05T00:08:57.197592Z","iopub.status.idle":"2025-05-05T00:08:57.351070Z","shell.execute_reply.started":"2025-05-05T00:08:57.197576Z","shell.execute_reply":"2025-05-05T00:08:57.350083Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_31/1096077962.py:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n  trainer = Seq2SeqTrainer(\nNo label_names provided for model class `PeftModelForSeq2SeqLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"\n# Step 6: Train\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T00:08:57.426654Z","iopub.execute_input":"2025-05-05T00:08:57.427386Z","iopub.status.idle":"2025-05-05T00:13:22.750630Z","shell.execute_reply.started":"2025-05-05T00:08:57.427364Z","shell.execute_reply":"2025-05-05T00:13:22.749713Z"},"scrolled":true},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1025' max='1025' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1025/1025 04:24, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>40.263900</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>36.129000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>37.994800</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>39.994100</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>43.616400</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>40.249800</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>34.235500</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>40.931900</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>35.533400</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>37.069800</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>37.160100</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>39.503300</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>37.039800</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>37.656700</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>34.822900</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>34.521300</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>34.448200</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>29.069100</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>27.777500</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>28.416800</td>\n    </tr>\n    <tr>\n      <td>21</td>\n      <td>28.229300</td>\n    </tr>\n    <tr>\n      <td>22</td>\n      <td>28.038200</td>\n    </tr>\n    <tr>\n      <td>23</td>\n      <td>27.139900</td>\n    </tr>\n    <tr>\n      <td>24</td>\n      <td>23.821700</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>21.418100</td>\n    </tr>\n    <tr>\n      <td>26</td>\n      <td>22.992100</td>\n    </tr>\n    <tr>\n      <td>27</td>\n      <td>23.833800</td>\n    </tr>\n    <tr>\n      <td>28</td>\n      <td>23.086200</td>\n    </tr>\n    <tr>\n      <td>29</td>\n      <td>17.978600</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>17.494400</td>\n    </tr>\n    <tr>\n      <td>31</td>\n      <td>19.782200</td>\n    </tr>\n    <tr>\n      <td>32</td>\n      <td>18.485700</td>\n    </tr>\n    <tr>\n      <td>33</td>\n      <td>15.296200</td>\n    </tr>\n    <tr>\n      <td>34</td>\n      <td>17.188400</td>\n    </tr>\n    <tr>\n      <td>35</td>\n      <td>15.457400</td>\n    </tr>\n    <tr>\n      <td>36</td>\n      <td>14.846700</td>\n    </tr>\n    <tr>\n      <td>37</td>\n      <td>12.493300</td>\n    </tr>\n    <tr>\n      <td>38</td>\n      <td>8.887100</td>\n    </tr>\n    <tr>\n      <td>39</td>\n      <td>9.641000</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>9.737000</td>\n    </tr>\n    <tr>\n      <td>41</td>\n      <td>9.220700</td>\n    </tr>\n    <tr>\n      <td>42</td>\n      <td>8.074900</td>\n    </tr>\n    <tr>\n      <td>43</td>\n      <td>7.988300</td>\n    </tr>\n    <tr>\n      <td>44</td>\n      <td>7.383600</td>\n    </tr>\n    <tr>\n      <td>45</td>\n      <td>6.476200</td>\n    </tr>\n    <tr>\n      <td>46</td>\n      <td>6.763600</td>\n    </tr>\n    <tr>\n      <td>47</td>\n      <td>6.448900</td>\n    </tr>\n    <tr>\n      <td>48</td>\n      <td>6.192800</td>\n    </tr>\n    <tr>\n      <td>49</td>\n      <td>6.346900</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>5.952200</td>\n    </tr>\n    <tr>\n      <td>51</td>\n      <td>5.629400</td>\n    </tr>\n    <tr>\n      <td>52</td>\n      <td>5.665100</td>\n    </tr>\n    <tr>\n      <td>53</td>\n      <td>5.082100</td>\n    </tr>\n    <tr>\n      <td>54</td>\n      <td>5.106900</td>\n    </tr>\n    <tr>\n      <td>55</td>\n      <td>5.404400</td>\n    </tr>\n    <tr>\n      <td>56</td>\n      <td>4.946500</td>\n    </tr>\n    <tr>\n      <td>57</td>\n      <td>4.970900</td>\n    </tr>\n    <tr>\n      <td>58</td>\n      <td>4.949500</td>\n    </tr>\n    <tr>\n      <td>59</td>\n      <td>4.678000</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>4.641200</td>\n    </tr>\n    <tr>\n      <td>61</td>\n      <td>4.849300</td>\n    </tr>\n    <tr>\n      <td>62</td>\n      <td>4.799200</td>\n    </tr>\n    <tr>\n      <td>63</td>\n      <td>4.600800</td>\n    </tr>\n    <tr>\n      <td>64</td>\n      <td>4.770000</td>\n    </tr>\n    <tr>\n      <td>65</td>\n      <td>4.769500</td>\n    </tr>\n    <tr>\n      <td>66</td>\n      <td>4.799400</td>\n    </tr>\n    <tr>\n      <td>67</td>\n      <td>4.567300</td>\n    </tr>\n    <tr>\n      <td>68</td>\n      <td>4.317000</td>\n    </tr>\n    <tr>\n      <td>69</td>\n      <td>4.661200</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>4.160500</td>\n    </tr>\n    <tr>\n      <td>71</td>\n      <td>4.639400</td>\n    </tr>\n    <tr>\n      <td>72</td>\n      <td>4.141300</td>\n    </tr>\n    <tr>\n      <td>73</td>\n      <td>3.953000</td>\n    </tr>\n    <tr>\n      <td>74</td>\n      <td>4.333700</td>\n    </tr>\n    <tr>\n      <td>75</td>\n      <td>4.179700</td>\n    </tr>\n    <tr>\n      <td>76</td>\n      <td>4.295700</td>\n    </tr>\n    <tr>\n      <td>77</td>\n      <td>4.599900</td>\n    </tr>\n    <tr>\n      <td>78</td>\n      <td>4.428000</td>\n    </tr>\n    <tr>\n      <td>79</td>\n      <td>4.227100</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>4.329300</td>\n    </tr>\n    <tr>\n      <td>81</td>\n      <td>3.818800</td>\n    </tr>\n    <tr>\n      <td>82</td>\n      <td>4.089000</td>\n    </tr>\n    <tr>\n      <td>83</td>\n      <td>4.043100</td>\n    </tr>\n    <tr>\n      <td>84</td>\n      <td>4.356700</td>\n    </tr>\n    <tr>\n      <td>85</td>\n      <td>4.357300</td>\n    </tr>\n    <tr>\n      <td>86</td>\n      <td>4.383400</td>\n    </tr>\n    <tr>\n      <td>87</td>\n      <td>4.086500</td>\n    </tr>\n    <tr>\n      <td>88</td>\n      <td>4.127200</td>\n    </tr>\n    <tr>\n      <td>89</td>\n      <td>4.145300</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>4.185000</td>\n    </tr>\n    <tr>\n      <td>91</td>\n      <td>4.159200</td>\n    </tr>\n    <tr>\n      <td>92</td>\n      <td>4.180400</td>\n    </tr>\n    <tr>\n      <td>93</td>\n      <td>4.263900</td>\n    </tr>\n    <tr>\n      <td>94</td>\n      <td>3.787800</td>\n    </tr>\n    <tr>\n      <td>95</td>\n      <td>4.134200</td>\n    </tr>\n    <tr>\n      <td>96</td>\n      <td>4.122300</td>\n    </tr>\n    <tr>\n      <td>97</td>\n      <td>4.298900</td>\n    </tr>\n    <tr>\n      <td>98</td>\n      <td>4.166500</td>\n    </tr>\n    <tr>\n      <td>99</td>\n      <td>4.166600</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>3.922400</td>\n    </tr>\n    <tr>\n      <td>101</td>\n      <td>4.271200</td>\n    </tr>\n    <tr>\n      <td>102</td>\n      <td>3.761100</td>\n    </tr>\n    <tr>\n      <td>103</td>\n      <td>4.129700</td>\n    </tr>\n    <tr>\n      <td>104</td>\n      <td>4.190400</td>\n    </tr>\n    <tr>\n      <td>105</td>\n      <td>4.064300</td>\n    </tr>\n    <tr>\n      <td>106</td>\n      <td>4.069500</td>\n    </tr>\n    <tr>\n      <td>107</td>\n      <td>3.921300</td>\n    </tr>\n    <tr>\n      <td>108</td>\n      <td>3.826800</td>\n    </tr>\n    <tr>\n      <td>109</td>\n      <td>3.940500</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>3.852900</td>\n    </tr>\n    <tr>\n      <td>111</td>\n      <td>3.897200</td>\n    </tr>\n    <tr>\n      <td>112</td>\n      <td>4.021800</td>\n    </tr>\n    <tr>\n      <td>113</td>\n      <td>4.050600</td>\n    </tr>\n    <tr>\n      <td>114</td>\n      <td>4.001200</td>\n    </tr>\n    <tr>\n      <td>115</td>\n      <td>4.095100</td>\n    </tr>\n    <tr>\n      <td>116</td>\n      <td>3.710900</td>\n    </tr>\n    <tr>\n      <td>117</td>\n      <td>3.834700</td>\n    </tr>\n    <tr>\n      <td>118</td>\n      <td>3.813200</td>\n    </tr>\n    <tr>\n      <td>119</td>\n      <td>3.687800</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>3.802300</td>\n    </tr>\n    <tr>\n      <td>121</td>\n      <td>3.885200</td>\n    </tr>\n    <tr>\n      <td>122</td>\n      <td>3.749700</td>\n    </tr>\n    <tr>\n      <td>123</td>\n      <td>3.729400</td>\n    </tr>\n    <tr>\n      <td>124</td>\n      <td>3.900100</td>\n    </tr>\n    <tr>\n      <td>125</td>\n      <td>3.812100</td>\n    </tr>\n    <tr>\n      <td>126</td>\n      <td>3.873600</td>\n    </tr>\n    <tr>\n      <td>127</td>\n      <td>3.778300</td>\n    </tr>\n    <tr>\n      <td>128</td>\n      <td>3.615100</td>\n    </tr>\n    <tr>\n      <td>129</td>\n      <td>3.844300</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>3.500200</td>\n    </tr>\n    <tr>\n      <td>131</td>\n      <td>3.816700</td>\n    </tr>\n    <tr>\n      <td>132</td>\n      <td>3.684900</td>\n    </tr>\n    <tr>\n      <td>133</td>\n      <td>3.612500</td>\n    </tr>\n    <tr>\n      <td>134</td>\n      <td>3.695300</td>\n    </tr>\n    <tr>\n      <td>135</td>\n      <td>3.840300</td>\n    </tr>\n    <tr>\n      <td>136</td>\n      <td>3.772700</td>\n    </tr>\n    <tr>\n      <td>137</td>\n      <td>3.777300</td>\n    </tr>\n    <tr>\n      <td>138</td>\n      <td>3.789800</td>\n    </tr>\n    <tr>\n      <td>139</td>\n      <td>3.455400</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>3.574700</td>\n    </tr>\n    <tr>\n      <td>141</td>\n      <td>3.692100</td>\n    </tr>\n    <tr>\n      <td>142</td>\n      <td>3.633100</td>\n    </tr>\n    <tr>\n      <td>143</td>\n      <td>3.592500</td>\n    </tr>\n    <tr>\n      <td>144</td>\n      <td>3.672100</td>\n    </tr>\n    <tr>\n      <td>145</td>\n      <td>3.572100</td>\n    </tr>\n    <tr>\n      <td>146</td>\n      <td>3.697400</td>\n    </tr>\n    <tr>\n      <td>147</td>\n      <td>3.198800</td>\n    </tr>\n    <tr>\n      <td>148</td>\n      <td>3.642100</td>\n    </tr>\n    <tr>\n      <td>149</td>\n      <td>3.663000</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>3.547100</td>\n    </tr>\n    <tr>\n      <td>151</td>\n      <td>3.461100</td>\n    </tr>\n    <tr>\n      <td>152</td>\n      <td>3.566900</td>\n    </tr>\n    <tr>\n      <td>153</td>\n      <td>3.380000</td>\n    </tr>\n    <tr>\n      <td>154</td>\n      <td>3.373900</td>\n    </tr>\n    <tr>\n      <td>155</td>\n      <td>3.406700</td>\n    </tr>\n    <tr>\n      <td>156</td>\n      <td>3.339100</td>\n    </tr>\n    <tr>\n      <td>157</td>\n      <td>3.539700</td>\n    </tr>\n    <tr>\n      <td>158</td>\n      <td>3.287800</td>\n    </tr>\n    <tr>\n      <td>159</td>\n      <td>3.323800</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>3.174900</td>\n    </tr>\n    <tr>\n      <td>161</td>\n      <td>3.181700</td>\n    </tr>\n    <tr>\n      <td>162</td>\n      <td>3.218500</td>\n    </tr>\n    <tr>\n      <td>163</td>\n      <td>3.320400</td>\n    </tr>\n    <tr>\n      <td>164</td>\n      <td>3.468900</td>\n    </tr>\n    <tr>\n      <td>165</td>\n      <td>3.198500</td>\n    </tr>\n    <tr>\n      <td>166</td>\n      <td>3.226300</td>\n    </tr>\n    <tr>\n      <td>167</td>\n      <td>3.267400</td>\n    </tr>\n    <tr>\n      <td>168</td>\n      <td>3.147400</td>\n    </tr>\n    <tr>\n      <td>169</td>\n      <td>3.255100</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>3.239400</td>\n    </tr>\n    <tr>\n      <td>171</td>\n      <td>3.280500</td>\n    </tr>\n    <tr>\n      <td>172</td>\n      <td>3.175300</td>\n    </tr>\n    <tr>\n      <td>173</td>\n      <td>3.172500</td>\n    </tr>\n    <tr>\n      <td>174</td>\n      <td>3.186800</td>\n    </tr>\n    <tr>\n      <td>175</td>\n      <td>3.279300</td>\n    </tr>\n    <tr>\n      <td>176</td>\n      <td>3.225500</td>\n    </tr>\n    <tr>\n      <td>177</td>\n      <td>3.196800</td>\n    </tr>\n    <tr>\n      <td>178</td>\n      <td>3.161700</td>\n    </tr>\n    <tr>\n      <td>179</td>\n      <td>3.283000</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>3.197100</td>\n    </tr>\n    <tr>\n      <td>181</td>\n      <td>3.193800</td>\n    </tr>\n    <tr>\n      <td>182</td>\n      <td>3.110400</td>\n    </tr>\n    <tr>\n      <td>183</td>\n      <td>3.073600</td>\n    </tr>\n    <tr>\n      <td>184</td>\n      <td>2.924600</td>\n    </tr>\n    <tr>\n      <td>185</td>\n      <td>3.021700</td>\n    </tr>\n    <tr>\n      <td>186</td>\n      <td>3.048000</td>\n    </tr>\n    <tr>\n      <td>187</td>\n      <td>3.057700</td>\n    </tr>\n    <tr>\n      <td>188</td>\n      <td>3.134100</td>\n    </tr>\n    <tr>\n      <td>189</td>\n      <td>3.066100</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>2.950100</td>\n    </tr>\n    <tr>\n      <td>191</td>\n      <td>3.235200</td>\n    </tr>\n    <tr>\n      <td>192</td>\n      <td>3.012400</td>\n    </tr>\n    <tr>\n      <td>193</td>\n      <td>2.990600</td>\n    </tr>\n    <tr>\n      <td>194</td>\n      <td>2.862200</td>\n    </tr>\n    <tr>\n      <td>195</td>\n      <td>3.216700</td>\n    </tr>\n    <tr>\n      <td>196</td>\n      <td>3.131500</td>\n    </tr>\n    <tr>\n      <td>197</td>\n      <td>2.950100</td>\n    </tr>\n    <tr>\n      <td>198</td>\n      <td>3.086100</td>\n    </tr>\n    <tr>\n      <td>199</td>\n      <td>3.121900</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>3.016900</td>\n    </tr>\n    <tr>\n      <td>201</td>\n      <td>3.016200</td>\n    </tr>\n    <tr>\n      <td>202</td>\n      <td>3.043300</td>\n    </tr>\n    <tr>\n      <td>203</td>\n      <td>2.887500</td>\n    </tr>\n    <tr>\n      <td>204</td>\n      <td>3.077900</td>\n    </tr>\n    <tr>\n      <td>205</td>\n      <td>3.028400</td>\n    </tr>\n    <tr>\n      <td>206</td>\n      <td>2.917100</td>\n    </tr>\n    <tr>\n      <td>207</td>\n      <td>2.954500</td>\n    </tr>\n    <tr>\n      <td>208</td>\n      <td>2.996200</td>\n    </tr>\n    <tr>\n      <td>209</td>\n      <td>2.905200</td>\n    </tr>\n    <tr>\n      <td>210</td>\n      <td>2.945500</td>\n    </tr>\n    <tr>\n      <td>211</td>\n      <td>2.741400</td>\n    </tr>\n    <tr>\n      <td>212</td>\n      <td>2.784800</td>\n    </tr>\n    <tr>\n      <td>213</td>\n      <td>2.920800</td>\n    </tr>\n    <tr>\n      <td>214</td>\n      <td>2.821900</td>\n    </tr>\n    <tr>\n      <td>215</td>\n      <td>2.848300</td>\n    </tr>\n    <tr>\n      <td>216</td>\n      <td>2.938700</td>\n    </tr>\n    <tr>\n      <td>217</td>\n      <td>2.769700</td>\n    </tr>\n    <tr>\n      <td>218</td>\n      <td>2.892500</td>\n    </tr>\n    <tr>\n      <td>219</td>\n      <td>3.058500</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>2.735200</td>\n    </tr>\n    <tr>\n      <td>221</td>\n      <td>2.940700</td>\n    </tr>\n    <tr>\n      <td>222</td>\n      <td>2.849000</td>\n    </tr>\n    <tr>\n      <td>223</td>\n      <td>2.848600</td>\n    </tr>\n    <tr>\n      <td>224</td>\n      <td>2.631400</td>\n    </tr>\n    <tr>\n      <td>225</td>\n      <td>2.799700</td>\n    </tr>\n    <tr>\n      <td>226</td>\n      <td>2.844700</td>\n    </tr>\n    <tr>\n      <td>227</td>\n      <td>2.751400</td>\n    </tr>\n    <tr>\n      <td>228</td>\n      <td>2.651400</td>\n    </tr>\n    <tr>\n      <td>229</td>\n      <td>2.818800</td>\n    </tr>\n    <tr>\n      <td>230</td>\n      <td>2.918700</td>\n    </tr>\n    <tr>\n      <td>231</td>\n      <td>2.854200</td>\n    </tr>\n    <tr>\n      <td>232</td>\n      <td>2.840400</td>\n    </tr>\n    <tr>\n      <td>233</td>\n      <td>2.775300</td>\n    </tr>\n    <tr>\n      <td>234</td>\n      <td>2.733600</td>\n    </tr>\n    <tr>\n      <td>235</td>\n      <td>2.796300</td>\n    </tr>\n    <tr>\n      <td>236</td>\n      <td>2.640100</td>\n    </tr>\n    <tr>\n      <td>237</td>\n      <td>2.796300</td>\n    </tr>\n    <tr>\n      <td>238</td>\n      <td>2.628800</td>\n    </tr>\n    <tr>\n      <td>239</td>\n      <td>2.646400</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>2.729000</td>\n    </tr>\n    <tr>\n      <td>241</td>\n      <td>2.845300</td>\n    </tr>\n    <tr>\n      <td>242</td>\n      <td>2.770500</td>\n    </tr>\n    <tr>\n      <td>243</td>\n      <td>2.763200</td>\n    </tr>\n    <tr>\n      <td>244</td>\n      <td>2.703900</td>\n    </tr>\n    <tr>\n      <td>245</td>\n      <td>2.803500</td>\n    </tr>\n    <tr>\n      <td>246</td>\n      <td>2.667600</td>\n    </tr>\n    <tr>\n      <td>247</td>\n      <td>2.638400</td>\n    </tr>\n    <tr>\n      <td>248</td>\n      <td>2.707400</td>\n    </tr>\n    <tr>\n      <td>249</td>\n      <td>2.647600</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>2.709900</td>\n    </tr>\n    <tr>\n      <td>251</td>\n      <td>2.592100</td>\n    </tr>\n    <tr>\n      <td>252</td>\n      <td>2.656300</td>\n    </tr>\n    <tr>\n      <td>253</td>\n      <td>2.557000</td>\n    </tr>\n    <tr>\n      <td>254</td>\n      <td>2.696800</td>\n    </tr>\n    <tr>\n      <td>255</td>\n      <td>2.618600</td>\n    </tr>\n    <tr>\n      <td>256</td>\n      <td>2.695000</td>\n    </tr>\n    <tr>\n      <td>257</td>\n      <td>2.638800</td>\n    </tr>\n    <tr>\n      <td>258</td>\n      <td>2.492200</td>\n    </tr>\n    <tr>\n      <td>259</td>\n      <td>2.789400</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>2.573300</td>\n    </tr>\n    <tr>\n      <td>261</td>\n      <td>2.609000</td>\n    </tr>\n    <tr>\n      <td>262</td>\n      <td>2.565300</td>\n    </tr>\n    <tr>\n      <td>263</td>\n      <td>2.591900</td>\n    </tr>\n    <tr>\n      <td>264</td>\n      <td>2.637500</td>\n    </tr>\n    <tr>\n      <td>265</td>\n      <td>2.646100</td>\n    </tr>\n    <tr>\n      <td>266</td>\n      <td>2.603600</td>\n    </tr>\n    <tr>\n      <td>267</td>\n      <td>2.593800</td>\n    </tr>\n    <tr>\n      <td>268</td>\n      <td>2.591900</td>\n    </tr>\n    <tr>\n      <td>269</td>\n      <td>2.565400</td>\n    </tr>\n    <tr>\n      <td>270</td>\n      <td>2.464800</td>\n    </tr>\n    <tr>\n      <td>271</td>\n      <td>2.796800</td>\n    </tr>\n    <tr>\n      <td>272</td>\n      <td>2.461700</td>\n    </tr>\n    <tr>\n      <td>273</td>\n      <td>2.654500</td>\n    </tr>\n    <tr>\n      <td>274</td>\n      <td>2.674200</td>\n    </tr>\n    <tr>\n      <td>275</td>\n      <td>2.575400</td>\n    </tr>\n    <tr>\n      <td>276</td>\n      <td>2.641100</td>\n    </tr>\n    <tr>\n      <td>277</td>\n      <td>2.685600</td>\n    </tr>\n    <tr>\n      <td>278</td>\n      <td>2.538700</td>\n    </tr>\n    <tr>\n      <td>279</td>\n      <td>2.527100</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>2.482600</td>\n    </tr>\n    <tr>\n      <td>281</td>\n      <td>2.459700</td>\n    </tr>\n    <tr>\n      <td>282</td>\n      <td>2.602000</td>\n    </tr>\n    <tr>\n      <td>283</td>\n      <td>2.514200</td>\n    </tr>\n    <tr>\n      <td>284</td>\n      <td>2.533400</td>\n    </tr>\n    <tr>\n      <td>285</td>\n      <td>2.531500</td>\n    </tr>\n    <tr>\n      <td>286</td>\n      <td>2.510100</td>\n    </tr>\n    <tr>\n      <td>287</td>\n      <td>2.562200</td>\n    </tr>\n    <tr>\n      <td>288</td>\n      <td>2.457000</td>\n    </tr>\n    <tr>\n      <td>289</td>\n      <td>2.523600</td>\n    </tr>\n    <tr>\n      <td>290</td>\n      <td>2.502700</td>\n    </tr>\n    <tr>\n      <td>291</td>\n      <td>2.500100</td>\n    </tr>\n    <tr>\n      <td>292</td>\n      <td>2.640400</td>\n    </tr>\n    <tr>\n      <td>293</td>\n      <td>2.547400</td>\n    </tr>\n    <tr>\n      <td>294</td>\n      <td>2.388900</td>\n    </tr>\n    <tr>\n      <td>295</td>\n      <td>2.506600</td>\n    </tr>\n    <tr>\n      <td>296</td>\n      <td>2.477200</td>\n    </tr>\n    <tr>\n      <td>297</td>\n      <td>2.508500</td>\n    </tr>\n    <tr>\n      <td>298</td>\n      <td>2.422800</td>\n    </tr>\n    <tr>\n      <td>299</td>\n      <td>2.558600</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>2.454800</td>\n    </tr>\n    <tr>\n      <td>301</td>\n      <td>2.467100</td>\n    </tr>\n    <tr>\n      <td>302</td>\n      <td>2.487600</td>\n    </tr>\n    <tr>\n      <td>303</td>\n      <td>2.529900</td>\n    </tr>\n    <tr>\n      <td>304</td>\n      <td>2.530200</td>\n    </tr>\n    <tr>\n      <td>305</td>\n      <td>2.465600</td>\n    </tr>\n    <tr>\n      <td>306</td>\n      <td>2.485000</td>\n    </tr>\n    <tr>\n      <td>307</td>\n      <td>2.612000</td>\n    </tr>\n    <tr>\n      <td>308</td>\n      <td>2.518200</td>\n    </tr>\n    <tr>\n      <td>309</td>\n      <td>2.589300</td>\n    </tr>\n    <tr>\n      <td>310</td>\n      <td>2.408000</td>\n    </tr>\n    <tr>\n      <td>311</td>\n      <td>2.454700</td>\n    </tr>\n    <tr>\n      <td>312</td>\n      <td>2.469800</td>\n    </tr>\n    <tr>\n      <td>313</td>\n      <td>2.487100</td>\n    </tr>\n    <tr>\n      <td>314</td>\n      <td>2.490600</td>\n    </tr>\n    <tr>\n      <td>315</td>\n      <td>2.487400</td>\n    </tr>\n    <tr>\n      <td>316</td>\n      <td>2.461500</td>\n    </tr>\n    <tr>\n      <td>317</td>\n      <td>2.423400</td>\n    </tr>\n    <tr>\n      <td>318</td>\n      <td>2.361900</td>\n    </tr>\n    <tr>\n      <td>319</td>\n      <td>2.552800</td>\n    </tr>\n    <tr>\n      <td>320</td>\n      <td>2.264200</td>\n    </tr>\n    <tr>\n      <td>321</td>\n      <td>2.409300</td>\n    </tr>\n    <tr>\n      <td>322</td>\n      <td>2.389800</td>\n    </tr>\n    <tr>\n      <td>323</td>\n      <td>2.389000</td>\n    </tr>\n    <tr>\n      <td>324</td>\n      <td>2.453400</td>\n    </tr>\n    <tr>\n      <td>325</td>\n      <td>2.460100</td>\n    </tr>\n    <tr>\n      <td>326</td>\n      <td>2.306900</td>\n    </tr>\n    <tr>\n      <td>327</td>\n      <td>2.531600</td>\n    </tr>\n    <tr>\n      <td>328</td>\n      <td>2.531200</td>\n    </tr>\n    <tr>\n      <td>329</td>\n      <td>2.426800</td>\n    </tr>\n    <tr>\n      <td>330</td>\n      <td>2.471500</td>\n    </tr>\n    <tr>\n      <td>331</td>\n      <td>2.512200</td>\n    </tr>\n    <tr>\n      <td>332</td>\n      <td>2.395400</td>\n    </tr>\n    <tr>\n      <td>333</td>\n      <td>2.439100</td>\n    </tr>\n    <tr>\n      <td>334</td>\n      <td>2.312800</td>\n    </tr>\n    <tr>\n      <td>335</td>\n      <td>2.301100</td>\n    </tr>\n    <tr>\n      <td>336</td>\n      <td>2.578100</td>\n    </tr>\n    <tr>\n      <td>337</td>\n      <td>2.364900</td>\n    </tr>\n    <tr>\n      <td>338</td>\n      <td>2.425500</td>\n    </tr>\n    <tr>\n      <td>339</td>\n      <td>2.349000</td>\n    </tr>\n    <tr>\n      <td>340</td>\n      <td>2.383200</td>\n    </tr>\n    <tr>\n      <td>341</td>\n      <td>2.379300</td>\n    </tr>\n    <tr>\n      <td>342</td>\n      <td>2.368600</td>\n    </tr>\n    <tr>\n      <td>343</td>\n      <td>2.368300</td>\n    </tr>\n    <tr>\n      <td>344</td>\n      <td>2.374200</td>\n    </tr>\n    <tr>\n      <td>345</td>\n      <td>2.399400</td>\n    </tr>\n    <tr>\n      <td>346</td>\n      <td>2.447000</td>\n    </tr>\n    <tr>\n      <td>347</td>\n      <td>2.431400</td>\n    </tr>\n    <tr>\n      <td>348</td>\n      <td>2.347600</td>\n    </tr>\n    <tr>\n      <td>349</td>\n      <td>2.352000</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>2.241400</td>\n    </tr>\n    <tr>\n      <td>351</td>\n      <td>2.275100</td>\n    </tr>\n    <tr>\n      <td>352</td>\n      <td>2.396500</td>\n    </tr>\n    <tr>\n      <td>353</td>\n      <td>2.321300</td>\n    </tr>\n    <tr>\n      <td>354</td>\n      <td>2.340800</td>\n    </tr>\n    <tr>\n      <td>355</td>\n      <td>2.443500</td>\n    </tr>\n    <tr>\n      <td>356</td>\n      <td>2.255500</td>\n    </tr>\n    <tr>\n      <td>357</td>\n      <td>2.397000</td>\n    </tr>\n    <tr>\n      <td>358</td>\n      <td>2.351600</td>\n    </tr>\n    <tr>\n      <td>359</td>\n      <td>2.422500</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>2.295000</td>\n    </tr>\n    <tr>\n      <td>361</td>\n      <td>2.403800</td>\n    </tr>\n    <tr>\n      <td>362</td>\n      <td>2.249700</td>\n    </tr>\n    <tr>\n      <td>363</td>\n      <td>2.341300</td>\n    </tr>\n    <tr>\n      <td>364</td>\n      <td>2.277300</td>\n    </tr>\n    <tr>\n      <td>365</td>\n      <td>2.480500</td>\n    </tr>\n    <tr>\n      <td>366</td>\n      <td>2.475900</td>\n    </tr>\n    <tr>\n      <td>367</td>\n      <td>2.341600</td>\n    </tr>\n    <tr>\n      <td>368</td>\n      <td>2.368900</td>\n    </tr>\n    <tr>\n      <td>369</td>\n      <td>2.244000</td>\n    </tr>\n    <tr>\n      <td>370</td>\n      <td>2.371500</td>\n    </tr>\n    <tr>\n      <td>371</td>\n      <td>2.382500</td>\n    </tr>\n    <tr>\n      <td>372</td>\n      <td>2.432300</td>\n    </tr>\n    <tr>\n      <td>373</td>\n      <td>2.398800</td>\n    </tr>\n    <tr>\n      <td>374</td>\n      <td>2.319200</td>\n    </tr>\n    <tr>\n      <td>375</td>\n      <td>2.252200</td>\n    </tr>\n    <tr>\n      <td>376</td>\n      <td>2.520200</td>\n    </tr>\n    <tr>\n      <td>377</td>\n      <td>2.191600</td>\n    </tr>\n    <tr>\n      <td>378</td>\n      <td>2.310000</td>\n    </tr>\n    <tr>\n      <td>379</td>\n      <td>2.272500</td>\n    </tr>\n    <tr>\n      <td>380</td>\n      <td>2.304300</td>\n    </tr>\n    <tr>\n      <td>381</td>\n      <td>2.271400</td>\n    </tr>\n    <tr>\n      <td>382</td>\n      <td>2.344000</td>\n    </tr>\n    <tr>\n      <td>383</td>\n      <td>2.462400</td>\n    </tr>\n    <tr>\n      <td>384</td>\n      <td>2.404000</td>\n    </tr>\n    <tr>\n      <td>385</td>\n      <td>2.334300</td>\n    </tr>\n    <tr>\n      <td>386</td>\n      <td>2.239300</td>\n    </tr>\n    <tr>\n      <td>387</td>\n      <td>2.264800</td>\n    </tr>\n    <tr>\n      <td>388</td>\n      <td>2.349500</td>\n    </tr>\n    <tr>\n      <td>389</td>\n      <td>2.288800</td>\n    </tr>\n    <tr>\n      <td>390</td>\n      <td>2.432800</td>\n    </tr>\n    <tr>\n      <td>391</td>\n      <td>2.266300</td>\n    </tr>\n    <tr>\n      <td>392</td>\n      <td>2.420200</td>\n    </tr>\n    <tr>\n      <td>393</td>\n      <td>2.310500</td>\n    </tr>\n    <tr>\n      <td>394</td>\n      <td>2.309600</td>\n    </tr>\n    <tr>\n      <td>395</td>\n      <td>2.339900</td>\n    </tr>\n    <tr>\n      <td>396</td>\n      <td>2.326500</td>\n    </tr>\n    <tr>\n      <td>397</td>\n      <td>2.271000</td>\n    </tr>\n    <tr>\n      <td>398</td>\n      <td>2.357900</td>\n    </tr>\n    <tr>\n      <td>399</td>\n      <td>2.326600</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>2.375100</td>\n    </tr>\n    <tr>\n      <td>401</td>\n      <td>2.222200</td>\n    </tr>\n    <tr>\n      <td>402</td>\n      <td>2.179000</td>\n    </tr>\n    <tr>\n      <td>403</td>\n      <td>2.358900</td>\n    </tr>\n    <tr>\n      <td>404</td>\n      <td>2.351500</td>\n    </tr>\n    <tr>\n      <td>405</td>\n      <td>2.219000</td>\n    </tr>\n    <tr>\n      <td>406</td>\n      <td>2.270700</td>\n    </tr>\n    <tr>\n      <td>407</td>\n      <td>2.242000</td>\n    </tr>\n    <tr>\n      <td>408</td>\n      <td>2.212700</td>\n    </tr>\n    <tr>\n      <td>409</td>\n      <td>2.296100</td>\n    </tr>\n    <tr>\n      <td>410</td>\n      <td>2.410300</td>\n    </tr>\n    <tr>\n      <td>411</td>\n      <td>2.310000</td>\n    </tr>\n    <tr>\n      <td>412</td>\n      <td>2.341100</td>\n    </tr>\n    <tr>\n      <td>413</td>\n      <td>2.319200</td>\n    </tr>\n    <tr>\n      <td>414</td>\n      <td>2.153000</td>\n    </tr>\n    <tr>\n      <td>415</td>\n      <td>2.333200</td>\n    </tr>\n    <tr>\n      <td>416</td>\n      <td>2.208800</td>\n    </tr>\n    <tr>\n      <td>417</td>\n      <td>2.193000</td>\n    </tr>\n    <tr>\n      <td>418</td>\n      <td>2.468900</td>\n    </tr>\n    <tr>\n      <td>419</td>\n      <td>2.271700</td>\n    </tr>\n    <tr>\n      <td>420</td>\n      <td>2.368500</td>\n    </tr>\n    <tr>\n      <td>421</td>\n      <td>2.230700</td>\n    </tr>\n    <tr>\n      <td>422</td>\n      <td>2.120500</td>\n    </tr>\n    <tr>\n      <td>423</td>\n      <td>2.217100</td>\n    </tr>\n    <tr>\n      <td>424</td>\n      <td>2.236600</td>\n    </tr>\n    <tr>\n      <td>425</td>\n      <td>2.331100</td>\n    </tr>\n    <tr>\n      <td>426</td>\n      <td>2.271000</td>\n    </tr>\n    <tr>\n      <td>427</td>\n      <td>2.271900</td>\n    </tr>\n    <tr>\n      <td>428</td>\n      <td>2.236400</td>\n    </tr>\n    <tr>\n      <td>429</td>\n      <td>2.324200</td>\n    </tr>\n    <tr>\n      <td>430</td>\n      <td>2.272700</td>\n    </tr>\n    <tr>\n      <td>431</td>\n      <td>2.215900</td>\n    </tr>\n    <tr>\n      <td>432</td>\n      <td>2.199600</td>\n    </tr>\n    <tr>\n      <td>433</td>\n      <td>2.311700</td>\n    </tr>\n    <tr>\n      <td>434</td>\n      <td>2.268100</td>\n    </tr>\n    <tr>\n      <td>435</td>\n      <td>2.305900</td>\n    </tr>\n    <tr>\n      <td>436</td>\n      <td>2.219200</td>\n    </tr>\n    <tr>\n      <td>437</td>\n      <td>2.285900</td>\n    </tr>\n    <tr>\n      <td>438</td>\n      <td>2.291700</td>\n    </tr>\n    <tr>\n      <td>439</td>\n      <td>2.299300</td>\n    </tr>\n    <tr>\n      <td>440</td>\n      <td>2.308600</td>\n    </tr>\n    <tr>\n      <td>441</td>\n      <td>2.247900</td>\n    </tr>\n    <tr>\n      <td>442</td>\n      <td>2.249900</td>\n    </tr>\n    <tr>\n      <td>443</td>\n      <td>2.258700</td>\n    </tr>\n    <tr>\n      <td>444</td>\n      <td>2.352400</td>\n    </tr>\n    <tr>\n      <td>445</td>\n      <td>2.264000</td>\n    </tr>\n    <tr>\n      <td>446</td>\n      <td>2.227100</td>\n    </tr>\n    <tr>\n      <td>447</td>\n      <td>2.274900</td>\n    </tr>\n    <tr>\n      <td>448</td>\n      <td>2.291600</td>\n    </tr>\n    <tr>\n      <td>449</td>\n      <td>2.206200</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>2.118500</td>\n    </tr>\n    <tr>\n      <td>451</td>\n      <td>2.250500</td>\n    </tr>\n    <tr>\n      <td>452</td>\n      <td>2.225000</td>\n    </tr>\n    <tr>\n      <td>453</td>\n      <td>2.359700</td>\n    </tr>\n    <tr>\n      <td>454</td>\n      <td>2.132400</td>\n    </tr>\n    <tr>\n      <td>455</td>\n      <td>2.241000</td>\n    </tr>\n    <tr>\n      <td>456</td>\n      <td>2.149200</td>\n    </tr>\n    <tr>\n      <td>457</td>\n      <td>2.210100</td>\n    </tr>\n    <tr>\n      <td>458</td>\n      <td>2.175100</td>\n    </tr>\n    <tr>\n      <td>459</td>\n      <td>2.212000</td>\n    </tr>\n    <tr>\n      <td>460</td>\n      <td>2.171000</td>\n    </tr>\n    <tr>\n      <td>461</td>\n      <td>2.230200</td>\n    </tr>\n    <tr>\n      <td>462</td>\n      <td>2.282200</td>\n    </tr>\n    <tr>\n      <td>463</td>\n      <td>2.169500</td>\n    </tr>\n    <tr>\n      <td>464</td>\n      <td>2.235700</td>\n    </tr>\n    <tr>\n      <td>465</td>\n      <td>2.265100</td>\n    </tr>\n    <tr>\n      <td>466</td>\n      <td>2.135000</td>\n    </tr>\n    <tr>\n      <td>467</td>\n      <td>2.228300</td>\n    </tr>\n    <tr>\n      <td>468</td>\n      <td>2.216500</td>\n    </tr>\n    <tr>\n      <td>469</td>\n      <td>2.378100</td>\n    </tr>\n    <tr>\n      <td>470</td>\n      <td>2.162400</td>\n    </tr>\n    <tr>\n      <td>471</td>\n      <td>2.256900</td>\n    </tr>\n    <tr>\n      <td>472</td>\n      <td>2.110700</td>\n    </tr>\n    <tr>\n      <td>473</td>\n      <td>2.214000</td>\n    </tr>\n    <tr>\n      <td>474</td>\n      <td>2.385800</td>\n    </tr>\n    <tr>\n      <td>475</td>\n      <td>2.167100</td>\n    </tr>\n    <tr>\n      <td>476</td>\n      <td>2.337300</td>\n    </tr>\n    <tr>\n      <td>477</td>\n      <td>2.262900</td>\n    </tr>\n    <tr>\n      <td>478</td>\n      <td>2.161600</td>\n    </tr>\n    <tr>\n      <td>479</td>\n      <td>2.156700</td>\n    </tr>\n    <tr>\n      <td>480</td>\n      <td>2.127300</td>\n    </tr>\n    <tr>\n      <td>481</td>\n      <td>2.133900</td>\n    </tr>\n    <tr>\n      <td>482</td>\n      <td>2.142200</td>\n    </tr>\n    <tr>\n      <td>483</td>\n      <td>2.203600</td>\n    </tr>\n    <tr>\n      <td>484</td>\n      <td>2.189200</td>\n    </tr>\n    <tr>\n      <td>485</td>\n      <td>2.218100</td>\n    </tr>\n    <tr>\n      <td>486</td>\n      <td>2.293900</td>\n    </tr>\n    <tr>\n      <td>487</td>\n      <td>2.375000</td>\n    </tr>\n    <tr>\n      <td>488</td>\n      <td>2.184300</td>\n    </tr>\n    <tr>\n      <td>489</td>\n      <td>2.368100</td>\n    </tr>\n    <tr>\n      <td>490</td>\n      <td>2.208100</td>\n    </tr>\n    <tr>\n      <td>491</td>\n      <td>2.290500</td>\n    </tr>\n    <tr>\n      <td>492</td>\n      <td>2.161300</td>\n    </tr>\n    <tr>\n      <td>493</td>\n      <td>2.255000</td>\n    </tr>\n    <tr>\n      <td>494</td>\n      <td>2.129700</td>\n    </tr>\n    <tr>\n      <td>495</td>\n      <td>2.179400</td>\n    </tr>\n    <tr>\n      <td>496</td>\n      <td>2.193400</td>\n    </tr>\n    <tr>\n      <td>497</td>\n      <td>2.245000</td>\n    </tr>\n    <tr>\n      <td>498</td>\n      <td>2.318100</td>\n    </tr>\n    <tr>\n      <td>499</td>\n      <td>2.182500</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>2.289300</td>\n    </tr>\n    <tr>\n      <td>501</td>\n      <td>2.067300</td>\n    </tr>\n    <tr>\n      <td>502</td>\n      <td>2.244000</td>\n    </tr>\n    <tr>\n      <td>503</td>\n      <td>2.206600</td>\n    </tr>\n    <tr>\n      <td>504</td>\n      <td>2.393000</td>\n    </tr>\n    <tr>\n      <td>505</td>\n      <td>2.243200</td>\n    </tr>\n    <tr>\n      <td>506</td>\n      <td>2.197500</td>\n    </tr>\n    <tr>\n      <td>507</td>\n      <td>2.142300</td>\n    </tr>\n    <tr>\n      <td>508</td>\n      <td>2.142900</td>\n    </tr>\n    <tr>\n      <td>509</td>\n      <td>2.279500</td>\n    </tr>\n    <tr>\n      <td>510</td>\n      <td>2.246400</td>\n    </tr>\n    <tr>\n      <td>511</td>\n      <td>2.196100</td>\n    </tr>\n    <tr>\n      <td>512</td>\n      <td>2.373000</td>\n    </tr>\n    <tr>\n      <td>513</td>\n      <td>2.257000</td>\n    </tr>\n    <tr>\n      <td>514</td>\n      <td>2.195600</td>\n    </tr>\n    <tr>\n      <td>515</td>\n      <td>2.221000</td>\n    </tr>\n    <tr>\n      <td>516</td>\n      <td>2.124500</td>\n    </tr>\n    <tr>\n      <td>517</td>\n      <td>2.394700</td>\n    </tr>\n    <tr>\n      <td>518</td>\n      <td>2.104000</td>\n    </tr>\n    <tr>\n      <td>519</td>\n      <td>2.260500</td>\n    </tr>\n    <tr>\n      <td>520</td>\n      <td>2.197600</td>\n    </tr>\n    <tr>\n      <td>521</td>\n      <td>2.206900</td>\n    </tr>\n    <tr>\n      <td>522</td>\n      <td>2.115300</td>\n    </tr>\n    <tr>\n      <td>523</td>\n      <td>2.200300</td>\n    </tr>\n    <tr>\n      <td>524</td>\n      <td>2.195700</td>\n    </tr>\n    <tr>\n      <td>525</td>\n      <td>2.218100</td>\n    </tr>\n    <tr>\n      <td>526</td>\n      <td>2.237400</td>\n    </tr>\n    <tr>\n      <td>527</td>\n      <td>2.282500</td>\n    </tr>\n    <tr>\n      <td>528</td>\n      <td>2.180000</td>\n    </tr>\n    <tr>\n      <td>529</td>\n      <td>2.096400</td>\n    </tr>\n    <tr>\n      <td>530</td>\n      <td>2.206200</td>\n    </tr>\n    <tr>\n      <td>531</td>\n      <td>2.176000</td>\n    </tr>\n    <tr>\n      <td>532</td>\n      <td>2.190100</td>\n    </tr>\n    <tr>\n      <td>533</td>\n      <td>2.197700</td>\n    </tr>\n    <tr>\n      <td>534</td>\n      <td>2.203900</td>\n    </tr>\n    <tr>\n      <td>535</td>\n      <td>2.106700</td>\n    </tr>\n    <tr>\n      <td>536</td>\n      <td>2.101300</td>\n    </tr>\n    <tr>\n      <td>537</td>\n      <td>2.132900</td>\n    </tr>\n    <tr>\n      <td>538</td>\n      <td>2.130800</td>\n    </tr>\n    <tr>\n      <td>539</td>\n      <td>2.208100</td>\n    </tr>\n    <tr>\n      <td>540</td>\n      <td>2.172000</td>\n    </tr>\n    <tr>\n      <td>541</td>\n      <td>2.248900</td>\n    </tr>\n    <tr>\n      <td>542</td>\n      <td>2.092100</td>\n    </tr>\n    <tr>\n      <td>543</td>\n      <td>2.159000</td>\n    </tr>\n    <tr>\n      <td>544</td>\n      <td>2.319200</td>\n    </tr>\n    <tr>\n      <td>545</td>\n      <td>2.171100</td>\n    </tr>\n    <tr>\n      <td>546</td>\n      <td>2.181800</td>\n    </tr>\n    <tr>\n      <td>547</td>\n      <td>2.197500</td>\n    </tr>\n    <tr>\n      <td>548</td>\n      <td>2.171400</td>\n    </tr>\n    <tr>\n      <td>549</td>\n      <td>2.172000</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>2.180200</td>\n    </tr>\n    <tr>\n      <td>551</td>\n      <td>2.233800</td>\n    </tr>\n    <tr>\n      <td>552</td>\n      <td>2.173400</td>\n    </tr>\n    <tr>\n      <td>553</td>\n      <td>2.321000</td>\n    </tr>\n    <tr>\n      <td>554</td>\n      <td>2.250600</td>\n    </tr>\n    <tr>\n      <td>555</td>\n      <td>2.213200</td>\n    </tr>\n    <tr>\n      <td>556</td>\n      <td>2.154500</td>\n    </tr>\n    <tr>\n      <td>557</td>\n      <td>2.005500</td>\n    </tr>\n    <tr>\n      <td>558</td>\n      <td>2.181800</td>\n    </tr>\n    <tr>\n      <td>559</td>\n      <td>2.087500</td>\n    </tr>\n    <tr>\n      <td>560</td>\n      <td>2.100600</td>\n    </tr>\n    <tr>\n      <td>561</td>\n      <td>2.232000</td>\n    </tr>\n    <tr>\n      <td>562</td>\n      <td>2.216800</td>\n    </tr>\n    <tr>\n      <td>563</td>\n      <td>2.175100</td>\n    </tr>\n    <tr>\n      <td>564</td>\n      <td>2.104900</td>\n    </tr>\n    <tr>\n      <td>565</td>\n      <td>2.172100</td>\n    </tr>\n    <tr>\n      <td>566</td>\n      <td>2.272700</td>\n    </tr>\n    <tr>\n      <td>567</td>\n      <td>2.149400</td>\n    </tr>\n    <tr>\n      <td>568</td>\n      <td>2.191300</td>\n    </tr>\n    <tr>\n      <td>569</td>\n      <td>2.268600</td>\n    </tr>\n    <tr>\n      <td>570</td>\n      <td>2.234200</td>\n    </tr>\n    <tr>\n      <td>571</td>\n      <td>2.196100</td>\n    </tr>\n    <tr>\n      <td>572</td>\n      <td>2.132300</td>\n    </tr>\n    <tr>\n      <td>573</td>\n      <td>2.240600</td>\n    </tr>\n    <tr>\n      <td>574</td>\n      <td>2.148000</td>\n    </tr>\n    <tr>\n      <td>575</td>\n      <td>2.143700</td>\n    </tr>\n    <tr>\n      <td>576</td>\n      <td>2.199000</td>\n    </tr>\n    <tr>\n      <td>577</td>\n      <td>2.221200</td>\n    </tr>\n    <tr>\n      <td>578</td>\n      <td>2.189700</td>\n    </tr>\n    <tr>\n      <td>579</td>\n      <td>2.160400</td>\n    </tr>\n    <tr>\n      <td>580</td>\n      <td>2.290200</td>\n    </tr>\n    <tr>\n      <td>581</td>\n      <td>2.121900</td>\n    </tr>\n    <tr>\n      <td>582</td>\n      <td>2.145500</td>\n    </tr>\n    <tr>\n      <td>583</td>\n      <td>2.154600</td>\n    </tr>\n    <tr>\n      <td>584</td>\n      <td>2.186600</td>\n    </tr>\n    <tr>\n      <td>585</td>\n      <td>2.207700</td>\n    </tr>\n    <tr>\n      <td>586</td>\n      <td>2.260000</td>\n    </tr>\n    <tr>\n      <td>587</td>\n      <td>2.084300</td>\n    </tr>\n    <tr>\n      <td>588</td>\n      <td>2.232300</td>\n    </tr>\n    <tr>\n      <td>589</td>\n      <td>2.182900</td>\n    </tr>\n    <tr>\n      <td>590</td>\n      <td>2.105200</td>\n    </tr>\n    <tr>\n      <td>591</td>\n      <td>2.143100</td>\n    </tr>\n    <tr>\n      <td>592</td>\n      <td>2.237500</td>\n    </tr>\n    <tr>\n      <td>593</td>\n      <td>2.123000</td>\n    </tr>\n    <tr>\n      <td>594</td>\n      <td>2.208600</td>\n    </tr>\n    <tr>\n      <td>595</td>\n      <td>2.092500</td>\n    </tr>\n    <tr>\n      <td>596</td>\n      <td>2.155900</td>\n    </tr>\n    <tr>\n      <td>597</td>\n      <td>2.224800</td>\n    </tr>\n    <tr>\n      <td>598</td>\n      <td>2.115600</td>\n    </tr>\n    <tr>\n      <td>599</td>\n      <td>2.320600</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>2.132000</td>\n    </tr>\n    <tr>\n      <td>601</td>\n      <td>2.238900</td>\n    </tr>\n    <tr>\n      <td>602</td>\n      <td>2.241000</td>\n    </tr>\n    <tr>\n      <td>603</td>\n      <td>2.154900</td>\n    </tr>\n    <tr>\n      <td>604</td>\n      <td>2.189000</td>\n    </tr>\n    <tr>\n      <td>605</td>\n      <td>2.151500</td>\n    </tr>\n    <tr>\n      <td>606</td>\n      <td>2.179800</td>\n    </tr>\n    <tr>\n      <td>607</td>\n      <td>2.167900</td>\n    </tr>\n    <tr>\n      <td>608</td>\n      <td>2.166600</td>\n    </tr>\n    <tr>\n      <td>609</td>\n      <td>2.084100</td>\n    </tr>\n    <tr>\n      <td>610</td>\n      <td>2.169600</td>\n    </tr>\n    <tr>\n      <td>611</td>\n      <td>2.070900</td>\n    </tr>\n    <tr>\n      <td>612</td>\n      <td>2.121500</td>\n    </tr>\n    <tr>\n      <td>613</td>\n      <td>2.174600</td>\n    </tr>\n    <tr>\n      <td>614</td>\n      <td>2.193200</td>\n    </tr>\n    <tr>\n      <td>615</td>\n      <td>2.241600</td>\n    </tr>\n    <tr>\n      <td>616</td>\n      <td>2.012900</td>\n    </tr>\n    <tr>\n      <td>617</td>\n      <td>2.181700</td>\n    </tr>\n    <tr>\n      <td>618</td>\n      <td>2.061600</td>\n    </tr>\n    <tr>\n      <td>619</td>\n      <td>2.193400</td>\n    </tr>\n    <tr>\n      <td>620</td>\n      <td>2.216600</td>\n    </tr>\n    <tr>\n      <td>621</td>\n      <td>2.050600</td>\n    </tr>\n    <tr>\n      <td>622</td>\n      <td>2.241900</td>\n    </tr>\n    <tr>\n      <td>623</td>\n      <td>2.122600</td>\n    </tr>\n    <tr>\n      <td>624</td>\n      <td>2.099800</td>\n    </tr>\n    <tr>\n      <td>625</td>\n      <td>2.109900</td>\n    </tr>\n    <tr>\n      <td>626</td>\n      <td>2.159700</td>\n    </tr>\n    <tr>\n      <td>627</td>\n      <td>2.103000</td>\n    </tr>\n    <tr>\n      <td>628</td>\n      <td>2.066900</td>\n    </tr>\n    <tr>\n      <td>629</td>\n      <td>2.192900</td>\n    </tr>\n    <tr>\n      <td>630</td>\n      <td>2.081000</td>\n    </tr>\n    <tr>\n      <td>631</td>\n      <td>2.223900</td>\n    </tr>\n    <tr>\n      <td>632</td>\n      <td>2.279100</td>\n    </tr>\n    <tr>\n      <td>633</td>\n      <td>2.185100</td>\n    </tr>\n    <tr>\n      <td>634</td>\n      <td>2.125900</td>\n    </tr>\n    <tr>\n      <td>635</td>\n      <td>2.086700</td>\n    </tr>\n    <tr>\n      <td>636</td>\n      <td>2.167900</td>\n    </tr>\n    <tr>\n      <td>637</td>\n      <td>2.270600</td>\n    </tr>\n    <tr>\n      <td>638</td>\n      <td>2.139200</td>\n    </tr>\n    <tr>\n      <td>639</td>\n      <td>2.165700</td>\n    </tr>\n    <tr>\n      <td>640</td>\n      <td>1.980600</td>\n    </tr>\n    <tr>\n      <td>641</td>\n      <td>2.168000</td>\n    </tr>\n    <tr>\n      <td>642</td>\n      <td>2.051600</td>\n    </tr>\n    <tr>\n      <td>643</td>\n      <td>2.073700</td>\n    </tr>\n    <tr>\n      <td>644</td>\n      <td>2.101200</td>\n    </tr>\n    <tr>\n      <td>645</td>\n      <td>2.210600</td>\n    </tr>\n    <tr>\n      <td>646</td>\n      <td>2.184900</td>\n    </tr>\n    <tr>\n      <td>647</td>\n      <td>2.084800</td>\n    </tr>\n    <tr>\n      <td>648</td>\n      <td>2.184900</td>\n    </tr>\n    <tr>\n      <td>649</td>\n      <td>2.194800</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>2.092600</td>\n    </tr>\n    <tr>\n      <td>651</td>\n      <td>2.220400</td>\n    </tr>\n    <tr>\n      <td>652</td>\n      <td>2.134900</td>\n    </tr>\n    <tr>\n      <td>653</td>\n      <td>2.048700</td>\n    </tr>\n    <tr>\n      <td>654</td>\n      <td>2.087900</td>\n    </tr>\n    <tr>\n      <td>655</td>\n      <td>2.037800</td>\n    </tr>\n    <tr>\n      <td>656</td>\n      <td>2.141900</td>\n    </tr>\n    <tr>\n      <td>657</td>\n      <td>2.231000</td>\n    </tr>\n    <tr>\n      <td>658</td>\n      <td>2.220800</td>\n    </tr>\n    <tr>\n      <td>659</td>\n      <td>2.106500</td>\n    </tr>\n    <tr>\n      <td>660</td>\n      <td>2.113000</td>\n    </tr>\n    <tr>\n      <td>661</td>\n      <td>2.134200</td>\n    </tr>\n    <tr>\n      <td>662</td>\n      <td>2.260600</td>\n    </tr>\n    <tr>\n      <td>663</td>\n      <td>2.180400</td>\n    </tr>\n    <tr>\n      <td>664</td>\n      <td>2.167700</td>\n    </tr>\n    <tr>\n      <td>665</td>\n      <td>2.147600</td>\n    </tr>\n    <tr>\n      <td>666</td>\n      <td>2.312800</td>\n    </tr>\n    <tr>\n      <td>667</td>\n      <td>2.144300</td>\n    </tr>\n    <tr>\n      <td>668</td>\n      <td>2.202700</td>\n    </tr>\n    <tr>\n      <td>669</td>\n      <td>2.179000</td>\n    </tr>\n    <tr>\n      <td>670</td>\n      <td>2.072200</td>\n    </tr>\n    <tr>\n      <td>671</td>\n      <td>1.978600</td>\n    </tr>\n    <tr>\n      <td>672</td>\n      <td>2.058100</td>\n    </tr>\n    <tr>\n      <td>673</td>\n      <td>2.125200</td>\n    </tr>\n    <tr>\n      <td>674</td>\n      <td>2.056700</td>\n    </tr>\n    <tr>\n      <td>675</td>\n      <td>2.193700</td>\n    </tr>\n    <tr>\n      <td>676</td>\n      <td>2.117100</td>\n    </tr>\n    <tr>\n      <td>677</td>\n      <td>2.207100</td>\n    </tr>\n    <tr>\n      <td>678</td>\n      <td>2.059900</td>\n    </tr>\n    <tr>\n      <td>679</td>\n      <td>2.162000</td>\n    </tr>\n    <tr>\n      <td>680</td>\n      <td>2.108100</td>\n    </tr>\n    <tr>\n      <td>681</td>\n      <td>2.016200</td>\n    </tr>\n    <tr>\n      <td>682</td>\n      <td>2.145800</td>\n    </tr>\n    <tr>\n      <td>683</td>\n      <td>2.117800</td>\n    </tr>\n    <tr>\n      <td>684</td>\n      <td>2.171200</td>\n    </tr>\n    <tr>\n      <td>685</td>\n      <td>2.098400</td>\n    </tr>\n    <tr>\n      <td>686</td>\n      <td>2.120800</td>\n    </tr>\n    <tr>\n      <td>687</td>\n      <td>2.042700</td>\n    </tr>\n    <tr>\n      <td>688</td>\n      <td>2.296900</td>\n    </tr>\n    <tr>\n      <td>689</td>\n      <td>2.149900</td>\n    </tr>\n    <tr>\n      <td>690</td>\n      <td>2.207200</td>\n    </tr>\n    <tr>\n      <td>691</td>\n      <td>2.276500</td>\n    </tr>\n    <tr>\n      <td>692</td>\n      <td>2.180200</td>\n    </tr>\n    <tr>\n      <td>693</td>\n      <td>2.051100</td>\n    </tr>\n    <tr>\n      <td>694</td>\n      <td>2.069300</td>\n    </tr>\n    <tr>\n      <td>695</td>\n      <td>2.050800</td>\n    </tr>\n    <tr>\n      <td>696</td>\n      <td>2.218600</td>\n    </tr>\n    <tr>\n      <td>697</td>\n      <td>2.186600</td>\n    </tr>\n    <tr>\n      <td>698</td>\n      <td>2.015800</td>\n    </tr>\n    <tr>\n      <td>699</td>\n      <td>2.073600</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>2.265000</td>\n    </tr>\n    <tr>\n      <td>701</td>\n      <td>2.160900</td>\n    </tr>\n    <tr>\n      <td>702</td>\n      <td>2.200300</td>\n    </tr>\n    <tr>\n      <td>703</td>\n      <td>2.116900</td>\n    </tr>\n    <tr>\n      <td>704</td>\n      <td>2.203600</td>\n    </tr>\n    <tr>\n      <td>705</td>\n      <td>2.075000</td>\n    </tr>\n    <tr>\n      <td>706</td>\n      <td>2.255300</td>\n    </tr>\n    <tr>\n      <td>707</td>\n      <td>2.127000</td>\n    </tr>\n    <tr>\n      <td>708</td>\n      <td>2.152300</td>\n    </tr>\n    <tr>\n      <td>709</td>\n      <td>2.121000</td>\n    </tr>\n    <tr>\n      <td>710</td>\n      <td>2.084800</td>\n    </tr>\n    <tr>\n      <td>711</td>\n      <td>2.189900</td>\n    </tr>\n    <tr>\n      <td>712</td>\n      <td>2.107800</td>\n    </tr>\n    <tr>\n      <td>713</td>\n      <td>2.233000</td>\n    </tr>\n    <tr>\n      <td>714</td>\n      <td>2.127500</td>\n    </tr>\n    <tr>\n      <td>715</td>\n      <td>2.255000</td>\n    </tr>\n    <tr>\n      <td>716</td>\n      <td>2.134600</td>\n    </tr>\n    <tr>\n      <td>717</td>\n      <td>2.081100</td>\n    </tr>\n    <tr>\n      <td>718</td>\n      <td>2.078100</td>\n    </tr>\n    <tr>\n      <td>719</td>\n      <td>2.055100</td>\n    </tr>\n    <tr>\n      <td>720</td>\n      <td>2.094700</td>\n    </tr>\n    <tr>\n      <td>721</td>\n      <td>2.101700</td>\n    </tr>\n    <tr>\n      <td>722</td>\n      <td>2.188500</td>\n    </tr>\n    <tr>\n      <td>723</td>\n      <td>2.092900</td>\n    </tr>\n    <tr>\n      <td>724</td>\n      <td>2.143100</td>\n    </tr>\n    <tr>\n      <td>725</td>\n      <td>2.086400</td>\n    </tr>\n    <tr>\n      <td>726</td>\n      <td>2.056600</td>\n    </tr>\n    <tr>\n      <td>727</td>\n      <td>2.137000</td>\n    </tr>\n    <tr>\n      <td>728</td>\n      <td>2.162100</td>\n    </tr>\n    <tr>\n      <td>729</td>\n      <td>2.213000</td>\n    </tr>\n    <tr>\n      <td>730</td>\n      <td>2.098700</td>\n    </tr>\n    <tr>\n      <td>731</td>\n      <td>2.114200</td>\n    </tr>\n    <tr>\n      <td>732</td>\n      <td>2.296200</td>\n    </tr>\n    <tr>\n      <td>733</td>\n      <td>2.122700</td>\n    </tr>\n    <tr>\n      <td>734</td>\n      <td>2.107000</td>\n    </tr>\n    <tr>\n      <td>735</td>\n      <td>2.110200</td>\n    </tr>\n    <tr>\n      <td>736</td>\n      <td>2.163200</td>\n    </tr>\n    <tr>\n      <td>737</td>\n      <td>2.024800</td>\n    </tr>\n    <tr>\n      <td>738</td>\n      <td>2.064700</td>\n    </tr>\n    <tr>\n      <td>739</td>\n      <td>2.183900</td>\n    </tr>\n    <tr>\n      <td>740</td>\n      <td>2.183900</td>\n    </tr>\n    <tr>\n      <td>741</td>\n      <td>2.150400</td>\n    </tr>\n    <tr>\n      <td>742</td>\n      <td>2.076100</td>\n    </tr>\n    <tr>\n      <td>743</td>\n      <td>2.054300</td>\n    </tr>\n    <tr>\n      <td>744</td>\n      <td>2.075300</td>\n    </tr>\n    <tr>\n      <td>745</td>\n      <td>2.166100</td>\n    </tr>\n    <tr>\n      <td>746</td>\n      <td>2.063100</td>\n    </tr>\n    <tr>\n      <td>747</td>\n      <td>2.156300</td>\n    </tr>\n    <tr>\n      <td>748</td>\n      <td>2.142100</td>\n    </tr>\n    <tr>\n      <td>749</td>\n      <td>2.134400</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>2.186200</td>\n    </tr>\n    <tr>\n      <td>751</td>\n      <td>2.166600</td>\n    </tr>\n    <tr>\n      <td>752</td>\n      <td>2.092300</td>\n    </tr>\n    <tr>\n      <td>753</td>\n      <td>2.025500</td>\n    </tr>\n    <tr>\n      <td>754</td>\n      <td>2.165400</td>\n    </tr>\n    <tr>\n      <td>755</td>\n      <td>2.166400</td>\n    </tr>\n    <tr>\n      <td>756</td>\n      <td>2.074800</td>\n    </tr>\n    <tr>\n      <td>757</td>\n      <td>2.072400</td>\n    </tr>\n    <tr>\n      <td>758</td>\n      <td>2.179000</td>\n    </tr>\n    <tr>\n      <td>759</td>\n      <td>2.048000</td>\n    </tr>\n    <tr>\n      <td>760</td>\n      <td>2.119300</td>\n    </tr>\n    <tr>\n      <td>761</td>\n      <td>2.275500</td>\n    </tr>\n    <tr>\n      <td>762</td>\n      <td>2.123700</td>\n    </tr>\n    <tr>\n      <td>763</td>\n      <td>2.165500</td>\n    </tr>\n    <tr>\n      <td>764</td>\n      <td>2.117200</td>\n    </tr>\n    <tr>\n      <td>765</td>\n      <td>2.172100</td>\n    </tr>\n    <tr>\n      <td>766</td>\n      <td>2.246600</td>\n    </tr>\n    <tr>\n      <td>767</td>\n      <td>2.160300</td>\n    </tr>\n    <tr>\n      <td>768</td>\n      <td>2.232700</td>\n    </tr>\n    <tr>\n      <td>769</td>\n      <td>2.067800</td>\n    </tr>\n    <tr>\n      <td>770</td>\n      <td>2.113200</td>\n    </tr>\n    <tr>\n      <td>771</td>\n      <td>2.010600</td>\n    </tr>\n    <tr>\n      <td>772</td>\n      <td>2.013800</td>\n    </tr>\n    <tr>\n      <td>773</td>\n      <td>2.143600</td>\n    </tr>\n    <tr>\n      <td>774</td>\n      <td>2.108200</td>\n    </tr>\n    <tr>\n      <td>775</td>\n      <td>2.028600</td>\n    </tr>\n    <tr>\n      <td>776</td>\n      <td>2.128000</td>\n    </tr>\n    <tr>\n      <td>777</td>\n      <td>2.036500</td>\n    </tr>\n    <tr>\n      <td>778</td>\n      <td>2.089200</td>\n    </tr>\n    <tr>\n      <td>779</td>\n      <td>2.207500</td>\n    </tr>\n    <tr>\n      <td>780</td>\n      <td>2.168500</td>\n    </tr>\n    <tr>\n      <td>781</td>\n      <td>2.197400</td>\n    </tr>\n    <tr>\n      <td>782</td>\n      <td>2.062400</td>\n    </tr>\n    <tr>\n      <td>783</td>\n      <td>2.180800</td>\n    </tr>\n    <tr>\n      <td>784</td>\n      <td>2.114700</td>\n    </tr>\n    <tr>\n      <td>785</td>\n      <td>2.279700</td>\n    </tr>\n    <tr>\n      <td>786</td>\n      <td>2.130200</td>\n    </tr>\n    <tr>\n      <td>787</td>\n      <td>2.038900</td>\n    </tr>\n    <tr>\n      <td>788</td>\n      <td>2.138200</td>\n    </tr>\n    <tr>\n      <td>789</td>\n      <td>2.096600</td>\n    </tr>\n    <tr>\n      <td>790</td>\n      <td>2.090900</td>\n    </tr>\n    <tr>\n      <td>791</td>\n      <td>2.026500</td>\n    </tr>\n    <tr>\n      <td>792</td>\n      <td>2.024700</td>\n    </tr>\n    <tr>\n      <td>793</td>\n      <td>2.146700</td>\n    </tr>\n    <tr>\n      <td>794</td>\n      <td>2.112600</td>\n    </tr>\n    <tr>\n      <td>795</td>\n      <td>2.164800</td>\n    </tr>\n    <tr>\n      <td>796</td>\n      <td>2.050900</td>\n    </tr>\n    <tr>\n      <td>797</td>\n      <td>2.207900</td>\n    </tr>\n    <tr>\n      <td>798</td>\n      <td>2.165200</td>\n    </tr>\n    <tr>\n      <td>799</td>\n      <td>2.066900</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>2.097700</td>\n    </tr>\n    <tr>\n      <td>801</td>\n      <td>2.111600</td>\n    </tr>\n    <tr>\n      <td>802</td>\n      <td>2.267300</td>\n    </tr>\n    <tr>\n      <td>803</td>\n      <td>2.200100</td>\n    </tr>\n    <tr>\n      <td>804</td>\n      <td>2.115200</td>\n    </tr>\n    <tr>\n      <td>805</td>\n      <td>2.031900</td>\n    </tr>\n    <tr>\n      <td>806</td>\n      <td>2.248500</td>\n    </tr>\n    <tr>\n      <td>807</td>\n      <td>2.116400</td>\n    </tr>\n    <tr>\n      <td>808</td>\n      <td>2.135700</td>\n    </tr>\n    <tr>\n      <td>809</td>\n      <td>2.046200</td>\n    </tr>\n    <tr>\n      <td>810</td>\n      <td>2.005800</td>\n    </tr>\n    <tr>\n      <td>811</td>\n      <td>2.192000</td>\n    </tr>\n    <tr>\n      <td>812</td>\n      <td>2.080500</td>\n    </tr>\n    <tr>\n      <td>813</td>\n      <td>2.130000</td>\n    </tr>\n    <tr>\n      <td>814</td>\n      <td>2.085900</td>\n    </tr>\n    <tr>\n      <td>815</td>\n      <td>2.159300</td>\n    </tr>\n    <tr>\n      <td>816</td>\n      <td>2.109800</td>\n    </tr>\n    <tr>\n      <td>817</td>\n      <td>2.181700</td>\n    </tr>\n    <tr>\n      <td>818</td>\n      <td>2.107000</td>\n    </tr>\n    <tr>\n      <td>819</td>\n      <td>2.157700</td>\n    </tr>\n    <tr>\n      <td>820</td>\n      <td>2.010800</td>\n    </tr>\n    <tr>\n      <td>821</td>\n      <td>2.118500</td>\n    </tr>\n    <tr>\n      <td>822</td>\n      <td>2.049300</td>\n    </tr>\n    <tr>\n      <td>823</td>\n      <td>2.264700</td>\n    </tr>\n    <tr>\n      <td>824</td>\n      <td>2.056100</td>\n    </tr>\n    <tr>\n      <td>825</td>\n      <td>2.161200</td>\n    </tr>\n    <tr>\n      <td>826</td>\n      <td>2.114300</td>\n    </tr>\n    <tr>\n      <td>827</td>\n      <td>2.137700</td>\n    </tr>\n    <tr>\n      <td>828</td>\n      <td>2.023600</td>\n    </tr>\n    <tr>\n      <td>829</td>\n      <td>2.215700</td>\n    </tr>\n    <tr>\n      <td>830</td>\n      <td>2.130600</td>\n    </tr>\n    <tr>\n      <td>831</td>\n      <td>2.063000</td>\n    </tr>\n    <tr>\n      <td>832</td>\n      <td>2.048600</td>\n    </tr>\n    <tr>\n      <td>833</td>\n      <td>2.154400</td>\n    </tr>\n    <tr>\n      <td>834</td>\n      <td>2.215600</td>\n    </tr>\n    <tr>\n      <td>835</td>\n      <td>2.123900</td>\n    </tr>\n    <tr>\n      <td>836</td>\n      <td>2.093700</td>\n    </tr>\n    <tr>\n      <td>837</td>\n      <td>2.209800</td>\n    </tr>\n    <tr>\n      <td>838</td>\n      <td>2.110000</td>\n    </tr>\n    <tr>\n      <td>839</td>\n      <td>2.155600</td>\n    </tr>\n    <tr>\n      <td>840</td>\n      <td>2.014500</td>\n    </tr>\n    <tr>\n      <td>841</td>\n      <td>2.056300</td>\n    </tr>\n    <tr>\n      <td>842</td>\n      <td>2.095100</td>\n    </tr>\n    <tr>\n      <td>843</td>\n      <td>2.116600</td>\n    </tr>\n    <tr>\n      <td>844</td>\n      <td>2.058700</td>\n    </tr>\n    <tr>\n      <td>845</td>\n      <td>2.116700</td>\n    </tr>\n    <tr>\n      <td>846</td>\n      <td>2.063500</td>\n    </tr>\n    <tr>\n      <td>847</td>\n      <td>2.081000</td>\n    </tr>\n    <tr>\n      <td>848</td>\n      <td>2.027900</td>\n    </tr>\n    <tr>\n      <td>849</td>\n      <td>2.257400</td>\n    </tr>\n    <tr>\n      <td>850</td>\n      <td>2.068400</td>\n    </tr>\n    <tr>\n      <td>851</td>\n      <td>2.074600</td>\n    </tr>\n    <tr>\n      <td>852</td>\n      <td>1.987700</td>\n    </tr>\n    <tr>\n      <td>853</td>\n      <td>2.186000</td>\n    </tr>\n    <tr>\n      <td>854</td>\n      <td>2.191800</td>\n    </tr>\n    <tr>\n      <td>855</td>\n      <td>2.157000</td>\n    </tr>\n    <tr>\n      <td>856</td>\n      <td>2.111200</td>\n    </tr>\n    <tr>\n      <td>857</td>\n      <td>2.192900</td>\n    </tr>\n    <tr>\n      <td>858</td>\n      <td>2.008200</td>\n    </tr>\n    <tr>\n      <td>859</td>\n      <td>2.084900</td>\n    </tr>\n    <tr>\n      <td>860</td>\n      <td>2.032600</td>\n    </tr>\n    <tr>\n      <td>861</td>\n      <td>2.225700</td>\n    </tr>\n    <tr>\n      <td>862</td>\n      <td>2.214700</td>\n    </tr>\n    <tr>\n      <td>863</td>\n      <td>2.051100</td>\n    </tr>\n    <tr>\n      <td>864</td>\n      <td>2.059600</td>\n    </tr>\n    <tr>\n      <td>865</td>\n      <td>2.065400</td>\n    </tr>\n    <tr>\n      <td>866</td>\n      <td>2.138200</td>\n    </tr>\n    <tr>\n      <td>867</td>\n      <td>2.314100</td>\n    </tr>\n    <tr>\n      <td>868</td>\n      <td>2.132500</td>\n    </tr>\n    <tr>\n      <td>869</td>\n      <td>2.211500</td>\n    </tr>\n    <tr>\n      <td>870</td>\n      <td>2.015700</td>\n    </tr>\n    <tr>\n      <td>871</td>\n      <td>2.065300</td>\n    </tr>\n    <tr>\n      <td>872</td>\n      <td>1.996100</td>\n    </tr>\n    <tr>\n      <td>873</td>\n      <td>2.144700</td>\n    </tr>\n    <tr>\n      <td>874</td>\n      <td>2.150800</td>\n    </tr>\n    <tr>\n      <td>875</td>\n      <td>2.124100</td>\n    </tr>\n    <tr>\n      <td>876</td>\n      <td>2.075600</td>\n    </tr>\n    <tr>\n      <td>877</td>\n      <td>2.065000</td>\n    </tr>\n    <tr>\n      <td>878</td>\n      <td>2.139900</td>\n    </tr>\n    <tr>\n      <td>879</td>\n      <td>2.038100</td>\n    </tr>\n    <tr>\n      <td>880</td>\n      <td>2.249400</td>\n    </tr>\n    <tr>\n      <td>881</td>\n      <td>2.197800</td>\n    </tr>\n    <tr>\n      <td>882</td>\n      <td>2.129700</td>\n    </tr>\n    <tr>\n      <td>883</td>\n      <td>2.019300</td>\n    </tr>\n    <tr>\n      <td>884</td>\n      <td>2.130200</td>\n    </tr>\n    <tr>\n      <td>885</td>\n      <td>1.992200</td>\n    </tr>\n    <tr>\n      <td>886</td>\n      <td>2.161800</td>\n    </tr>\n    <tr>\n      <td>887</td>\n      <td>2.083600</td>\n    </tr>\n    <tr>\n      <td>888</td>\n      <td>2.084600</td>\n    </tr>\n    <tr>\n      <td>889</td>\n      <td>2.083400</td>\n    </tr>\n    <tr>\n      <td>890</td>\n      <td>2.069200</td>\n    </tr>\n    <tr>\n      <td>891</td>\n      <td>2.095700</td>\n    </tr>\n    <tr>\n      <td>892</td>\n      <td>2.164400</td>\n    </tr>\n    <tr>\n      <td>893</td>\n      <td>2.076400</td>\n    </tr>\n    <tr>\n      <td>894</td>\n      <td>2.193800</td>\n    </tr>\n    <tr>\n      <td>895</td>\n      <td>2.287400</td>\n    </tr>\n    <tr>\n      <td>896</td>\n      <td>2.110300</td>\n    </tr>\n    <tr>\n      <td>897</td>\n      <td>2.014500</td>\n    </tr>\n    <tr>\n      <td>898</td>\n      <td>2.090800</td>\n    </tr>\n    <tr>\n      <td>899</td>\n      <td>2.137500</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>2.015800</td>\n    </tr>\n    <tr>\n      <td>901</td>\n      <td>2.072000</td>\n    </tr>\n    <tr>\n      <td>902</td>\n      <td>2.140800</td>\n    </tr>\n    <tr>\n      <td>903</td>\n      <td>2.109000</td>\n    </tr>\n    <tr>\n      <td>904</td>\n      <td>2.091700</td>\n    </tr>\n    <tr>\n      <td>905</td>\n      <td>2.124900</td>\n    </tr>\n    <tr>\n      <td>906</td>\n      <td>2.012800</td>\n    </tr>\n    <tr>\n      <td>907</td>\n      <td>2.066000</td>\n    </tr>\n    <tr>\n      <td>908</td>\n      <td>1.987800</td>\n    </tr>\n    <tr>\n      <td>909</td>\n      <td>2.170900</td>\n    </tr>\n    <tr>\n      <td>910</td>\n      <td>2.108600</td>\n    </tr>\n    <tr>\n      <td>911</td>\n      <td>2.157600</td>\n    </tr>\n    <tr>\n      <td>912</td>\n      <td>2.199500</td>\n    </tr>\n    <tr>\n      <td>913</td>\n      <td>1.954000</td>\n    </tr>\n    <tr>\n      <td>914</td>\n      <td>2.063800</td>\n    </tr>\n    <tr>\n      <td>915</td>\n      <td>2.044600</td>\n    </tr>\n    <tr>\n      <td>916</td>\n      <td>2.012200</td>\n    </tr>\n    <tr>\n      <td>917</td>\n      <td>2.110700</td>\n    </tr>\n    <tr>\n      <td>918</td>\n      <td>2.111200</td>\n    </tr>\n    <tr>\n      <td>919</td>\n      <td>2.132600</td>\n    </tr>\n    <tr>\n      <td>920</td>\n      <td>2.110300</td>\n    </tr>\n    <tr>\n      <td>921</td>\n      <td>2.124300</td>\n    </tr>\n    <tr>\n      <td>922</td>\n      <td>2.083600</td>\n    </tr>\n    <tr>\n      <td>923</td>\n      <td>2.093000</td>\n    </tr>\n    <tr>\n      <td>924</td>\n      <td>2.041800</td>\n    </tr>\n    <tr>\n      <td>925</td>\n      <td>2.091200</td>\n    </tr>\n    <tr>\n      <td>926</td>\n      <td>2.099500</td>\n    </tr>\n    <tr>\n      <td>927</td>\n      <td>2.084000</td>\n    </tr>\n    <tr>\n      <td>928</td>\n      <td>2.033800</td>\n    </tr>\n    <tr>\n      <td>929</td>\n      <td>2.121800</td>\n    </tr>\n    <tr>\n      <td>930</td>\n      <td>2.227800</td>\n    </tr>\n    <tr>\n      <td>931</td>\n      <td>2.105600</td>\n    </tr>\n    <tr>\n      <td>932</td>\n      <td>2.194700</td>\n    </tr>\n    <tr>\n      <td>933</td>\n      <td>2.009400</td>\n    </tr>\n    <tr>\n      <td>934</td>\n      <td>2.173500</td>\n    </tr>\n    <tr>\n      <td>935</td>\n      <td>2.175100</td>\n    </tr>\n    <tr>\n      <td>936</td>\n      <td>2.120500</td>\n    </tr>\n    <tr>\n      <td>937</td>\n      <td>2.086800</td>\n    </tr>\n    <tr>\n      <td>938</td>\n      <td>2.056300</td>\n    </tr>\n    <tr>\n      <td>939</td>\n      <td>1.993100</td>\n    </tr>\n    <tr>\n      <td>940</td>\n      <td>2.099200</td>\n    </tr>\n    <tr>\n      <td>941</td>\n      <td>1.975300</td>\n    </tr>\n    <tr>\n      <td>942</td>\n      <td>2.123900</td>\n    </tr>\n    <tr>\n      <td>943</td>\n      <td>2.148000</td>\n    </tr>\n    <tr>\n      <td>944</td>\n      <td>2.157800</td>\n    </tr>\n    <tr>\n      <td>945</td>\n      <td>2.088000</td>\n    </tr>\n    <tr>\n      <td>946</td>\n      <td>2.140000</td>\n    </tr>\n    <tr>\n      <td>947</td>\n      <td>2.159000</td>\n    </tr>\n    <tr>\n      <td>948</td>\n      <td>2.062400</td>\n    </tr>\n    <tr>\n      <td>949</td>\n      <td>2.054900</td>\n    </tr>\n    <tr>\n      <td>950</td>\n      <td>2.046900</td>\n    </tr>\n    <tr>\n      <td>951</td>\n      <td>2.267500</td>\n    </tr>\n    <tr>\n      <td>952</td>\n      <td>2.122100</td>\n    </tr>\n    <tr>\n      <td>953</td>\n      <td>2.044700</td>\n    </tr>\n    <tr>\n      <td>954</td>\n      <td>2.145500</td>\n    </tr>\n    <tr>\n      <td>955</td>\n      <td>2.174100</td>\n    </tr>\n    <tr>\n      <td>956</td>\n      <td>2.004200</td>\n    </tr>\n    <tr>\n      <td>957</td>\n      <td>2.031800</td>\n    </tr>\n    <tr>\n      <td>958</td>\n      <td>2.057600</td>\n    </tr>\n    <tr>\n      <td>959</td>\n      <td>2.125800</td>\n    </tr>\n    <tr>\n      <td>960</td>\n      <td>2.121200</td>\n    </tr>\n    <tr>\n      <td>961</td>\n      <td>2.024300</td>\n    </tr>\n    <tr>\n      <td>962</td>\n      <td>2.189400</td>\n    </tr>\n    <tr>\n      <td>963</td>\n      <td>2.075700</td>\n    </tr>\n    <tr>\n      <td>964</td>\n      <td>2.100900</td>\n    </tr>\n    <tr>\n      <td>965</td>\n      <td>2.048800</td>\n    </tr>\n    <tr>\n      <td>966</td>\n      <td>2.069800</td>\n    </tr>\n    <tr>\n      <td>967</td>\n      <td>2.207700</td>\n    </tr>\n    <tr>\n      <td>968</td>\n      <td>2.065200</td>\n    </tr>\n    <tr>\n      <td>969</td>\n      <td>2.077600</td>\n    </tr>\n    <tr>\n      <td>970</td>\n      <td>2.013800</td>\n    </tr>\n    <tr>\n      <td>971</td>\n      <td>2.097700</td>\n    </tr>\n    <tr>\n      <td>972</td>\n      <td>2.015100</td>\n    </tr>\n    <tr>\n      <td>973</td>\n      <td>2.122100</td>\n    </tr>\n    <tr>\n      <td>974</td>\n      <td>2.021900</td>\n    </tr>\n    <tr>\n      <td>975</td>\n      <td>2.109800</td>\n    </tr>\n    <tr>\n      <td>976</td>\n      <td>1.970300</td>\n    </tr>\n    <tr>\n      <td>977</td>\n      <td>2.143900</td>\n    </tr>\n    <tr>\n      <td>978</td>\n      <td>2.137000</td>\n    </tr>\n    <tr>\n      <td>979</td>\n      <td>2.135300</td>\n    </tr>\n    <tr>\n      <td>980</td>\n      <td>2.050600</td>\n    </tr>\n    <tr>\n      <td>981</td>\n      <td>2.103600</td>\n    </tr>\n    <tr>\n      <td>982</td>\n      <td>2.117400</td>\n    </tr>\n    <tr>\n      <td>983</td>\n      <td>2.215000</td>\n    </tr>\n    <tr>\n      <td>984</td>\n      <td>2.055100</td>\n    </tr>\n    <tr>\n      <td>985</td>\n      <td>2.058300</td>\n    </tr>\n    <tr>\n      <td>986</td>\n      <td>2.085800</td>\n    </tr>\n    <tr>\n      <td>987</td>\n      <td>2.115700</td>\n    </tr>\n    <tr>\n      <td>988</td>\n      <td>2.084700</td>\n    </tr>\n    <tr>\n      <td>989</td>\n      <td>2.067800</td>\n    </tr>\n    <tr>\n      <td>990</td>\n      <td>2.071800</td>\n    </tr>\n    <tr>\n      <td>991</td>\n      <td>2.036400</td>\n    </tr>\n    <tr>\n      <td>992</td>\n      <td>2.083500</td>\n    </tr>\n    <tr>\n      <td>993</td>\n      <td>2.084300</td>\n    </tr>\n    <tr>\n      <td>994</td>\n      <td>2.143400</td>\n    </tr>\n    <tr>\n      <td>995</td>\n      <td>2.100200</td>\n    </tr>\n    <tr>\n      <td>996</td>\n      <td>2.119200</td>\n    </tr>\n    <tr>\n      <td>997</td>\n      <td>2.126900</td>\n    </tr>\n    <tr>\n      <td>998</td>\n      <td>2.112300</td>\n    </tr>\n    <tr>\n      <td>999</td>\n      <td>2.079700</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>2.086800</td>\n    </tr>\n    <tr>\n      <td>1001</td>\n      <td>2.047400</td>\n    </tr>\n    <tr>\n      <td>1002</td>\n      <td>1.958900</td>\n    </tr>\n    <tr>\n      <td>1003</td>\n      <td>2.105200</td>\n    </tr>\n    <tr>\n      <td>1004</td>\n      <td>2.064100</td>\n    </tr>\n    <tr>\n      <td>1005</td>\n      <td>2.055900</td>\n    </tr>\n    <tr>\n      <td>1006</td>\n      <td>2.076600</td>\n    </tr>\n    <tr>\n      <td>1007</td>\n      <td>2.093700</td>\n    </tr>\n    <tr>\n      <td>1008</td>\n      <td>2.114400</td>\n    </tr>\n    <tr>\n      <td>1009</td>\n      <td>1.929400</td>\n    </tr>\n    <tr>\n      <td>1010</td>\n      <td>2.120100</td>\n    </tr>\n    <tr>\n      <td>1011</td>\n      <td>2.047800</td>\n    </tr>\n    <tr>\n      <td>1012</td>\n      <td>2.082000</td>\n    </tr>\n    <tr>\n      <td>1013</td>\n      <td>2.176500</td>\n    </tr>\n    <tr>\n      <td>1014</td>\n      <td>2.103500</td>\n    </tr>\n    <tr>\n      <td>1015</td>\n      <td>2.051100</td>\n    </tr>\n    <tr>\n      <td>1016</td>\n      <td>2.115700</td>\n    </tr>\n    <tr>\n      <td>1017</td>\n      <td>2.065200</td>\n    </tr>\n    <tr>\n      <td>1018</td>\n      <td>2.008100</td>\n    </tr>\n    <tr>\n      <td>1019</td>\n      <td>2.002400</td>\n    </tr>\n    <tr>\n      <td>1020</td>\n      <td>2.038200</td>\n    </tr>\n    <tr>\n      <td>1021</td>\n      <td>2.067900</td>\n    </tr>\n    <tr>\n      <td>1022</td>\n      <td>2.142400</td>\n    </tr>\n    <tr>\n      <td>1023</td>\n      <td>2.019800</td>\n    </tr>\n    <tr>\n      <td>1024</td>\n      <td>2.034900</td>\n    </tr>\n    <tr>\n      <td>1025</td>\n      <td>2.078200</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1025, training_loss=3.509699987434759, metrics={'train_runtime': 264.7733, 'train_samples_per_second': 15.466, 'train_steps_per_second': 3.871, 'total_flos': 765549886832640.0, 'train_loss': 3.509699987434759, 'epoch': 5.0})"},"metadata":{}}],"execution_count":32},{"cell_type":"code","source":"peft_dir = \"./peft-flan-t5\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T00:13:22.751889Z","iopub.execute_input":"2025-05-05T00:13:22.752134Z","iopub.status.idle":"2025-05-05T00:13:22.756551Z","shell.execute_reply.started":"2025-05-05T00:13:22.752115Z","shell.execute_reply":"2025-05-05T00:13:22.755627Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"model.save_pretrained(peft_dir)\ntokenizer.save_pretrained(peft_dir)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T00:13:22.757372Z","iopub.execute_input":"2025-05-05T00:13:22.757639Z","iopub.status.idle":"2025-05-05T00:13:22.913418Z","shell.execute_reply.started":"2025-05-05T00:13:22.757611Z","shell.execute_reply":"2025-05-05T00:13:22.912651Z"}},"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"('./peft-flan-t5/tokenizer_config.json',\n './peft-flan-t5/special_tokens_map.json',\n './peft-flan-t5/spiece.model',\n './peft-flan-t5/added_tokens.json')"},"metadata":{}}],"execution_count":34},{"cell_type":"code","source":"# Load the fine-tuned model\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\ntokenizer_peft_lora = T5Tokenizer.from_pretrained(peft_dir)\nmodel_peft_lora = T5ForConditionalGeneration.from_pretrained(peft_dir)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T00:13:22.914856Z","iopub.execute_input":"2025-05-05T00:13:22.915104Z","iopub.status.idle":"2025-05-05T00:13:23.453528Z","shell.execute_reply.started":"2025-05-05T00:13:22.915088Z","shell.execute_reply":"2025-05-05T00:13:23.452894Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"from peft import PeftModel, PeftConfig\n\nmodel_peft_lora = PeftModel.from_pretrained(model_peft_lora, peft_dir)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T00:13:23.454454Z","iopub.execute_input":"2025-05-05T00:13:23.454657Z","iopub.status.idle":"2025-05-05T00:13:23.521309Z","shell.execute_reply.started":"2025-05-05T00:13:23.454642Z","shell.execute_reply":"2025-05-05T00:13:23.520592Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"\n# Input example\nraw_dialogue= \"Edward: Rachel, I think I'm in love with Bella.\\nRachel: Don't say anything else.\\nEdward: What do you mean??\\nRachel: Open your fu**ing door.. I'm outside\"\ninput_text = \"Summarize the following dialogue:\\n\" + format_dialogue(raw_dialogue)\n\ninputs = tokenizer_peft_lora(input_text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n\n# Generate summary\n\noutput_ids = model_peft_lora.model.generate(\n    inputs[\"input_ids\"], \n    max_length=128, \n    num_beams=4\n)\nsummary_with_peft_lora = tokenizer_peft_lora.decode(output_ids[0], skip_special_tokens=True)\n\nprint(\"Generated Summary:\", summary_with_peft_lora)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T00:13:23.522250Z","iopub.execute_input":"2025-05-05T00:13:23.522539Z","iopub.status.idle":"2025-05-05T00:13:24.051162Z","shell.execute_reply.started":"2025-05-05T00:13:23.522515Z","shell.execute_reply":"2025-05-05T00:13:24.050121Z"}},"outputs":[{"name":"stdout","text":"Generated Summary: Edward is in love with Bella.\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"print(\"summary with peft and lora: \"+summary_with_peft_lora)\nprint(\"Base model summary: \"+summary_base_model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T00:13:24.052149Z","iopub.execute_input":"2025-05-05T00:13:24.052526Z","iopub.status.idle":"2025-05-05T00:13:24.057001Z","shell.execute_reply.started":"2025-05-05T00:13:24.052500Z","shell.execute_reply":"2025-05-05T00:13:24.056098Z"}},"outputs":[{"name":"stdout","text":"summary with peft and lora: Edward is in love with Bella.\nBase model summary: Rachel is in love with Bella.\n","output_type":"stream"}],"execution_count":38},{"cell_type":"markdown","source":"# Day 3, Learning Dataset formats and Downloading Open Source Dataset","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\n\n\nds= load_dataset(\"argilla/sharegpt-text-descriptives\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T14:05:02.432185Z","iopub.status.idle":"2025-05-04T14:05:02.432514Z","shell.execute_reply.started":"2025-05-04T14:05:02.432378Z","shell.execute_reply":"2025-05-04T14:05:02.432393Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check basic info\nprint(ds)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T14:05:02.433889Z","iopub.status.idle":"2025-05-04T14:05:02.434223Z","shell.execute_reply.started":"2025-05-04T14:05:02.434058Z","shell.execute_reply":"2025-05-04T14:05:02.434092Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# View a single sample\nprint(ds['train'][0])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T14:05:02.435307Z","iopub.status.idle":"2025-05-04T14:05:02.435540Z","shell.execute_reply.started":"2025-05-04T14:05:02.435423Z","shell.execute_reply":"2025-05-04T14:05:02.435433Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nprint(ds['train'][0]['response-suggestion'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T14:05:02.437891Z","iopub.status.idle":"2025-05-04T14:05:02.438212Z","shell.execute_reply.started":"2025-05-04T14:05:02.438024Z","shell.execute_reply":"2025-05-04T14:05:02.438040Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 8/5/25 Creating Custom Dataset using GitHub Issues of Pytorch","metadata":{}},{"cell_type":"code","source":"import requests\n\nurl = \"https://api.github.com/repos/pytorch/pytorch/issues?page=1&per_page=1\"\nresponse = requests.get(url)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T11:39:48.834470Z","iopub.execute_input":"2025-05-08T11:39:48.835013Z","iopub.status.idle":"2025-05-08T11:39:49.080492Z","shell.execute_reply.started":"2025-05-08T11:39:48.834989Z","shell.execute_reply":"2025-05-08T11:39:49.079882Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"response.status_code","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T11:39:49.171076Z","iopub.execute_input":"2025-05-08T11:39:49.171709Z","iopub.status.idle":"2025-05-08T11:39:49.176771Z","shell.execute_reply.started":"2025-05-08T11:39:49.171687Z","shell.execute_reply":"2025-05-08T11:39:49.176017Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"200"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"response.json()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T11:39:50.439092Z","iopub.execute_input":"2025-05-08T11:39:50.439940Z","iopub.status.idle":"2025-05-08T11:39:50.446420Z","shell.execute_reply.started":"2025-05-08T11:39:50.439909Z","shell.execute_reply":"2025-05-08T11:39:50.445613Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"[{'url': 'https://api.github.com/repos/pytorch/pytorch/issues/153144',\n  'repository_url': 'https://api.github.com/repos/pytorch/pytorch',\n  'labels_url': 'https://api.github.com/repos/pytorch/pytorch/issues/153144/labels{/name}',\n  'comments_url': 'https://api.github.com/repos/pytorch/pytorch/issues/153144/comments',\n  'events_url': 'https://api.github.com/repos/pytorch/pytorch/issues/153144/events',\n  'html_url': 'https://github.com/pytorch/pytorch/pull/153144',\n  'id': 3048581827,\n  'node_id': 'PR_kwDOA-j9z86VabfF',\n  'number': 153144,\n  'title': '[ROCm][Windows] Fix building torch 2.8 wheel with ROCm (added hipblasLt and rocblas directories)',\n  'user': {'login': 'tvukovic-amd',\n   'id': 127323445,\n   'node_id': 'U_kgDOB5bNNQ',\n   'avatar_url': 'https://avatars.githubusercontent.com/u/127323445?v=4',\n   'gravatar_id': '',\n   'url': 'https://api.github.com/users/tvukovic-amd',\n   'html_url': 'https://github.com/tvukovic-amd',\n   'followers_url': 'https://api.github.com/users/tvukovic-amd/followers',\n   'following_url': 'https://api.github.com/users/tvukovic-amd/following{/other_user}',\n   'gists_url': 'https://api.github.com/users/tvukovic-amd/gists{/gist_id}',\n   'starred_url': 'https://api.github.com/users/tvukovic-amd/starred{/owner}{/repo}',\n   'subscriptions_url': 'https://api.github.com/users/tvukovic-amd/subscriptions',\n   'organizations_url': 'https://api.github.com/users/tvukovic-amd/orgs',\n   'repos_url': 'https://api.github.com/users/tvukovic-amd/repos',\n   'events_url': 'https://api.github.com/users/tvukovic-amd/events{/privacy}',\n   'received_events_url': 'https://api.github.com/users/tvukovic-amd/received_events',\n   'type': 'User',\n   'user_view_type': 'public',\n   'site_admin': False},\n  'labels': [{'id': 1078897659,\n    'node_id': 'MDU6TGFiZWwxMDc4ODk3NjU5',\n    'url': 'https://api.github.com/repos/pytorch/pytorch/labels/module:%20rocm',\n    'name': 'module: rocm',\n    'color': 'f7e101',\n    'default': False,\n    'description': 'AMD GPU support for Pytorch'},\n   {'id': 1392590051,\n    'node_id': 'MDU6TGFiZWwxMzkyNTkwMDUx',\n    'url': 'https://api.github.com/repos/pytorch/pytorch/labels/open%20source',\n    'name': 'open source',\n    'color': 'ededed',\n    'default': False,\n    'description': None},\n   {'id': 3773064655,\n    'node_id': 'LA_kwDOA-j9z87g5GXP',\n    'url': 'https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing',\n    'name': 'topic: not user facing',\n    'color': 'B08798',\n    'default': False,\n    'description': 'topic category'}],\n  'state': 'open',\n  'locked': False,\n  'assignee': None,\n  'assignees': [],\n  'milestone': None,\n  'comments': 1,\n  'created_at': '2025-05-08T10:36:36Z',\n  'updated_at': '2025-05-08T10:47:53Z',\n  'closed_at': None,\n  'author_association': 'CONTRIBUTOR',\n  'type': None,\n  'active_lock_reason': None,\n  'draft': False,\n  'pull_request': {'url': 'https://api.github.com/repos/pytorch/pytorch/pulls/153144',\n   'html_url': 'https://github.com/pytorch/pytorch/pull/153144',\n   'diff_url': 'https://github.com/pytorch/pytorch/pull/153144.diff',\n   'patch_url': 'https://github.com/pytorch/pytorch/pull/153144.patch',\n   'merged_at': None},\n  'body': \"Since rocblas.dll and hipblaslt.dll are copied to torch/lib, rocblas and hipblaslt directories are needed to be stored there too (otherwise we have an error after wheel installation while searching for files in rocblas/library and hipblaslt/library which doesn't exist). This PR fixes this issue. \\n\\ncc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd\",\n  'closed_by': None,\n  'reactions': {'url': 'https://api.github.com/repos/pytorch/pytorch/issues/153144/reactions',\n   'total_count': 0,\n   '+1': 0,\n   '-1': 0,\n   'laugh': 0,\n   'hooray': 0,\n   'confused': 0,\n   'heart': 0,\n   'rocket': 0,\n   'eyes': 0},\n  'timeline_url': 'https://api.github.com/repos/pytorch/pytorch/issues/153144/timeline',\n  'performed_via_github_app': None,\n  'state_reason': None}]"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_1 = user_secrets.get_secret(\"Github_token\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T14:43:01.119771Z","iopub.execute_input":"2025-05-08T14:43:01.120493Z","iopub.status.idle":"2025-05-08T14:43:01.209562Z","shell.execute_reply.started":"2025-05-08T14:43:01.120467Z","shell.execute_reply":"2025-05-08T14:43:01.208993Z"}},"outputs":[],"execution_count":62},{"cell_type":"code","source":"GITHUB_TOKEN = secret_value_1# Copy your GitHub token here\nheaders = {\"Authorization\": f\"token {GITHUB_TOKEN}\"}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T14:43:08.225927Z","iopub.execute_input":"2025-05-08T14:43:08.226206Z","iopub.status.idle":"2025-05-08T14:43:08.230074Z","shell.execute_reply.started":"2025-05-08T14:43:08.226184Z","shell.execute_reply":"2025-05-08T14:43:08.229317Z"}},"outputs":[],"execution_count":64},{"cell_type":"code","source":"import time\nimport math\nfrom pathlib import Path\nimport pandas as pd\nfrom tqdm.notebook import tqdm\n\n\ndef fetch_issues(\n    owner=\"pytorch\",\n    repo=\"pytorch\",\n    num_issues=10_000,\n    rate_limit=5_000,\n    issues_path=Path(\".\"),\n):\n    if not issues_path.is_dir():\n        issues_path.mkdir(exist_ok=True)\n\n    batch = []\n    all_issues = []\n    per_page = 100  # Number of issues to return per page\n    num_pages = math.ceil(num_issues / per_page)\n    base_url = \"https://api.github.com/repos\"\n\n    for page in tqdm(range(num_pages)):\n        # Query with state=all to get both open and closed issues\n        query = f\"issues?page={page}&per_page={per_page}&state=all\"\n        issues = requests.get(f\"{base_url}/{owner}/{repo}/{query}\", headers=headers)\n        batch.extend(issues.json())\n\n        if len(batch) > rate_limit and len(all_issues) < num_issues:\n            all_issues.extend(batch)\n            batch = []  # Flush batch for next time period\n            print(f\"Reached GitHub rate limit. Sleeping for one hour ...\")\n            time.sleep(60 * 60 + 1)\n\n    all_issues.extend(batch)\n    df = pd.DataFrame.from_records(all_issues)\n    df.to_json(f\"{issues_path}/{repo}-issues.jsonl\", orient=\"records\", lines=True)\n    print(\n        f\"Downloaded all the issues for {repo}! Dataset stored at {issues_path}/{repo}-issues.jsonl\"\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T10:40:33.166715Z","iopub.execute_input":"2025-05-08T10:40:33.167068Z","iopub.status.idle":"2025-05-08T10:40:33.173446Z","shell.execute_reply.started":"2025-05-08T10:40:33.167045Z","shell.execute_reply":"2025-05-08T10:40:33.172642Z"}},"outputs":[],"execution_count":51},{"cell_type":"code","source":"# Depending on your internet connection, this can take several minutes to run...\nfetch_issues()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\njsonObj = pd.read_json(path_or_buf=\"/kaggle/working/pytorch-issues.jsonl\", lines=True)\n\n# Show all column names\nprint(jsonObj.columns.tolist())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T13:44:41.157529Z","iopub.execute_input":"2025-05-08T13:44:41.158072Z","iopub.status.idle":"2025-05-08T13:44:42.339814Z","shell.execute_reply.started":"2025-05-08T13:44:41.158049Z","shell.execute_reply":"2025-05-08T13:44:42.339051Z"}},"outputs":[{"name":"stdout","text":"['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'type', 'active_lock_reason', 'sub_issues_summary', 'body', 'closed_by', 'reactions', 'timeline_url', 'performed_via_github_app', 'state_reason', 'draft', 'pull_request']\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":" > using the above features to create a dataset for the pytorch-issues ","metadata":{}},{"cell_type":"code","source":"print(jsonObj.info())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T13:44:42.340875Z","iopub.execute_input":"2025-05-08T13:44:42.341099Z","iopub.status.idle":"2025-05-08T13:44:42.376801Z","shell.execute_reply.started":"2025-05-08T13:44:42.341073Z","shell.execute_reply":"2025-05-08T13:44:42.376198Z"}},"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 10000 entries, 0 to 9999\nData columns (total 33 columns):\n #   Column                    Non-Null Count  Dtype              \n---  ------                    --------------  -----              \n 0   url                       10000 non-null  object             \n 1   repository_url            10000 non-null  object             \n 2   labels_url                10000 non-null  object             \n 3   comments_url              10000 non-null  object             \n 4   events_url                10000 non-null  object             \n 5   html_url                  10000 non-null  object             \n 6   id                        10000 non-null  int64              \n 7   node_id                   10000 non-null  object             \n 8   number                    10000 non-null  int64              \n 9   title                     10000 non-null  object             \n 10  user                      10000 non-null  object             \n 11  labels                    10000 non-null  object             \n 12  state                     10000 non-null  object             \n 13  locked                    10000 non-null  bool               \n 14  assignee                  946 non-null    object             \n 15  assignees                 10000 non-null  object             \n 16  milestone                 151 non-null    object             \n 17  comments                  10000 non-null  int64              \n 18  created_at                10000 non-null  datetime64[ns, UTC]\n 19  updated_at                10000 non-null  datetime64[ns, UTC]\n 20  closed_at                 7016 non-null   datetime64[ns, UTC]\n 21  author_association        10000 non-null  object             \n 22  type                      34 non-null     object             \n 23  active_lock_reason        0 non-null      float64            \n 24  sub_issues_summary        3280 non-null   object             \n 25  body                      9954 non-null   object             \n 26  closed_by                 7231 non-null   object             \n 27  reactions                 10000 non-null  object             \n 28  timeline_url              10000 non-null  object             \n 29  performed_via_github_app  0 non-null      float64            \n 30  state_reason              1663 non-null   object             \n 31  draft                     6720 non-null   float64            \n 32  pull_request              6720 non-null   object             \ndtypes: bool(1), datetime64[ns, UTC](3), float64(3), int64(3), object(23)\nmemory usage: 2.5+ MB\nNone\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"jsonObj['body'][2000]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T14:00:15.417017Z","iopub.execute_input":"2025-05-08T14:00:15.417542Z","iopub.status.idle":"2025-05-08T14:00:15.422540Z","shell.execute_reply.started":"2025-05-08T14:00:15.417517Z","shell.execute_reply":"2025-05-08T14:00:15.421803Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"'cc @jeffdaily @sunway513 @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd'"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"jsonObj['state']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T14:20:02.642558Z","iopub.execute_input":"2025-05-08T14:20:02.642876Z","iopub.status.idle":"2025-05-08T14:20:02.649467Z","shell.execute_reply.started":"2025-05-08T14:20:02.642854Z","shell.execute_reply":"2025-05-08T14:20:02.648678Z"}},"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"0         open\n1         open\n2         open\n3         open\n4         open\n         ...  \n9995    closed\n9996    closed\n9997    closed\n9998    closed\n9999    closed\nName: state, Length: 10000, dtype: object"},"metadata":{}}],"execution_count":31},{"cell_type":"code","source":"jsonObj['author_association'].unique()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T14:21:12.501963Z","iopub.execute_input":"2025-05-08T14:21:12.502701Z","iopub.status.idle":"2025-05-08T14:21:12.509320Z","shell.execute_reply.started":"2025-05-08T14:21:12.502673Z","shell.execute_reply":"2025-05-08T14:21:12.508756Z"}},"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"array(['COLLABORATOR', 'NONE', 'CONTRIBUTOR', 'MEMBER'], dtype=object)"},"metadata":{}}],"execution_count":33},{"cell_type":"code","source":"jsonObj['body']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T14:22:03.769786Z","iopub.execute_input":"2025-05-08T14:22:03.770095Z","iopub.status.idle":"2025-05-08T14:22:03.777704Z","shell.execute_reply.started":"2025-05-08T14:22:03.770071Z","shell.execute_reply":"2025-05-08T14:22:03.776976Z"}},"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"0       ### 🐛 Describe the bug\\n\\nUsing NNPACK for con...\n1       ### 📚 The doc issue\\n\\nI have noticed there is...\n2       ### 🐛 Describe the bug\\n\\n## Description\\n\\nWh...\n3       ### 🐛 Describe the bug\\n\\nI use nn.MultiheadAt...\n4       This PR replaces most `std::chrono::system_clo...\n                              ...                        \n9995    cc @voznesenskym @penguinwu @EikanWang @jgong5...\n9996    Stack from [ghstack](https://github.com/ezyang...\n9997    By just extending the matrix and invoking scri...\n9998    Stack from [ghstack](https://github.com/ezyang...\n9999    Stack from [ghstack](https://github.com/ezyang...\nName: body, Length: 10000, dtype: object"},"metadata":{}}],"execution_count":34},{"cell_type":"code","source":"jsonObj['user'][0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T14:12:14.605275Z","iopub.execute_input":"2025-05-08T14:12:14.605693Z","iopub.status.idle":"2025-05-08T14:12:14.610993Z","shell.execute_reply.started":"2025-05-08T14:12:14.605661Z","shell.execute_reply":"2025-05-08T14:12:14.610371Z"}},"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"{'login': 'Flamefire',\n 'id': 309017,\n 'node_id': 'MDQ6VXNlcjMwOTAxNw==',\n 'avatar_url': 'https://avatars.githubusercontent.com/u/309017?v=4',\n 'gravatar_id': '',\n 'url': 'https://api.github.com/users/Flamefire',\n 'html_url': 'https://github.com/Flamefire',\n 'followers_url': 'https://api.github.com/users/Flamefire/followers',\n 'following_url': 'https://api.github.com/users/Flamefire/following{/other_user}',\n 'gists_url': 'https://api.github.com/users/Flamefire/gists{/gist_id}',\n 'starred_url': 'https://api.github.com/users/Flamefire/starred{/owner}{/repo}',\n 'subscriptions_url': 'https://api.github.com/users/Flamefire/subscriptions',\n 'organizations_url': 'https://api.github.com/users/Flamefire/orgs',\n 'repos_url': 'https://api.github.com/users/Flamefire/repos',\n 'events_url': 'https://api.github.com/users/Flamefire/events{/privacy}',\n 'received_events_url': 'https://api.github.com/users/Flamefire/received_events',\n 'type': 'User',\n 'user_view_type': 'public',\n 'site_admin': False}"},"metadata":{}}],"execution_count":30},{"cell_type":"code","source":"if jsonObj['user'][1]['login'] == \"shadow150519\":\n    print(jsonObj['user'][1])\nelse:\n    print(\"User Not Found\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T14:07:07.823785Z","iopub.execute_input":"2025-05-08T14:07:07.824070Z","iopub.status.idle":"2025-05-08T14:07:07.829006Z","shell.execute_reply.started":"2025-05-08T14:07:07.824049Z","shell.execute_reply":"2025-05-08T14:07:07.828191Z"}},"outputs":[{"name":"stdout","text":"{'login': 'shadow150519', 'id': 55205022, 'node_id': 'MDQ6VXNlcjU1MjA1MDIy', 'avatar_url': 'https://avatars.githubusercontent.com/u/55205022?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/shadow150519', 'html_url': 'https://github.com/shadow150519', 'followers_url': 'https://api.github.com/users/shadow150519/followers', 'following_url': 'https://api.github.com/users/shadow150519/following{/other_user}', 'gists_url': 'https://api.github.com/users/shadow150519/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/shadow150519/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/shadow150519/subscriptions', 'organizations_url': 'https://api.github.com/users/shadow150519/orgs', 'repos_url': 'https://api.github.com/users/shadow150519/repos', 'events_url': 'https://api.github.com/users/shadow150519/events{/privacy}', 'received_events_url': 'https://api.github.com/users/shadow150519/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"jsonObj['number']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T14:24:11.359051Z","iopub.execute_input":"2025-05-08T14:24:11.359695Z","iopub.status.idle":"2025-05-08T14:24:11.365676Z","shell.execute_reply.started":"2025-05-08T14:24:11.359669Z","shell.execute_reply":"2025-05-08T14:24:11.364932Z"}},"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"0       153139\n1       153138\n2       153137\n3       153136\n4       153135\n         ...  \n9995    143214\n9996    143213\n9997    143212\n9998    143211\n9999    143210\nName: number, Length: 10000, dtype: int64"},"metadata":{}}],"execution_count":35},{"cell_type":"code","source":"jsonObj[\"type\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T14:27:11.029562Z","iopub.execute_input":"2025-05-08T14:27:11.030168Z","iopub.status.idle":"2025-05-08T14:27:11.036067Z","shell.execute_reply.started":"2025-05-08T14:27:11.030142Z","shell.execute_reply":"2025-05-08T14:27:11.035297Z"}},"outputs":[{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"0       None\n1       None\n2       None\n3       None\n4       None\n        ... \n9995    None\n9996    None\n9997    None\n9998    None\n9999    None\nName: type, Length: 10000, dtype: object"},"metadata":{}}],"execution_count":43},{"cell_type":"code","source":"import json\n\ninput_path = \"/kaggle/working/pytorch-issues.jsonl\"\noutput_path = \"/kaggle/working/pytorch-issues-selected.jsonl\"\n\n# List the fields you want to keep\nfields_to_keep = [\"id\", \"title\", \"user\", \"state\", \"labels\", 'comments', \"author_association\", \"body\", ]\n\nwith open(input_path, \"r\") as f_in, open(output_path, \"w\") as f_out:\n    for line in f_in:\n        data = json.loads(line)\n        filtered_data = {k: data.get(k) for k in fields_to_keep}\n        # Optional: flatten nested 'user'\n        if isinstance(filtered_data.get(\"user\"), dict):\n            filtered_data[\"user\"] = filtered_data[\"user\"].get(\"login\", str(filtered_data[\"user\"]))\n        if isinstance(filtered_data.get(\"labels\"), list):\n            filtered_data[\"labels\"] = [label[\"name\"] if isinstance(label, dict) else label for label in filtered_data[\"labels\"]]\n        f_out.write(json.dumps(filtered_data) + \"\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T14:27:41.109136Z","iopub.execute_input":"2025-05-08T14:27:41.109643Z","iopub.status.idle":"2025-05-08T14:27:42.045394Z","shell.execute_reply.started":"2025-05-08T14:27:41.109606Z","shell.execute_reply":"2025-05-08T14:27:42.044842Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"from datasets import load_dataset\nissue_dataset = load_dataset(\"json\", data_files= \"/kaggle/working/pytorch-issues-selected.jsonl\")\nissue_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T14:27:53.839903Z","iopub.execute_input":"2025-05-08T14:27:53.840525Z","iopub.status.idle":"2025-05-08T14:27:54.098969Z","shell.execute_reply.started":"2025-05-08T14:27:53.840502Z","shell.execute_reply":"2025-05-08T14:27:54.098256Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"06f9b7c57b934713b8120dee5d866c51"}},"metadata":{}},{"execution_count":45,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'title', 'user', 'state', 'labels', 'comments', 'author_association', 'body'],\n        num_rows: 10000\n    })\n})"},"metadata":{}}],"execution_count":45},{"cell_type":"code","source":"\nissues_dataset  = load_dataset(\n    \"json\",\n    data_files=\"/kaggle/working/pytorch-issues-selected.jsonl\",\n    split=\"train\"\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T10:41:45.907512Z","iopub.execute_input":"2025-05-09T10:41:45.907787Z","iopub.status.idle":"2025-05-09T10:41:46.199735Z","shell.execute_reply.started":"2025-05-09T10:41:45.907767Z","shell.execute_reply":"2025-05-09T10:41:46.199012Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1bdebf9033434be38fd3637ffcd10239"}},"metadata":{}}],"execution_count":56},{"cell_type":"code","source":"issues_dataset['user'][:5]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T14:30:13.549172Z","iopub.execute_input":"2025-05-08T14:30:13.549876Z","iopub.status.idle":"2025-05-08T14:30:13.564949Z","shell.execute_reply.started":"2025-05-08T14:30:13.549850Z","shell.execute_reply":"2025-05-08T14:30:13.564131Z"}},"outputs":[{"execution_count":50,"output_type":"execute_result","data":{"text/plain":"['Flamefire', 'shadow150519', 'SilentTester73', 'Neronjust2017', 'cyyever']"},"metadata":{}}],"execution_count":50},{"cell_type":"markdown","source":">  basically here url is input and pr is output for our instruction tuning model","metadata":{}},{"cell_type":"code","source":"sample = issue_dataset[\"train\"].shuffle(seed=666).select(range(3))\n\n# Print out the title and body as input/output\nfor title, body in zip(sample[\"title\"], sample[\"body\"]):\n    print(f\">> Title (input): {title}\")\n    print(f\">> Body (output): {body}\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T14:33:47.864232Z","iopub.execute_input":"2025-05-08T14:33:47.864812Z","iopub.status.idle":"2025-05-08T14:33:47.880673Z","shell.execute_reply.started":"2025-05-08T14:33:47.864788Z","shell.execute_reply":"2025-05-08T14:33:47.880051Z"},"scrolled":true},"outputs":[{"name":"stdout","text":">> Title (input): Fix corner case in `torch.arange()` where int64_t truncation leads to size 0\n>> Body (output): Fixes #149097\n\n### Changes\n\nThis PR introduces a workaround for corner case where casting start/end/step to int64_t may introduce precision loss. If all values are within the range that double can represent exactly (i.e., [-2^53, 2^53]), we prefer using double arithmetic for consistency across devices. Otherwise, fallback to int64_t computation.\n\n### Tests\n\nAll results are same as np\n\n```\npython test/test_torch.py -k test_arange\n```\n\ncc: @albanD \n\n>> Title (input): Support SymmetricMemory's signaling kernels on sm60 and sm70\n>> Body (output): Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146308\n\nBy leveraging libcudacxx's utilities: https://nvidia.github.io/cccl/libcudacxx/extended_api/synchronization_primitives/atomic_ref.html\n\ncc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o\n\n>> Title (input): [ca][ddp] loud error with c++ reducer\n>> Body (output): Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #150258\n* __->__ #150073\n* #150074\n\n\n\ncc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o\n\n","output_type":"stream"}],"execution_count":55},{"cell_type":"code","source":"issues_dataset = issues_dataset.map(\n    lambda x: {\"is_title\": False if x[\"title\"] is None else True}\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T14:34:12.947070Z","iopub.execute_input":"2025-05-08T14:34:12.947743Z","iopub.status.idle":"2025-05-08T14:34:13.889035Z","shell.execute_reply.started":"2025-05-08T14:34:12.947718Z","shell.execute_reply":"2025-05-08T14:34:13.888228Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"319cd5f89ec34be383c736519b8d71b3"}},"metadata":{}}],"execution_count":56},{"cell_type":"code","source":"headers = {\n    \"Authorization\": f\"token {GITHUB_TOKEN}\",\n    \"Accept\": \"application/vnd.github+json\"\n}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T14:43:18.054225Z","iopub.execute_input":"2025-05-08T14:43:18.054775Z","iopub.status.idle":"2025-05-08T14:43:18.058127Z","shell.execute_reply.started":"2025-05-08T14:43:18.054752Z","shell.execute_reply":"2025-05-08T14:43:18.057376Z"}},"outputs":[],"execution_count":65},{"cell_type":"markdown","source":"*well that looks helping for our finetuning environment*","metadata":{}},{"cell_type":"code","source":"print(issues_dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T15:22:32.665742Z","iopub.execute_input":"2025-05-08T15:22:32.666021Z","iopub.status.idle":"2025-05-08T15:22:32.670131Z","shell.execute_reply.started":"2025-05-08T15:22:32.666002Z","shell.execute_reply":"2025-05-08T15:22:32.669435Z"}},"outputs":[{"name":"stdout","text":"Dataset({\n    features: ['id', 'title', 'user', 'state', 'labels', 'comments', 'author_association', 'body', 'is_title'],\n    num_rows: 10000\n})\n","output_type":"stream"}],"execution_count":76},{"cell_type":"code","source":"print(issues_dataset.column_names)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T15:23:25.394689Z","iopub.execute_input":"2025-05-08T15:23:25.396228Z","iopub.status.idle":"2025-05-08T15:23:25.401033Z","shell.execute_reply.started":"2025-05-08T15:23:25.396198Z","shell.execute_reply":"2025-05-08T15:23:25.399963Z"}},"outputs":[{"name":"stdout","text":"['id', 'title', 'user', 'state', 'labels', 'comments', 'author_association', 'body', 'is_title']\n","output_type":"stream"}],"execution_count":78},{"cell_type":"code","source":"print((issues_dataset[\"title\"][81]))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T15:23:34.643997Z","iopub.execute_input":"2025-05-08T15:23:34.644258Z","iopub.status.idle":"2025-05-08T15:23:34.661520Z","shell.execute_reply.started":"2025-05-08T15:23:34.644239Z","shell.execute_reply":"2025-05-08T15:23:34.660981Z"},"scrolled":true},"outputs":[{"name":"stdout","text":"`cuda.Event` handling in dynamo is broken\n","output_type":"stream"}],"execution_count":79},{"cell_type":"code","source":"print((issues_dataset[\"body\"][81]))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T15:23:58.678988Z","iopub.execute_input":"2025-05-08T15:23:58.679540Z","iopub.status.idle":"2025-05-08T15:23:58.732801Z","shell.execute_reply.started":"2025-05-08T15:23:58.679517Z","shell.execute_reply":"2025-05-08T15:23:58.732068Z"},"scrolled":true},"outputs":[{"name":"stdout","text":"Here's an example:\n```\nimport torch\n\nlst = []\n\n@torch.compile(backend=\"eager\", fullgraph=True)\ndef f(x):\n    start_event = torch.cuda.Event(enable_timing=True)\n    end_event = torch.cuda.Event(enable_timing=True)\n\n    start_event.record()\n    out = torch.matmul(x, x)\n    end_event.record()\n\n    lst.append(start_event)\n    lst.append(end_event)\n    return out\n\nx = torch.randn(5000, device='cuda')\nout = f(x)\nprint(lst[0].elapsed_time(lst[1]))\n```\n\nwithout compile this prints the elapsed time between the two events.\n```\n55.96131134033203\n```\n\nwith compile this gives an error:\n```\nTraceback (most recent call last):\n  File \"/data/users/hirsheybar/a/pytorch/tmp6.py\", line 20, in <module>\n    print(lst[0].elapsed_time(lst[1]))\n  File \"/data/users/hirsheybar/a/pytorch/torch/cuda/streams.py\", line 216, in elapsed_time\n    return super().elapsed_time(end_event)\nValueError: Both events must be recorded before calculating elapsed time.\n```\n\nWhy? here's the generated dynamo graph + residual bytecode below. It looks like:\n\n(1) dynamo handles the `cuda.Event()` creation + list appending as compile-time constants, stashing them as globals and putting them in the list as residual bytecode\n\n(2) dynamo *also* proxies the `cuda.Event()` object into the graph, even though it is also treating it as a constant. The `Event` object is unused though and gets DCEd\n\n(3) dynamo also proxies the `cuda.Event.record` calls into the graph, but they are DCEd\n\n(4) at runtime, none of the logic to record the events runs (even if it did it wouldn't run because dynamo is ignoring the events that were proxied in the graph)\n```\n# graph\n ===== __compiled_fn_1 =====\n /data/users/hirsheybar/a/pytorch/torch/fx/_lazy_graph_module.py class GraphModule(torch.nn.Module):\n    def forward(self, L_x_: \"f32[5000][1]cuda:0\"):\n        l_x_ = L_x_\n\n         # File: /data/users/hirsheybar/a/pytorch/tmp6.py:7 in f, code: start_event = torch.cuda.Event(enable_timing=True)\n        event = torch.cuda.streams.Event(enable_timing = True)\n\n         # File: /data/users/hirsheybar/a/pytorch/tmp6.py:8 in f, code: end_event = torch.cuda.Event(enable_timing=True)\n        event_1 = torch.cuda.streams.Event(enable_timing = True)\n\n         # File: /data/users/hirsheybar/a/pytorch/tmp6.py:10 in f, code: start_event.record()\n        record = event.record();  event = record = None\n\n         # File: /data/users/hirsheybar/a/pytorch/tmp6.py:11 in f, code: out = torch.matmul(x, x)\n        out: \"f32[][]cuda:0\" = torch.matmul(l_x_, l_x_);  l_x_ = None\n\n         # File: /data/users/hirsheybar/a/pytorch/tmp6.py:12 in f, code: end_event.record()\n        record_1 = event_1.record();  event_1 = record_1 = None\n        return (out,)\n\n# bytecode\nDEBUG: MODIFIED BYTECODE f /data/users/hirsheybar/a/pytorch/tmp6.py line 5\n  5           0 LOAD_GLOBAL              9 (__compiled_fn_1)\n              2 LOAD_FAST                0 (x)\n              4 DUP_TOP\n              6 STORE_FAST               7 (tmp_3)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (graph_out_0)\n             12 LOAD_FAST                4 (graph_out_0)\n             14 LOAD_CONST               3 (0)\n             16 BINARY_SUBSCR\n             18 LOAD_GLOBAL              7 (_event_140622680852736_c0)\n             20 LOAD_GLOBAL              8 (_event_140622672089088_c0)\n             22 BUILD_LIST               2\n             24 LOAD_GLOBAL              5 (lst)\n             26 DUP_TOP\n             28 STORE_FAST               6 (tmp_2)\n             30 LOAD_CONST               0 (None)\n             32 LOAD_CONST               0 (None)\n             34 BUILD_SLICE              2\n             36 STORE_SUBSCR\n             38 DELETE_FAST              4 (graph_out_0)\n             40 RETURN_VALUE\n```\n\ncc @ptrblck @msaroufim @eqy @jerryzh168 @chauhang @penguinwu @voznesenskym @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @amjames\n","output_type":"stream"}],"execution_count":81},{"cell_type":"code","source":"issues_dataset.push_to_hub(\"github-pytorch-issues\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T15:24:25.980230Z","iopub.execute_input":"2025-05-08T15:24:25.980794Z","iopub.status.idle":"2025-05-08T15:24:29.143237Z","shell.execute_reply.started":"2025-05-08T15:24:25.980770Z","shell.execute_reply":"2025-05-08T15:24:29.142494Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc486d031c0a41799af70291ec572f4e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Creating parquet from Arrow format:   0%|          | 0/10 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb5a4fef18f0468fadf5c5f5cdc6fa37"}},"metadata":{}},{"execution_count":82,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/datasets/mayankpuvvala/github-pytorch-issues/commit/a0a58a44501a7d2715771f14fed814e09b135ebc', commit_message='Upload dataset', commit_description='', oid='a0a58a44501a7d2715771f14fed814e09b135ebc', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/mayankpuvvala/github-pytorch-issues', endpoint='https://huggingface.co', repo_type='dataset', repo_id='mayankpuvvala/github-pytorch-issues'), pr_revision=None, pr_num=None)"},"metadata":{}}],"execution_count":82},{"cell_type":"markdown","source":"> we can see our custom dataset loaded to hugging face, change your dataset card readme for more better understanding of the dataset","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\ndataset = load_dataset(\"mayankpuvvala/github-pytorch-issues\", split=\"train\")\ndataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T17:09:09.626726Z","iopub.execute_input":"2025-05-10T17:09:09.627330Z","iopub.status.idle":"2025-05-10T17:09:15.517076Z","shell.execute_reply.started":"2025-05-10T17:09:09.627304Z","shell.execute_reply":"2025-05-10T17:09:15.516514Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/2.10k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b384f9e025045c7992293165fedde30"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/7.32M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc1bd016505e4c3ea538352856720854"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"495e9b2b07054ba1ad556243a2579397"}},"metadata":{}},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['id', 'title', 'user', 'state', 'labels', 'comments', 'author_association', 'body', 'is_title'],\n    num_rows: 10000\n})"},"metadata":{}}],"execution_count":7},{"cell_type":"markdown","source":"#  9/5/25 Fine tuning using PEFT+ LoRA on the custom dataset","metadata":{}},{"cell_type":"code","source":"from datasets import Dataset\nimport pandas as pd\nfrom transformers import AutoTokenizer\n\n# 1. Convert original dataset to pandas and remove all unwanted columns\ndf = dataset.to_pandas()\ndf = df[[\"title\", \"body\"]].copy()  # Keep only needed columns\ndf.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T17:09:15.518227Z","iopub.execute_input":"2025-05-10T17:09:15.518545Z","iopub.status.idle":"2025-05-10T17:09:15.623692Z","shell.execute_reply.started":"2025-05-10T17:09:15.518527Z","shell.execute_reply":"2025-05-10T17:09:15.623043Z"}},"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 10000 entries, 0 to 9999\nData columns (total 2 columns):\n #   Column  Non-Null Count  Dtype \n---  ------  --------------  ----- \n 0   title   10000 non-null  object\n 1   body    9954 non-null   object\ndtypes: object(2)\nmemory usage: 156.4+ KB\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# Rebuild dataset with sanitized dataframe\ndataset = Dataset.from_pandas(df, preserve_index=False)\nprint(dataset)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T17:09:15.874644Z","iopub.execute_input":"2025-05-10T17:09:15.875215Z","iopub.status.idle":"2025-05-10T17:09:16.079442Z","shell.execute_reply.started":"2025-05-10T17:09:15.875195Z","shell.execute_reply":"2025-05-10T17:09:16.078787Z"}},"outputs":[{"name":"stdout","text":"Dataset({\n    features: ['title', 'body'],\n    num_rows: 10000\n})\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"\ndataset = dataset.train_test_split(test_size=0.1, seed=42)\ndataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T17:09:19.212099Z","iopub.execute_input":"2025-05-10T17:09:19.212744Z","iopub.status.idle":"2025-05-10T17:09:19.224431Z","shell.execute_reply.started":"2025-05-10T17:09:19.212717Z","shell.execute_reply":"2025-05-10T17:09:19.223716Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['title', 'body'],\n        num_rows: 9000\n    })\n    test: Dataset({\n        features: ['title', 'body'],\n        num_rows: 1000\n    })\n})"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\ntokenizer.push_to_hub(\"mayankpuvvala/peft_lora_t5_merged_model_pytorch_issues\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T10:26:24.212190Z","iopub.execute_input":"2025-05-11T10:26:24.212976Z","iopub.status.idle":"2025-05-11T10:26:26.408481Z","shell.execute_reply.started":"2025-05-11T10:26:24.212946Z","shell.execute_reply":"2025-05-11T10:26:26.407562Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"267dfdb3f6d04b4e81fbfc5044f29112"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b6dee5487f5e4ebb97b5355c1a779876"}},"metadata":{}},{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/mayankpuvvala/peft_lora_t5_merged_model_pytorch_issues/commit/5c326874f96fa2f0f3493fe8be4627dea32e5b63', commit_message='Upload tokenizer', commit_description='', oid='5c326874f96fa2f0f3493fe8be4627dea32e5b63', pr_url=None, repo_url=RepoUrl('https://huggingface.co/mayankpuvvala/peft_lora_t5_merged_model_pytorch_issues', endpoint='https://huggingface.co', repo_type='model', repo_id='mayankpuvvala/peft_lora_t5_merged_model_pytorch_issues'), pr_revision=None, pr_num=None)"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"def preprocess(example):\n    # Ensure values are strings\n    title = example[\"title\"] if example[\"title\"] is not None else \"\"\n    body = example[\"body\"] if example[\"body\"] is not None else \"\"\n\n    # Tokenize the input (title)\n    model_input = tokenizer(\n        text=title,\n        max_length=128,\n        padding=\"max_length\",\n        truncation=True\n    )\n\n    # Tokenize the target (body)\n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer(\n            text=body,\n            max_length=128,\n            padding=\"max_length\",\n            truncation=True\n        )\n\n    model_input[\"labels\"] = labels[\"input_ids\"]\n    return model_input\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T17:44:04.806414Z","iopub.execute_input":"2025-05-10T17:44:04.806957Z","iopub.status.idle":"2025-05-10T17:44:04.811810Z","shell.execute_reply.started":"2025-05-10T17:44:04.806934Z","shell.execute_reply":"2025-05-10T17:44:04.811034Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"tokenized_dataset = dataset.map(\n    preprocess,\n    batched=False,\n    remove_columns=[\"title\", \"body\"]  # we no longer need raw text after tokenization\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T19:22:06.519963Z","iopub.execute_input":"2025-05-09T19:22:06.520236Z","iopub.status.idle":"2025-05-09T19:22:26.716380Z","shell.execute_reply.started":"2025-05-09T19:22:06.520211Z","shell.execute_reply":"2025-05-09T19:22:26.715241Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/9000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f8eac08ff97f4274b342b5b77a91c19e"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:3980: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63fae0dd0c4d454db23c37b8a4a9b047"}},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"from peft import get_peft_model, LoraConfig, TaskType\nfrom transformers import AutoModelForSeq2SeqLM\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")\n\nlora_config = LoraConfig(\n    r=8,\n    lora_alpha=32,\n    target_modules=[\"q\", \"v\"],\n    lora_dropout=0.1,\n    bias=\"none\",\n    task_type=TaskType.SEQ_2_SEQ_LM\n)\n\npeft_model = get_peft_model(model, lora_config)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T17:01:09.522091Z","iopub.execute_input":"2025-05-10T17:01:09.522357Z","iopub.status.idle":"2025-05-10T17:01:12.019431Z","shell.execute_reply.started":"2025-05-10T17:01:09.522338Z","shell.execute_reply":"2025-05-10T17:01:12.018877Z"},"scrolled":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9f36a26342c46ca84da372a09ecd85c"}},"metadata":{}},{"name":"stderr","text":"Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"029cbd3b1ab34645b39c56ecd3916d45"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"993feb7c57d844d39101ff66572356bb"}},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"./peft+lora_FineTuning_Custom_Dataset\",\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    num_train_epochs=3,\n    logging_dir=\"./logs\",\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    logging_strategy=\"steps\",\n    logging_steps=10,\n    disable_tqdm=False,  # 👈 ensures tqdm is enabled\n    report_to=\"none\",     # optionally disable WandB etc.\n    # fp16= True,\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T19:35:32.831820Z","iopub.execute_input":"2025-05-09T19:35:32.832151Z","iopub.status.idle":"2025-05-09T19:35:32.861851Z","shell.execute_reply.started":"2025-05-09T19:35:32.832124Z","shell.execute_reply":"2025-05-09T19:35:32.861324Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"# from transformers import DataCollatorForSeq2Seq, default_data_collator\n# def custom_collator(batch):\n#     for k in batch[0].keys():\n#         print(f\"BATCH KEY: {k}\")\n#     return default_data_collator(batch)\n\n# collator = custom_collator\ntrainer = Seq2SeqTrainer(\n    model=peft_model,\n    args=training_args,\n    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"test\"],\n    processing_class=tokenizer,\n    # data_collator=collator,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T19:35:34.175131Z","iopub.execute_input":"2025-05-09T19:35:34.175834Z","iopub.status.idle":"2025-05-09T19:35:34.189053Z","shell.execute_reply.started":"2025-05-09T19:35:34.175802Z","shell.execute_reply":"2025-05-09T19:35:34.188573Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer.save_model(\"full-checkpoint-trainer\")  # Saves model + trainer state\ntrainer.save_state()                      # Optional but saves training arguments, RNG etc.\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T19:35:42.045455Z","iopub.execute_input":"2025-05-09T19:35:42.045750Z","iopub.status.idle":"2025-05-09T19:35:42.517750Z","shell.execute_reply.started":"2025-05-09T19:35:42.045727Z","shell.execute_reply":"2025-05-09T19:35:42.516908Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"peft_model.save_pretrained(\"./lora-t5-pytorch\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T19:35:42.519027Z","iopub.execute_input":"2025-05-09T19:35:42.519258Z","iopub.status.idle":"2025-05-09T19:35:42.977178Z","shell.execute_reply.started":"2025-05-09T19:35:42.519240Z","shell.execute_reply":"2025-05-09T19:35:42.976486Z"},"scrolled":true},"outputs":[],"execution_count":37},{"cell_type":"markdown","source":"> pushing the lora model to HuggingFace Hub","metadata":{}},{"cell_type":"code","source":"# model_name = \"lora-t5-pytorch-issues\"  # change this to your preferred model name\n# hf_username = \"mayankpuvvala\"     # replace with your HF username\n\n# # Save and push the model\n# peft_model.push_to_hub(f\"{hf_username}/{model_name}\")\n# tokenizer.push_to_hub(f\"{hf_username}/{model_name}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T19:35:42.978280Z","iopub.execute_input":"2025-05-09T19:35:42.978873Z","iopub.status.idle":"2025-05-09T19:35:42.982308Z","shell.execute_reply.started":"2025-05-09T19:35:42.978847Z","shell.execute_reply":"2025-05-09T19:35:42.981658Z"}},"outputs":[],"execution_count":38},{"cell_type":"markdown","source":"\n# 10/5/25 Inference on the custom model\n","metadata":{}},{"cell_type":"code","source":"from peft import PeftModel\npeft_model = PeftModel.from_pretrained(model, \"./lora-t5-pytorch\")  # Load your trained LoRA adapter\nmerged_model =  peft_model.merge_and_unload()\n\nmerged_model.save_pretrained(\"merged-model\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T17:43:41.385978Z","iopub.status.idle":"2025-05-10T17:43:41.386206Z","shell.execute_reply.started":"2025-05-10T17:43:41.386088Z","shell.execute_reply":"2025-05-10T17:43:41.386098Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Print a few named layers to inspect structure\nfor name, param in list(merged_model.named_parameters())[:5]:\n    print(f\"{name}: {param.shape}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nmerged_model.push_to_hub(\"mayankpuvvala/peft_lora_t5_merged_model_pytorch_issues\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\ninput_text = \"bump XNNPACK dependency to fix GCC 14 build on aarch64-linux\"\ninputs = tokenizer(input_text, return_tensors=\"pt\").to(merged_model.device)\n\nwith torch.no_grad():\n    outputs = model.generate(\n    **inputs,\n    max_new_tokens=200,\n    eos_token_id=tokenizer.eos_token_id,\n    do_sample= True,\n    temperature=0.95\n)\n\n\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T17:41:26.935306Z","iopub.execute_input":"2025-05-10T17:41:26.935862Z","iopub.status.idle":"2025-05-10T17:41:28.227054Z","shell.execute_reply.started":"2025-05-10T17:41:26.935825Z","shell.execute_reply":"2025-05-10T17:41:28.226295Z"}},"outputs":[{"name":"stdout","text":"GitHub: @kungshinio's iPhone, reclassified to XNMACK....speak.com @kongwaopun.com @doync@echotbopun.nu @githubzang @shrkong\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"test_dataset = dataset[\"test\"]\ntest_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T17:43:34.482907Z","iopub.execute_input":"2025-05-10T17:43:34.483480Z","iopub.status.idle":"2025-05-10T17:43:34.488270Z","shell.execute_reply.started":"2025-05-10T17:43:34.483461Z","shell.execute_reply":"2025-05-10T17:43:34.487682Z"}},"outputs":[{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['title', 'body'],\n    num_rows: 1000\n})"},"metadata":{}}],"execution_count":39},{"cell_type":"code","source":"# !pip install rouge_score evaluate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T17:44:11.589179Z","iopub.execute_input":"2025-05-10T17:44:11.589457Z","iopub.status.idle":"2025-05-10T17:44:11.592791Z","shell.execute_reply.started":"2025-05-10T17:44:11.589438Z","shell.execute_reply":"2025-05-10T17:44:11.592124Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"from transformers import T5ForConditionalGeneration, T5Tokenizer, Seq2SeqTrainer, Seq2SeqTrainingArguments\nimport evaluate\nimport torch\n\ntokenized_test = test_dataset.map(preprocess, batched=False, remove_columns=[\"title\", \"body\"]  )\ntokenized_test","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T17:47:11.630448Z","iopub.execute_input":"2025-05-10T17:47:11.631100Z","iopub.status.idle":"2025-05-10T17:47:13.646855Z","shell.execute_reply.started":"2025-05-10T17:47:11.631071Z","shell.execute_reply":"2025-05-10T17:47:13.646096Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aeaa1f58bbb54350a60436608ae7715d"}},"metadata":{}},{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['input_ids', 'attention_mask', 'labels'],\n    num_rows: 1000\n})"},"metadata":{}}],"execution_count":46},{"cell_type":"markdown","source":"> testing inference on a single input sentence using the merged inference model of lora+t5 ","metadata":{}},{"cell_type":"code","source":"import torch\ninput_text = \"bump XNNPACK dependency to fix GCC 14 build on aarch64-linux\"\ninputs = tokenizer(input_text, return_tensors=\"pt\").to(merged_model.device)\n\nwith torch.no_grad():\n    outputs = merged_model.generate(\n    **inputs,\n    max_new_tokens=200,\n    eos_token_id=tokenizer.eos_token_id,\n    do_sample= True,\n    temperature=0.7\n)\n\nmerged_model_output= tokenizer.decode(outputs[0], skip_special_tokens=True)\n# print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T19:16:14.437463Z","iopub.execute_input":"2025-05-10T19:16:14.437734Z","iopub.status.idle":"2025-05-10T19:16:16.902340Z","shell.execute_reply.started":"2025-05-10T19:16:14.437715Z","shell.execute_reply":"2025-05-10T19:16:16.901691Z"}},"outputs":[],"execution_count":72},{"cell_type":"code","source":"import torch\ninput_text = \"bump XNNPACK dependency to fix GCC 14 build on aarch64-linux\"\ninputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n\nwith torch.no_grad():\n    outputs = model.generate(\n    **inputs,\n    max_new_tokens=200,\n    eos_token_id=tokenizer.eos_token_id,\n    do_sample= True,\n    # temperature=0.95\n)\nbase_model_output= tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n# print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T19:16:16.903798Z","iopub.execute_input":"2025-05-10T19:16:16.904312Z","iopub.status.idle":"2025-05-10T19:16:18.141991Z","shell.execute_reply.started":"2025-05-10T19:16:16.904292Z","shell.execute_reply":"2025-05-10T19:16:18.141357Z"}},"outputs":[],"execution_count":73},{"cell_type":"markdown","source":"> Comparing base model and custom built model","metadata":{}},{"cell_type":"code","source":"print(\"Base Model Output for the issue: \" + base_model_output)\nprint(\"\\n\")\nprint(\"My Model Output for the issue: \" + merged_model_output)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T19:16:18.142781Z","iopub.execute_input":"2025-05-10T19:16:18.143060Z","iopub.status.idle":"2025-05-10T19:16:18.147453Z","shell.execute_reply.started":"2025-05-10T19:16:18.143037Z","shell.execute_reply":"2025-05-10T19:16:18.146885Z"}},"outputs":[{"name":"stdout","text":"Base Model Output for the issue: \"?, \"Chang (Chongqing's), \"Chang (\"Ham-Salz\" = stag-\" = vgb) = '-:same ton \", if in a linux language-shifter it's only one error,\" xx.\n\n\nMy Model Output for the issue: Uninstalled XnNPACK dependency on XNNPACK - aarch64-linux-cma. This will help support the build on arg #0  XNNPACK - #0   XnNPACK    #0         @sui.j  nnnnNPACK is    #10 @Bossap: #cttynnnn.com/ji.\n","output_type":"stream"}],"execution_count":74},{"cell_type":"code","source":"import evaluate\nrouge = evaluate.load(\"rouge\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T19:16:18.148794Z","iopub.execute_input":"2025-05-10T19:16:18.149021Z","iopub.status.idle":"2025-05-10T19:16:19.217570Z","shell.execute_reply.started":"2025-05-10T19:16:18.149007Z","shell.execute_reply":"2025-05-10T19:16:19.216780Z"}},"outputs":[],"execution_count":75},{"cell_type":"code","source":"references = test_dataset[\"body\"]  # These are your gold labels\nlen(references)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T17:58:28.067772Z","iopub.execute_input":"2025-05-10T17:58:28.068076Z","iopub.status.idle":"2025-05-10T17:58:28.081234Z","shell.execute_reply.started":"2025-05-10T17:58:28.068057Z","shell.execute_reply":"2025-05-10T17:58:28.080579Z"}},"outputs":[{"execution_count":53,"output_type":"execute_result","data":{"text/plain":"1000"},"metadata":{}}],"execution_count":53},{"cell_type":"code","source":"from tqdm import tqdm\n\nbatch_size = 16\npredicted_outcome = []\n\nfor i in range(0, len(test_dataset), batch_size):\n    titles = test_dataset[\"title\"][i:i+batch_size]\n    inputs = tokenizer(titles, return_tensors=\"pt\", padding=True, truncation=True).to(merged_model.device)\n\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=200,\n            eos_token_id=tokenizer.eos_token_id,\n            do_sample=True,\n            temperature=0.95\n        )\n\n    # Decode all outputs in batch\n    decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n    predicted_outcome.extend(decoded)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T18:58:17.625257Z","iopub.execute_input":"2025-05-10T18:58:17.625891Z","iopub.status.idle":"2025-05-10T19:11:12.854102Z","shell.execute_reply.started":"2025-05-10T18:58:17.625867Z","shell.execute_reply":"2025-05-10T19:11:12.853480Z"}},"outputs":[],"execution_count":63},{"cell_type":"code","source":"# Make sure both predictions and references contain only strings, no None\npredictions = [p if p is not None else \"\" for p in predicted_outcome]\nreferences = [r if r is not None else \"\" for r in test_dataset[\"body\"]]\n\n# Now compute ROUGE\nresults = rouge.compute(predictions=predictions, references=references)\nprint(\"ROUGE results:\", results)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T19:11:12.855474Z","iopub.execute_input":"2025-05-10T19:11:12.855747Z","iopub.status.idle":"2025-05-10T19:11:19.078727Z","shell.execute_reply.started":"2025-05-10T19:11:12.855726Z","shell.execute_reply":"2025-05-10T19:11:19.078036Z"}},"outputs":[{"name":"stdout","text":"ROUGE results: {'rouge1': 0.0638729614520744, 'rouge2': 0.012113988201299337, 'rougeL': 0.047053712668345155, 'rougeLsum': 0.055277033992282835}\n","output_type":"stream"}],"execution_count":64},{"cell_type":"markdown","source":"> Bert Score","metadata":{}},{"cell_type":"code","source":"# ! pip install bert-score\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T19:21:40.395504Z","iopub.execute_input":"2025-05-10T19:21:40.395804Z","iopub.status.idle":"2025-05-10T19:21:40.399792Z","shell.execute_reply.started":"2025-05-10T19:21:40.395783Z","shell.execute_reply":"2025-05-10T19:21:40.399165Z"}},"outputs":[],"execution_count":77},{"cell_type":"code","source":"from bert_score import score\n\n# Compute BERTScore\nP, R, F1 = score(predictions, references, lang=\"en\", model_type=\"bert-base-uncased\")\n\n# Print averaged scores\nprint(f\"BERTScore Precision: {P.mean().item():.4f}\")\nprint(f\"BERTScore Recall:    {R.mean().item():.4f}\")\nprint(f\"BERTScore F1:        {F1.mean().item():.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T19:24:20.469266Z","iopub.execute_input":"2025-05-10T19:24:20.469528Z","iopub.status.idle":"2025-05-10T19:24:50.796644Z","shell.execute_reply.started":"2025-05-10T19:24:20.469511Z","shell.execute_reply":"2025-05-10T19:24:50.795906Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a20501917c8e48f0a3d180a6606d027b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8af6823e56224a1aa8ca574421a6b756"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e99fa243fe4b444f8dfd52101a8b23cf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f0f53d342ffb45f2b81439f8dbb2659f"}},"metadata":{}},{"name":"stderr","text":"Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02d0ca5f5f4246bab2d93ec244f94584"}},"metadata":{}},{"name":"stderr","text":"Warning: Empty reference sentence detected; setting raw BERTScores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty reference sentence detected; setting raw BERTScores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty reference sentence detected; setting raw BERTScores to 0.\nWarning: Empty reference sentence detected; setting raw BERTScores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","output_type":"stream"},{"name":"stdout","text":"BERTScore Precision: 0.5320\nBERTScore Recall:    0.4619\nBERTScore F1:        0.4902\n","output_type":"stream"}],"execution_count":79},{"cell_type":"code","source":"# # ! pip install vllm\n# ! python3 -m vllm.entrypoints.openai.api_server --model \"mayankpuvvala/peft_lora_t5_merged_model_pytorch_issues\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T17:32:55.163475Z","iopub.execute_input":"2025-05-10T17:32:55.163751Z","iopub.status.idle":"2025-05-10T17:32:55.167097Z","shell.execute_reply.started":"2025-05-10T17:32:55.163733Z","shell.execute_reply":"2025-05-10T17:32:55.166395Z"}},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"! dir","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T10:19:32.745123Z","iopub.execute_input":"2025-05-11T10:19:32.745402Z","iopub.status.idle":"2025-05-11T10:19:32.893488Z","shell.execute_reply.started":"2025-05-11T10:19:32.745384Z","shell.execute_reply":"2025-05-11T10:19:32.892653Z"}},"outputs":[{"name":"stdout","text":"flan-t5-custom\t\t peft+lora_FineTuning_Custom_Dataset\nformatted.jsonl\t\t pytorch_fine_tune_training_args\nfull-checkpoint-trainer  pytorch-issues-flat.jsonl\nlogs\t\t\t pytorch-issues.jsonl\nlora-t5-pytorch\t\t pytorch-issues-selected.jsonl\nmerged-model\t\t runs\npeft-flan-t5\t\t state.db\npeft-flan-t5-lora\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# Use a pipeline as a high-level helper\nfrom transformers import pipeline\n\npipe = pipeline(\"text2text-generation\", model=\"mayankpuvvala/peft_lora_t5_merged_model_pytorch_issues\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T10:26:32.742761Z","iopub.execute_input":"2025-05-11T10:26:32.743083Z","iopub.status.idle":"2025-05-11T10:26:34.077378Z","shell.execute_reply.started":"2025-05-11T10:26:32.743056Z","shell.execute_reply":"2025-05-11T10:26:34.076641Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/20.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dbf3d641e4a9454f9eb636041b063d8e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"673decc2705d437186b9e459019b34d3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"504e92fdd38740349eb826f5c0a9af31"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"94d6a6dcac4544b098b3787d34876274"}},"metadata":{}},{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}